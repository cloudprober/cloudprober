[{"categories":null,"contents":"Adding additional labels You can add additional labels to probe metrics using a probe-level field: additional_label. An additional label's value can be static, or it can be determined at the run-time: from the environment that the probe is running in (e.g. GCE instance labels), or target's labels.\nExample config here demonstrates adding various types of additional labels to probe metrics. For this config (also listed below for quick rerefence):\n if ingress target has label \u0026ldquo;fqdn:app.example.com\u0026rdquo;, and prober is running in the GCE zone us-east1-c, and prober's GCE instance has label env:prod.  Probe metrics will look like the following:\n total{probe=\u0026quot;my_ingress\u0026quot;,ptype=\u0026quot;http\u0026quot;,metrictype=\u0026quot;prober\u0026quot;,env=\u0026quot;prod\u0026quot;,src_zone=\u0026quot;us-east1-c\u0026quot;,host=\u0026quot;app.example.com\u0026quot;}: 90 success{probe=\u0026quot;my_ingress\u0026quot;,ptype=\u0026quot;http\u0026quot;,metrictype=\u0026quot;prober\u0026quot;,env=\u0026quot;prod\u0026quot;,src_zone=\u0026quot;us-east1-c\u0026quot;,host=\u0026quot;app.example.com\u0026quot;}: 80 probe { name: \u0026#34;my_ingress\u0026#34; type: HTTP targets { rds_targets { resource_path: \u0026#34;k8s://ingresses\u0026#34; filter { key: \u0026#34;namespace\u0026#34; value: \u0026#34;default\u0026#34; } } } # Static label additional_label { key: \u0026#34;metrictype\u0026#34; value: \u0026#34;prober\u0026#34; } # Label is configured at the run-time, based on the prober instance label (GCE). additional_label { key: \u0026#34;env\u0026#34; value: \u0026#34;{{.label_env}}\u0026#34; } # Label is configured at the run-time, based on the prober environment (GCE). additional_label { key: \u0026#34;src_zone\u0026#34; value: \u0026#34;{{.zone}}\u0026#34; } # Label is configured based on the target\u0026#39;s labels. additional_label { key: \u0026#34;host\u0026#34; value: \u0026#34;@target.label.fqdn@\u0026#34; } http_probe {} } (Listing source: examples/additional_label/cloudprober.cfg)\nAdding your own metrics For external probes, Cloudprober also allows external programs to provide additional metrics. See External Probe for more details.\n","permalink":"https://cloudprober.org/how-to/additional-labels/","tags":null,"title":"Additional Labels"},{"categories":null,"contents":"Cloudprober can natively export metrics to AWS Cloudwatch using the cloudwatch surfacer. Adding the cloudwatch surfacer to cloudprover is as simple as adding the following stanza to the config:\nsurfacer { type: CLOUDWATCH } Authentication The cloudwatch surfacer uses the AWS Go SDK, and supports the default credential chain:\n Environment variables. Shared credentials file. If your application uses an ECS task definition or RunTask API operation, IAM role for tasks. If your application is running on an Amazon EC2 instance, IAM role for Amazon EC2.  Cloudwatch Region The list below is the order of precedence that will be used to determine the AWS region that Cloudprober will publish metrics to.\n Region configuration EC2 metadata. AWS_REGION environment variable. AWS_DEFAULT_REGION environment variable, if AWS_SDK_LOAD_CONFIG is set (See AWS package documentation for more details).  Authorization In order to permit Cloudprober to publish metric data to cloudwatch, ensure the profile being used for authentication has the following permissions, where the \u0026ldquo;cloudwatch:namespace\u0026rdquo; is the metric namespace used by Cloudprober.\nIf the default metric namespace is changed, also change the condition in the IAM policy below to match the same value.\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Condition\u0026quot;: { \u0026quot;StringEqualsIgnoreCase\u0026quot;: { \u0026quot;cloudwatch:namespace\u0026quot;: \u0026quot;cloudprober\u0026quot; } }, \u0026quot;Action\u0026quot;: [ \u0026quot;cloudwatch:PutMetricData\u0026quot; ], \u0026quot;Resource\u0026quot;: [ \u0026quot;*\u0026quot; ], \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Sid\u0026quot;: \u0026quot;PutMetrics\u0026quot; } ] } Metric Namespace The metric namespace used to publish metrics to by default is set to cloudprober. This can be changed by expanding the surfacer configuration:\nsurfacer { type: CLOUDWATCH cloudwatch_surfacer { namespace: \u0026quot;/cloudprober/website/probes\u0026quot; } } Note: If the namespace is modified, also modify the IAM policy condition for the namespace PutMetricData call.\nConfiguration Options The full list of configuration options for the cloudwatch surfacer is:\n// The cloudwatch metric namespace  optional string namespace = 1 [default = \u0026#34;cloudprober\u0026#34;]; // The cloudwatch resolution value, lowering this below 60 will incur  // additional charges as the metrics will be charged at a high resolution rate.  optional int64 resolution = 2 [default=60]; // The AWS Region, used to create a CloudWatch session.  // The order of fallback for evaluating the AWS Region:  // 1. This config value.  // 2. EC2 metadata endpoint, via cloudprober sysvars.  // 3. AWS_REGION environment value.  // 4. AWS_DEFAULT_REGION environment value, if AWS_SDK_LOAD_CONFIG is set.  // https://docs.aws.amazon.com/sdk-for-go/api/aws/session/  optional string region = 3;(Source: https://github.com/cloudprober/cloudprober/blob/master/surfacers/cloudwatch/proto/config.proto)\nCalculating the metric delta with Cloudwatch Metric Maths The metrics produced by Cloudprober are cumulative. Most services producing metrics into cloudwatch produce snapshot data whereby the metrics are recorded for a specific point in time.\nIn order to achieve a similar effect here, the Cloudwatch Metric Maths RATE and PERIOD functions can be used to determine the delta values.\nRATE(m1) * PERIOD(m1) Whereby m1 is the metric id for the Cloudprober metrics, for example:\nnamespace: cloudprober metric name: latency dst: google.com ptype: http probe: probe name ","permalink":"https://cloudprober.org/surfacers/cloudwatch/","tags":null,"title":"Cloudwatch (AWS Cloud Monitoring)"},{"categories":null,"contents":"Percentiles give you a deeper insight into how your system is behaving. For example, if your application's response latency is very low 94 times out 100 but very high for the remaining 6 times, your average latency will still be low but it won't be a great experience for your users. In other words, this is the case where your 95th percentile latency is high, even though your average and median (50th-%ile) latency is very low.\nA typical way to measure percentiles from continuous monitoring data, which you may have to aggregate across various sources, is to use histograms (also called, distributions). In a histogram, you assign the incoming data points (samples) to pre-defined buckets. Each data point increases the count for the bucket that it falls into; data point itself is discarded after that. You can take a look at the bucket counts at any point of time and get an estimate of the percentiles. Histograms make it easy to aggregate data across multiple entities, for example, from probes running on multiple machines.\nFollowing diagram shows distribution of latencies into 9 equal sized histogram buckets:\n(Above diagram shows histogram for the following samples: 5.1, 6.2, 9.0, 12.1, 8.3, 9.7, 9.4, 10.3, 14.1, 11.2, 16.6, 9.9, 10.6, 14.1, 0.9, 7.1, 17.7)\nHistograms in Cloudprober (Distributions) Cloudprober uses a metric type called \u0026lsquo;distribution\u0026rsquo; to create and export histograms. Cloudprober supports creating distributions for probe latencies, and for metrics generated from external probe payloads. To create distributions, you have to specify how the data should be bucketed \u0026ndash; you can either explicitly specify all bucket bounds, or use exponential buckets type which generates bucket bounds from only a few variables.\nHere is an example of using explicit buckets for latencies:\nprobe { name: \u0026quot;...\u0026quot; type: HTTP targets { host_names: \u0026quot;...\u0026quot; } latency_unit: \u0026quot;ms\u0026quot; latency_distribution { explicit_buckets: \u0026quot;0.01,0.1,0.15,0.2,0.25,0.35,0.5,0.75,1.0,1.5,2.0,3.0,4.0,5.0,10.0,15.0,20.0\u0026quot; } } Configuring distributions As seen in the example above, for latencies you configure distribution at the probe level by adding a field called latency_distribution. Without this field, cloudprober exports only cumulative latencies. To create distributions from an external probe's data, take a look at the external probe's documentation.\nFormat for the distribution field is in turn defined in dist.proto.\n// Dist defines a Distribution data type. message Dist { oneof buckets { // Comma-separated list of lower bounds, where each lower bound is a float  // value. Example: 0.5,1,2,4,8.  string explicit_buckets = 1; // Exponentially growing buckets  ExponentialBuckets exponential_buckets = 2; }}// ExponentialBucket defines a set of num_buckets+2 buckets: // bucket[0] covers (−Inf, 0) // bucket[1] covers [0, scale_factor) // bucket[2] covers [scale_factor, scale_factor*base) // ... // bucket[i] covers [scale_factor*base^(i−2), scale_factor*base^(i−1)) // ... // bucket[num_buckets+1] covers [scale_factor*base^(num_buckets−1), +Inf) // Note: Base must be at least 1.01. message ExponentialBuckets { optional float scale_factor = 1 [default = 1.0]; optional float base = 2 [default = 2]; optional uint32 num_buckets = 3 [default = 20];}Percentiles and Heatmap Now that we've configured cloudprober to generate distributions, how do we make use of this new information. This depends on the monitoring system (prometheus, stackdriver, postgres, etc) you're exporting your data to.\nBoth prometheus and stackdriver support computing and plotting percentiles from the distributions data. Stackdriver can natively create heatmaps from distributions while for prometheus you need to use grafana to create heatmaps.\nStackdriver (Google Cloud Monitoring) Stackdriver automatically shows percentile aggregator for distribution metrics in metrics explorer (example). You can also use Stackdriver MQL to create percentiles (see stackdriver documentation for other usages of MQL for cloudprober metrics):\nfetch gce_instance | metric \u0026#39;custom.googleapis.com/cloudprober/http/google_homepage/latency\u0026#39; | filter (resource.zone == \u0026#39;us-central1-a\u0026#39;) | align delta(1m) | every 1m | group_by [resource.zone], [value_latency_percentile: percentile(value.latency, 95)] Stackdriver has detailed documentation on charting distributions.\nPrometheus Cloudprober surfaces distributions to prometheus as prometheus metric type histogram. Here is an example of prometheus metrics page created by cloudprober:\n# TYPE latency histogram latency_sum{ptype=\u0026#34;http\u0026#34;,probe=\u0026#34;my_probe\u0026#34;,dst=\u0026#34;hostA\u0026#34;} 77557.14022499947 1607766316442 latency_count{ptype=\u0026#34;http\u0026#34;,probe=\u0026#34;my_probe\u0026#34;,dst=\u0026#34;hostA\u0026#34;} 172150 1607766316442 latency_bucket{ptype=\u0026#34;http\u0026#34;,probe=\u0026#34;my_probe\u0026#34;,dst=\u0026#34;hostA\u0026#34;,le=\u0026#34;0.01\u0026#34;} 0 1607766316442 latency_bucket{ptype=\u0026#34;http\u0026#34;,probe=\u0026#34;my_probe\u0026#34;,dst=\u0026#34;hostA\u0026#34;,le=\u0026#34;0.1\u0026#34;} 0 1607766316442 ... ... latency_bucket{ptype=\u0026#34;http\u0026#34;,probe=\u0026#34;my_probe\u0026#34;,dst=\u0026#34;hostA\u0026#34;,le=\u0026#34;75\u0026#34;} 172150 1607766316442 latency_bucket{ptype=\u0026#34;http\u0026#34;,probe=\u0026#34;my_probe\u0026#34;,dst=\u0026#34;hostA\u0026#34;,le=\u0026#34;100\u0026#34;} 172150 1607766316442 latency_bucket{ptype=\u0026#34;http\u0026#34;,probe=\u0026#34;my_probe\u0026#34;,dst=\u0026#34;hostA\u0026#34;,le=\u0026#34;+Inf\u0026#34;} 172150 1607766316442 Fortunately there is already a plenty of good documentation on how to make use of histograms in prometheus and grafana:\n Grafana blog on how to visualize prometheus histograms in grafana. Prometheus documentation on histrograms.  More Resources  The Problem with Percentiles – Aggregation brings Aggravation. Why percentiles don't work the way you think.  ","permalink":"https://cloudprober.org/how-to/percentiles/","tags":null,"title":"Percentiles, Histograms, and Distributions"},{"categories":null,"contents":"Cloudprober can natively export metrics to Google Cloud Monitoring (formerly, Stackdriver) using stackdriver surfacer. Adding stackdriver surfacer to cloudprober is as simple as adding the following stanza to the config:\nsurfacer { type: STACKDRIVER } This config will work if you're running on GCP and your VM (or GKE pod) has access to Cloud Monitoring (Stackdriver). If running on any other platform, you'll have to specify the GCP project where you want to send the metrics, and you'll have to configure your environment for Google Application Default Credentials.\nBy default, stackdriver surfacer exports metrics with the following prefix: custom.googleapis.com/cloudprober/\u0026lt;probe-type\u0026gt;/\u0026lt;probe\u0026gt;. For example, for HTTP probe named google_com, standard metrics will be exported as:\ncustom.googleapis.com/cloudprober/http/google_com/total custom.googleapis.com/cloudprober/http/google_com/success custom.googleapis.com/cloudprober/http/google_com/failure custom.googleapis.com/cloudprober/http/google_com/latency Here are all the config options for stackdriver surfacer:\n// GCP project name for stackdriver. If not specified and running on GCP,  // local project is used.  optional string project = 1; // If allowed_metrics_regex is specified, only metrics matching the given  // regular expression will be exported to stackdriver. Since probe type and  // probe name are part of the metric name, you can use this field to restrict  // stackdriver metrics to a particular probe.  // Example:  // allowed_metrics_regex: \u0026#34;.*(http|ping).*(success|validation_failure).*\u0026#34;  optional string allowed_metrics_regex = 3; // Monitoring URL base. Full metric URL looks like the following:  // \u0026lt;monitoring_url\u0026gt;/\u0026lt;ptype\u0026gt;/\u0026lt;probe\u0026gt;/\u0026lt;metric\u0026gt;  // Example:  // custom.googleapis.com/cloudprober/http/google-homepage/latency  optional string monitoring_url = 4 [default = \u0026#34;custom.googleapis.com/cloudprober/\u0026#34;];(Source: https://github.com/cloudprober/cloudprober/blob/master/surfacers/stackdriver/proto/config.proto)\nFor example, you can configure stackdriver surfacer to export only metrics that match a specific regex:\nsurfacer { stackdriver_surfacer { # Export only \u0026#34;http\u0026#34; probe metrics. allowed_metrics_regex: \u0026#34;.*\\\\/http\\\\/.*\u0026#34; }}Accessing the data Cloudprober exports metrics to stackdriver as custom metrics. Since all cloudprober metrics are counters (total number of probes, success, latency), you'll see rates of these metrics in stackdriver metrics explorer by default. This data may not be very useful as it is (unless you're using distributions in cludprober, more on that later).\nHowever, stackdriver now provides a powerful monitoring query language,MQL, using which we can get more useful metrics.\nMQL to get failure ratio:\nfetch global | { metric \u0026#39;custom.googleapis.com/cloudprober/http/google_com/failure\u0026#39; ; metric \u0026#39;custom.googleapis.com/cloudprober/http/google_com/total\u0026#39; } | align delta(1m) | join | div MQL to get average latency for a probe:\nfetch global | { metric \u0026#39;custom.googleapis.com/cloudprober/http/google_com/latency\u0026#39; ; metric \u0026#39;custom.googleapis.com/cloudprober/http/google_com/success\u0026#39; } | align delta(1m) | join | div You can use MQL to create graphs and generate alerts. Note that in the examples here we are fetching from the \u0026ldquo;global\u0026rdquo; source (fetch global); if you're running on GCP, you can improve performance of your queries by specifying the \u0026ldquo;gce_instance\u0026rdquo; resource type: fetch gce_instance.\n","permalink":"https://cloudprober.org/surfacers/stackdriver/","tags":null,"title":"Stackdriver (Google Cloud Monitoring)"},{"categories":null,"contents":"One of the biggest strengths of cloudprober is that it can export data to multiple monitoring systems, even simultaneously, just based on simple configuration. Cloudprober does that using a built-in mechanism, called surfacers. Each surfacer type implements interface for a specific monitoring system, for example, pubsub surfacer publishes data to Google Pub/Sub. You can configure multiple surfacers at the same time. If you don't specify any surfacer, prometheus and file surfacers are enabled automatically.\nWhy other monitoring systems? Cloudprober's main purpose is to run probes and build standard, usable metrics based on the results of those probes. It doesn't take any action on the generated data. Instead, it provides an easy interface to make that probe data available to systems that provide ways to consume monitoring data, for example for graphing and alerting.\nCloudprober currently supports following surfacer types:\n Prometheus (config) Stackdriver (Google Cloud Monitoring) Google Pub/Sub (config) Postgres (config) File (config) Cloudwatch (AWS Cloud Monitoring)  Source: surfacers config.\nIt's easy to add more surfacers without having to understand the internals of cloudprober. You only need to implement the Surfacer interface.\nConfiguration Adding surfacers to cloudprober is as easy as adding \u0026ldquo;surfacer\u0026rdquo; config stanzas to your config, like the following:\n# Enable prometheus and stackdriver surfacers. # Make probe metrics available at the URL :\u0026lt;cloudprober_port\u0026gt;/metrics, for # scraping by prometheus. surfacer { type: PROMETHEUS prometheus_surfacer { # Following option adds a prefix to exported metrics, for example, # \u0026#34;total\u0026#34; metric is exported as \u0026#34;cloudprober_total\u0026#34;. metrics_prefix: \u0026#34;cloudprober_\u0026#34; } } # Stackdriver (Google Cloud Monitoring) surfacer. No other configuration # is necessary if running on GCP. surfacer { type: STACKDRIVER } Filtering Metrics It is possible to filter the metrics that the surfacers receive.\nFiltering by Label Cloudprober can filter the metrics that are published to surfacers. To filter metrics by labels, reference one of the following keys in the surfacer configuration:\n allow_metrics_with_label ignore_metrics_with_label  Note: ignore_metrics_with_label takes precedence over allow_metrics_with_label.\nFor example, to ignore all sysvar metrics:\nsurfacer { type: PROMETHEUS ignore_metrics_with_label { key: \u0026quot;probe\u0026quot;, value: \u0026quot;sysvars\u0026quot;, } } Or to only allow metrics from http probes:\nsurfacer { type: PROMETHEUS allow_metrics_with_label { key: \u0026quot;ptype\u0026quot;, value: \u0026quot;http\u0026quot;, } } Filtering by Metric Name For certain surfacers, cloudprober can filter the metrics that are published by name. The surfacers that support this functionality are:\n Cloudwatch Prometheus Stackdriver  Within the surfacer configuration, the following options are defined:\n allow_metrics_with_name ignore_metrics_with_name  Note: ignore_metrics_with_name takes precedence over allow_metrics_with_name.\nTo filter out all validation_failure metrics by name:\nsurfacer { type: PROMETHEUS ignore_metrics_with_name: \u0026quot;validation_failure\u0026quot; } (Source: https://github.com/cloudprober/cloudprober/blob/master/surfacers/proto/config.proto)\n","permalink":"https://cloudprober.org/surfacers/overview/","tags":null,"title":"Surfacers"},{"categories":null,"contents":"Automatic and continuous discovery of the targets is one of the core features of Cloudprober. This feature is specially critical for the dynamic environments that today's cloud based deployments make possible. For exmaple, in a kubernetes cluster number of pods and their IPs can change on the fly, either in response to replica count changes or node failures. Automated targets discovery makes sure that we don't have to reconfigure Cloudprober in response to such events.\nOverview The main idea behind Cloudprober's targets discovery is to use an independent source of truth to figure out the targets we are supposed to monitor. This source of truth is usually the resource provider's API, for example, GCE API and Kubernetes API. Cloudprober periodically polls these APIs to get the latest list of resources.\nExample targets configuration:\ntargets { rds_targets { # Monitor all endpoints for the service service-a resource_path: \u0026#34;k8s://endpoints/service-a\u0026#34; } } Some salient features of the cloudprober's targets discovery:\n Continuous discovery. We don't just discover targets in the beginning, but keep refreshing them at a regular interval. Protection against the upstream provider failures. If refreshing of the targets fails during one of the refresh cycles, we continue using the existing set of targets. Targets data includes name, labels, IP and ports (labels and ports are optional). These details allow configuring probes, for example, automatically setting port in the HTTP probe, and using target lables for probe results labeling. Well-defined protobuf based API (RDS \u0026ndash; more on it below) to add support for more target types. Currently supports GCE and Kubernetes resources. Adding more resource types should be straightforward.  Cloudprober currently (as of v0.10.7) supports following types of dynamic targets:\n   Resource Provider Resource Types     Kubernetes (k8s) pods, endpoints, services, ingresses   GCP (gcp) gce_instances, pubsub_messages    Resource Discovery Service To provide a consistent interface between targets\u0026rsquo; configuration and the actual implementation, Cloudprober defines and uses a protocol called RDS (Resource Discovery Service). Cloudprober's targets module includes a targets type \u0026ldquo;rds_targets\u0026rdquo;, that talks to an RDS backend that is either part of the same process or available over gRPC.\n\nHere are the RDS targets configuration (RDSTargets) options:\nmessage RDSTargets { // RDS server options, for example:  // rds_server_options {  // server_address: \u0026#34;rds-server.xyz:9314\u0026#34;  // oauth_config: {  // ...  // }  // }  // Default is to use the local server if any.  optional rds.ClientConf.ServerOptions rds_server_options = 1; // Resource path specifies the resources to return. Resources paths have the  // following format:  // \u0026lt;resource_provider\u0026gt;://\u0026lt;resource_type\u0026gt;/\u0026lt;additional_params\u0026gt;  //  // Examples:  // For GCE instances in projectA: \u0026#34;gcp://gce_instances/\u0026lt;projectA\u0026gt;\u0026#34;  // Kubernetes Pods : \u0026#34;k8s://pods\u0026#34;  optional string resource_path = 2; // Filters to filter resources by. Example:  // filter {  // key: \u0026#34;namespace\u0026#34;  // value: \u0026#34;mynamesspace\u0026#34;  // }  // filter {  // key: \u0026#34;labels.app\u0026#34;  // value: \u0026#34;web-service\u0026#34;  // }  repeated rds.Filter filter = 3; // IP config to specify the IP address to pick for a resource. IPConfig  // is defined here:  // https://github.com/cloudprober/cloudprober/blob/master/rds/proto/rds.proto  optional rds.IPConfig ip_config = 4;}Most options are explained in the comments for quick references. Here is the further explanation of some of these options:\nrds_server_options This field specifies how to connect to the RDS server: server address and security options (OAuth and TLS). If left unspecified, it connects to the local server if any (started through rds_server option). Next up it looks for the rds_server_options in global_targets_options.\nresource_path Resource path specifies the resources we are interested in. It consists of resource provider, resource type and optional relative path: \u0026lt;resource_provider\u0026gt;://\u0026lt;resource_type\u0026gt;/\u0026lt;optional_relative_path\u0026gt;\n resource_provider: Resource provider is a generic concept within the RDS protocol but usually maps to the cloud provider. Cloudprober RDS server currently implements the Kubernetes (k8s) and GCP (gcp) resource providers. We plan to add more resource providers in future. resource_type: Available resource types depend on the providers, for example, for k8s provider supports the following resource types: pods, endpoints, and services. optional_relative_path: For most resource types you can specify resource name in the resource path itself, e.g. k8s://services/cloudprober. Alternatively, you can use filters to filter by name, resource, etc.  filter Filters are key-value strings that can be used to filter resources by various fields. Filters depend on the resource types, but most resources support filtering by name and labels.\n# Return resources that start with \u0026quot;web\u0026quot; and have label \u0026quot;service:service-a\u0026quot; ... filter { key: \u0026quot;name\u0026quot; value: \u0026quot;^web.*\u0026quot; } filter { key: \u0026quot;labels.service\u0026quot; value: \u0026quot;service-a\u0026quot; }  Filters supported by kubernetes resources: k8s filters. Filters supported by GCP:  GCE Instances Pub/Sub Messages    Running RDS Server RDS server can either be run as an independent process or it can be part of the main prober process. Former mode is useful for large deployments where you may want to reduce the API upcall traffic (for example, to GCP). For example, if you run 1000+ prober processes, it will be much more economical, from the API quota usage point of view, to have a centralized RDS service with much fewer (2-3) instances instead of having each prober process make its own API calls.\nRDS server can be added to a cloudprober process using the rds_server stanza. If you're running RDS server in a remote process, you'll have to enable gRPC server in that process (using grpc_port) so that other instances can access it remotely.\nHere is an example RDS server configuration:\nrds_server { # GCP provider to discover GCP resources. provider { gcp_config { # Projects to discover resources in. project: \u0026#34;test-project-1\u0026#34; project: \u0026#34;test-project-2\u0026#34; # Discover GCE instances in us-central1. gce_instances { zone_filter: \u0026#34;name = us-central1-*\u0026#34; re_eval_sec: 60 # How often to refresh, default is 300s. } # GCE forwarding rules. forwarding_rules {} } } # Kubernetes targets are further discussed at: # https://cloudprober.org/how-to/run-on-kubernetes/#kubernetes-targets provider { kubernetes_config { endpoints {} } } } # Enable gRPC server for RDS. Only required for remote access to RDS server. grpc_port: 9314 For the remote RDS server setup, if accessing over external network, you can secure the underlying gRPC communication using TLS certificates.\n","permalink":"https://cloudprober.org/concepts/targets/","tags":null,"title":"Targets Discovery"},{"categories":null,"contents":"Cloudprober has a few built in servers. This is useful when you are probing that a connection is working, or as a baseline to compare the probing results from your actual service to.\nHTTP server { type: HTTP http_server { port: 8080 } } This creates an HTTP server that responds on port 8080. By default it will respond to the following endpoints:\n /healthcheck /lameduck  server { type: HTTP http_server { port: 8080 pattern_data_handler { response_size: 1024 } pattern_data_handler { response_size: 4 pattern: \u0026#34;four\u0026#34; } } } This adds two endpoints to the HTTP server:\n /data_1024 which responds with 1024 bytes of cloudprobercloudprobercloudprober. /data_4 which responds with four.  See servers/http/proto/config.go for all HTTP server configuration options.\nUDP A Cloudprober UDP server can be configured to either echo or discard packets it receives.\nserver { type: UDP udp_server { port: 85 type: ECHO } } server { type: UDP udp_server { port: 90 type: DISCARD } } See servers/udp/proto/config.go for all UDP server configuration options.\nGRPC See servers/grpc/proto/config.go for all GRPC server configuration options.\n","permalink":"https://cloudprober.org/how-to/built-in-servers/","tags":null,"title":"Built-in Servers"},{"categories":null,"contents":"Kubernetes is a popular platform for running containers, and Cloudprober container runs on Kubernetes right out of the box. This document shows how you can use config map to provide config to cloudprober and reload cloudprober on config changes.\nConfigMap In Kubernetes, a convenient way to provide config to containers is to use config maps. Let's create a config that specifies a probe to monitor \u0026ldquo;google.com\u0026rdquo;.\nprobe { name: \u0026#34;google-http\u0026#34; type: HTTP targets { host_names: \u0026#34;www.google.com\u0026#34; } http_probe {} interval_msec: 15000 timeout_msec: 1000 } Save this config in cloudprober.cfg, create a config map using the following command:\nkubectl create configmap cloudprober-config \\  --from-file=cloudprober.cfg=cloudprober.cfg If you change the config, you can update the config map using the following command:\nkubectl create configmap cloudprober-config \\  --from-file=cloudprober.cfg=cloudprober.cfg -o yaml --dry-run | \\  kubectl replace -f - Deployment Map Now let's add a deployment.yaml to add the config volume and cloudprober container:\napiVersion:apps/v1kind:Deploymentmetadata:name:cloudproberspec:replicas:1selector:matchLabels:app:cloudprobertemplate:metadata:annotations:checksum/config:\u0026#39;${CONFIG_CHECKSUM}\u0026#39;labels:app:cloudproberspec:volumes:-name:cloudprober-configconfigMap:name:cloudprober-configcontainers:-name:cloudproberimage:cloudprober/cloudprobercommand:[\u0026#39;/cloudprober\u0026#39;]args:[\u0026#39;--config_file\u0026#39;,\u0026#39;/cfg/cloudprober.cfg\u0026#39;,\u0026#39;--logtostderr\u0026#39;]volumeMounts:-name:cloudprober-configmountPath:/cfgports:-name:httpcontainerPort:9313---apiVersion:v1kind:Servicemetadata:name:cloudproberlabels:app:cloudproberspec:ports:-port:9313protocol:TCPtargetPort:9313selector:app:cloudprobertype:NodePortNote that we added an annotation to the deployment spec; this annotation allows us to update the deployment whenever cloudprober config changes. We can update this annotation based on the local cloudprober config content, and update the deployment using the following one-liner:\n# Update the config checksum annotation in deployment.yaml before running # kubectl apply. export CONFIG_CHECKSUM=$(kubectl get cm/cloudprober-config -o yaml | sha256sum) \u0026amp;\u0026amp; \\ cat deployment.yaml | envsubst | kubectl apply -f - (Note: If you use Helm for Kubernetes deployments, Helm provides a more native way to include config checksums in deployments.)\nApplying the above yaml file, should create a deployment with a service at port 9313:\n$ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE cloudprober 1/1 1 1 94m $ kubectl get service cloudprober NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE cloudprober NodePort 10.31.249.108 \u0026lt;none\u0026gt; 9313:31367/TCP 94m Now you should be able to access various cloudprober URLs (/status for status,/config for config, /metrics for prometheus-format metrics) from within the cluster. For quick verification you can also set up a port forwarder and access these URLs locally at localhost:9313:\nkubectl port-forward svc/cloudprober 9313:9313 Once you've verified that everything is working as expected, you can go on setting up metrics collection through prometheus (or stackdriver) in usual ways.\nKubernetes Targets If you're running on Kuberenetes, you'd probably want to monitor Kubernetes resources (e.g. pods, endpoints, etc) as well. Good news is that cloudprober supports dynamic targets discovery of Kubernetes resources.\nFor example, following config adds HTTP probing of Kubernetes endpoints named \u0026lsquo;cloudprober\u0026rsquo; (equivalent to kubectl get ep cloudprober).\nprobe { name: \u0026#34;pod-to-endpoints\u0026#34; type: HTTP targets { # RDS (resource discovery service) targets # Equivalent to kubectl get ep cloudprober rds_targets { resource_path: \u0026#34;k8s://endpoints/cloudprober\u0026#34; } } http_probe { relative_url: \u0026#34;/status\u0026#34; } } # Run an RDS gRPC server to discover Kubernetes targets. rds_server { provider { # For all options, please take a look at: # https://github.com/cloudprober/cloudprober/blob/master/rds/kubernetes/proto/config.proto#L38 kubernetes_config { endpoints {} } } } This config adds a probe for endpoints named \u0026lsquo;cloudprober\u0026rsquo;. Kubernetes targets configuration is further explained in the section below.\nKubernetes RDS Targets As explained here, cloudprober uses RDS for dynamic targets discovery. In the above config, we add an internal RDS server that provides expansion for kubernetes endpoints (other supported types are \u0026ndash; pods, services). Inside the probe, we specify targets of the type rds_targets with resource path, k8s://endpoints/cloudprober. This resource path specifies resource of the type \u0026lsquo;endpoints\u0026rsquo; and with the name \u0026lsquo;cloudprober\u0026rsquo; (Hint: you can skip the name part of the resource path to discover all endpoints in the cluster).\nCluster Resources Access RDS server that we added above discovers cluster resources using kubernetes APIs. It assumes that we are interested in the cluster we are running it in, and uses in-cluster config to talk to the kubernetes API server. For this set up to work, we need to give our container read-only access to kubernetes resources:\n# Define a ClusterRole (resource-reader) for read-only access to the cluster# resources and bind this ClusterRole to the default service account.cat\u0026lt;\u0026lt;EOF|kubectlapply-f-apiVersion:rbac.authorization.k8s.io/v1kind:ClusterRolemetadata:annotations:rbac.authorization.kubernetes.io/autoupdate:\u0026#34;true\u0026#34;name:resource-readernamespace:defaultrules:-apiGroups:[\u0026#34;\u0026#34;]resources:[\u0026#34;*\u0026#34;]verbs:[\u0026#34;get\u0026#34;,\u0026#34;list\u0026#34;]-apiGroups:-extensions-\u0026#34;networking.k8s.io\u0026#34;# k8s 1.14+resources:-ingresses-ingresses/statusverbs:[\u0026#34;get\u0026#34;,\u0026#34;list\u0026#34;]---apiVersion:rbac.authorization.k8s.io/v1kind:ClusterRoleBindingmetadata:name:default-resource-readernamespace:defaultsubjects:-kind:ServiceAccountname:defaultnamespace:defaultroleRef:kind:ClusterRolename:resource-readerapiGroup:rbac.authorization.k8s.ioEOFThis will give default service account read-only access to the cluster resources. If you don't want to give the \u0026ldquo;default\u0026rdquo; user this access, you can create a new service account for cloudprober and use it in the deployment spec above.\nPush Config Update To push new cloudprober config to the cluster:\n# Update the config map kubectl create configmap cloudprober-config \\  --from-file=cloudprober.cfg=cloudprober.cfg -o yaml --dry-run | \\  kubectl replace -f - # Update deployment export CONFIG_CHECKSUM=$(kubectl get cm/cloudprober-config -o yaml | sha256sum) \u0026amp;\u0026amp; \\ cat deployment.yaml | envsubst | kubectl apply -f - Cloudprober should now start monitoring cloudprober endpoints. To verify:\n# Set up port fowarding such that you can access cloudprober:9313 through # localhost:9313. kubectl port-forward svc/cloudprober 9313:9313 \u0026amp; # Check config curl localhost:9313/config # Check metrics curl localhost:9313/metrics If you're running on GKE and have not disabled cloud logging, you'll also see logs in Stackdriver Logging.\n","permalink":"https://cloudprober.org/how-to/run-on-kubernetes/","tags":null,"title":"Running On Kubernetes"},{"categories":null,"contents":"Validators allow you to run checks on the probe request output (if any). For example, you can specify if you expect the probe output to match a certain regex or return a certain status code (for HTTP). You can configure more than one validators and all validators should succeed for the probe to be marked as success.\nprobe { name: \u0026#34;google_homepage\u0026#34; type: HTTP targets { host_names: \u0026#34;www.google.com\u0026#34; } interval_msec: 10000 # Probe every 10s # This validator should succeed. validator { name: \u0026#34;status_code_2xx\u0026#34; http_validator { success_status_codes: \u0026#34;200-299\u0026#34; } } # This validator will fail, notice missing \u0026#39;o\u0026#39; in our regex. validator { name: \u0026#34;gogle_re\u0026#34; regex: \u0026#34;gogle\u0026#34; } } (Full listing: https://github.com/cloudprober/cloudprober/blob/master/examples/validators/cloudprober_validator.cfg)\nTo make the debugging easier, validation failures are logged and exported as an independent map counter \u0026ndash; validation_failure, with validator key. For example, the above example will result in the following counters being exported after 5 runs:\ntotal{probe=\u0026#34;google_homepage\u0026#34;,dst=\u0026#34;www.google.com\u0026#34;} 5 success{probe=\u0026#34;google_homepage\u0026#34;,dst=\u0026#34;www.google.com\u0026#34;} 0 validation_failure{validator=\u0026#34;status_code_2xx\u0026#34;,probe=\u0026#34;google_homepage\u0026#34;,dst=\u0026#34;www.google.com\u0026#34;} 0 validation_failure{validator=\u0026#34;gogle_re\u0026#34;,probe=\u0026#34;google_homepage\u0026#34;,dst=\u0026#34;www.google.com\u0026#34;} 5 Note that validator counter will not go up if probe fails for other reasons, for example web server timing out. That's why you typically don't want to alert only on validation failures. That said, in some cases, validation failures could be the only thing you're interested in, for example, if you're trying to make sure that a certain copyright is always present in your web pages or you want to catch data integrity issues in your network.\nLet's take a look at the types of validators you can configure.\nRegex Validator Regex validator simply checks for a regex in the probe request output. It works for all probe types except for UDP and UDP_LISTENER - these probe types don't support any validators at the moment.\nHTTP Validator HTTP response validator works only for the HTTP probe type. You can currently use HTTP validator to define success and failure status codes (represented by success_status_codes and failure_stauts_codes in the config):\n If failure_status_codes is defined and response status code falls within that range, validator is considered to have failed. If success_status_codes is defined and response status code does not fall within that range, validator is considered to have failed. If failure_header is defined and HTTP response include specified header and there are matching values, validator is considered to have failed. Leaving value_regex empty checks only for header name. If success_header is defined and HTTP response does not include specified header with matching values, validator is considered to have failed. Leaving value_regex empty checks only for header name.  Data Integrity Validator Data integrity validator is designed to catch the packet corruption issues in the network. We have a basic check that verifies that the probe output is made up purely of a pattern repeated many times over.\n","permalink":"https://cloudprober.org/how-to/validators/","tags":null,"title":"Validators"},{"categories":null,"contents":"Cloudprober allows you to extend it across \u0026ldquo;probe\u0026rdquo; and \u0026ldquo;target\u0026rdquo; dimensions, that is, you can add new probe and target types to it without having to fork the entire codebase. Note that to extend cloudprober in this way, you will have to maintain your own cloudprober binary (which is mostly a wrapper around the \u0026ldquo;cloudprober package\u0026rdquo;), but you'll be able to use rest of the cloudprober code from the common location.\nSample probe type To demonstrate how it works, let's add a new probe-type to Cloudprober. We'll take the sample redis probe that we added in the external probe how-to, and convert it into a probe type that one can easily re-use. Let's say that this probe-type provides a way to test redis server functionality and it takes the following options - operation (GET vs SET vs DELETE), key, value. This probe's configuration looks like this:\nprobe { name: \u0026#34;redis_set\u0026#34; type: EXTENSION targets { host_names: \u0026#34;localhost:6379\u0026#34; } redis_probe { op: \u0026#34;set\u0026#34; key: \u0026#34;testkey\u0026#34; value: \u0026#34;testval\u0026#34; } } To make cloudprober understand this config, we'll have to do a few things:\n  Define the probe config in a protobuf (.proto) file and mark it as an extension of the overall config.\n  Implement the probe type, possibly as a Go package, even though it can be embedded directly into the top-level binary.\n  Create a new cloudprober binary that includes the new probe type package.\n  Protobuf for the new probe type Let's create a new directory for our code: $GOPATH/src/myprober.\n// File: $GOPATH/src/myprober/myprobe/myprobe.proto syntax = \u0026#34;proto2\u0026#34;;import \u0026#34;github.com/cloudprober/cloudprober/probes/proto/config.proto\u0026#34;;package myprober;message ProbeConf { // Redis operation  required string op = 1; // Key and value for the redis operation  required string key = 2; optional string value = 3;}extend cloudprober.probes.ProbeDef { optional ProbeConf redis_probe = 200;} Let's generate Go code for this protobuf:\n# From the myprober directory protoc --go_out=.,import_path=myprobe:. --proto_path=$GOPATH/src:. myprobe/*.proto $ ls myprobe/ myprobe.pb.go myprobe.proto Implement the probe type Now let's implement our probe type. Our probe type should implement the probes.Probe interface.\npackage myprobe // Probe holds aggregate information about all probe runs, per-target. type Probe struct { name string c *configpb.ProbeConf targets []string opts *options.Options ... } // Init initializes the probe with the given params. func (p *Probe) Init(name string, opts *options.Options) error { c, ok := opts.ProbeConf.(*ProbeConf) if !ok { return fmt.Errorf(\u0026#34;not a my probe config\u0026#34;) } // initialize p fields, p.name = name, etc. } // Start runs the probe indefinitely, at the configured interval. func (p *Probe) Start(ctx context.Context, dataChan chan *metrics.EventMetrics) { probeTicker := time.NewTicker(p.opts.Interval) for { select { case \u0026lt;-ctx.Done(): probeTicker.Stop() return case \u0026lt;-probeTicker.C: for _, em := range p.res { dataChan \u0026lt;- em } p.targets = p.opts.Targets.List() ... probeCtx, cancelFunc := context.WithDeadline(ctx, time.Now().Add(p.opts.Timeout)) p.runProbe(probeCtx) cancelFunc() } } } // runProbe runs probe for all targets and update EventMetrics. func (p *Probe) runProbe(ctx context.Context) { p.targets = p.opts.Targets.List() var wg sync.WaitGroup for _, target := range p.targets { wg.Add(1) go func(target string, em *metrics.EventMetrics) { defer wg.Done() em.Metric(\u0026#34;total\u0026#34;).AddInt64(1) start := time.Now() err := p.runProbeForTarget(ctx, target) // run probe just for a single target  if err != nil { p.l.Errorf(err.Error()) return } em.Metric(\u0026#34;success\u0026#34;).AddInt64(1) em.Metric(\u0026#34;latency\u0026#34;).AddFloat64(time.Now().Sub(start).Seconds() / p.opts.LatencyUnit.Seconds()) }(target, p.res[target]) } wg.Wait() } Full example in examples/extensions/myprober/myprobe/myprobe.go.\nThis probe type sets or gets (depending on the configuration) a key-valye in redis and records success and time taken (latency) if operation is successful.\nImplement a cloudprober binary that includes support for our probe package main ... func main() { flag.Parse() // Register our probe type  probes.RegisterProbeType(int(myprobe.E_RedisProbe.Field), func() probes.Probe { return \u0026amp;myprobe.Probe{} }) err := cloudprober.InitFromConfig(getConfig()) // getConfig not shown here.  if err != nil { glog.Exitf(\u0026#34;Error initializing cloudprober. Err: %v\u0026#34;, err) } // web.Init sets up web UI for cloudprober.  web.Init() cloudprober.Start(context.Background()) // Wait forever  select {} } Full example in examples/extensions/myprober/myprober.go.\nLet's write a test config that uses the newly defined probe type:\nprobe { name: \u0026#34;redis_set\u0026#34; type: EXTENSION interval_msec: 10000 timeout_msec: 5000 targets { host_names: \u0026#34;localhost:6379\u0026#34; } [myprober.redis_probe] { op: \u0026#34;set\u0026#34; key: \u0026#34;testkey\u0026#34; value: \u0026#34;testval\u0026#34; } } Full example in examples/extensions/myprober/myprober.cfg.\nLet's compile our prober and run it with the above config:\ngo build ./myprober.go ./myprober --config_file=myprober.cfg you should see an output like the following: cloudprober 1540848577649139842 1540848587 labels=ptype=redis,probe=redis_set,dst=localhost:6379 total=31 success=31 latency=70579.823 cloudprober 1540848577649139843 1540848887 labels=ptype=sysvars,probe=sysvars hostname=\u0026#34;manugarg-macbookpro5.roam.corp.google.com\u0026#34; start_timestamp=\u0026#34;1540848577\u0026#34; cloudprober 1540848577649139844 1540848887 labels=ptype=sysvars,probe=sysvars uptime_msec=310007.784 gc_time_msec=0.000 mallocs=14504 frees=826 cloudprober 1540848577649139845 1540848887 labels=ptype=sysvars,probe=sysvars goroutines=12 mem_stats_sys_bytes=7211256 cloudprober 1540848577649139846 1540848587 labels=ptype=redis,probe=redis_set,dst=localhost:6379 total=32 success=32 latency=72587.981 cloudprober 1540848577649139847 1540848897 labels=ptype=sysvars,probe=sysvars hostname=\u0026#34;manugarg-macbookpro5.roam.corp.google.com\u0026#34; start_timestamp=\u0026#34;1540848577\u0026#34; cloudprober 1540848577649139848 1540848897 labels=ptype=sysvars,probe=sysvars uptime_msec=320006.541 gc_time_msec=0.000 mallocs=14731 frees=844 cloudprober 1540848577649139849 1540848897 labels=ptype=sysvars,probe=sysvars goroutines=12 mem_stats_sys_bytes=7211256\nYou can import this data in prometheus following the process outlined at: Running Prometheus.\nConclusion The article shows how to add a new probe type to cloudprober. Extending cloudprober allows you to implement new probe types that may make sense for your organization, but not for the open source community. You have to implement the logic for the probe type, but other cloudprober features work as it is \u0026ndash; targets, metrics (e.g. latency distribution if you configure it), surfacers - data can be multiple systems simultaneously, etc.\n","permalink":"https://cloudprober.org/how-to/extensions/","tags":null,"title":"Extending Cloudprober"},{"categories":null,"contents":"External probe type allows you to run arbitrary, complex probes through Cloudprober. An external probe runs an independent external program for actual probing. Cloudprober calculates probe metrics based on program's exit status and time elapsed in execution.\nCloudprober also allows external programs to provide additional metrics. Every message sent to stdout will be parsed as a new metrics to be emitted. For general logging you can use another I/O stream like stderr.\nSample Probe To understand how it works, lets create a sample probe that sets and gets a key in a redis server. Here is the main function of such a probe:\nfunc main() { var client redis.Client var key = \u0026#34;hello\u0026#34; startTime := time.Now() client.Set(key, []byte(\u0026#34;world\u0026#34;)) fmt.Printf(\u0026#34;op_latency_ms{op=set} %f\\n\u0026#34;, float64(time.Since(startTime).Nanoseconds())/1e6) startTime = time.Now() val, _ := client.Get(\u0026#34;hello\u0026#34;) log.Printf(\u0026#34;%s=%s\u0026#34;, key, string(val)) fmt.Printf(\u0026#34;op_latency_ms{op=get} %f\\n\u0026#34;, float64(time.Since(startTime).Nanoseconds())/1e6) } (Full listing: https://github.com/cloudprober/cloudprober/blob/master/examples/external/redis_probe.go)\nThis program sets and gets a key in redis and prints the time taken for both operations. op_latency_ms{op=get|set} will be emitted as metrics. You could also define your own labels using this format:\nCloudprober can use this program as an external probe, to verify the availability and performance of the redis server. This program assumes that redis server is running locally, at its default port. For the sake of demonstration, lets run a local redis server (you can also easily modify this program to use a different server.)\n#!bash OS=$(uname) [[ \u0026#34;$OS\u0026#34; == \u0026#34;Darwin\u0026#34; ]] \u0026amp;\u0026amp; brew install redis [[ \u0026#34;$OS\u0026#34; == \u0026#34;Linux\u0026#34; ]] \u0026amp;\u0026amp; sudo apt install redis Let's compile our probe program (redis_probe.go) and verify that it's working as expected:\n#!bash CGO_ENABLED=0 go build -ldflags “-extldflags=-static” examples/external/redis_probe.go ./redis_probe 2022/02/24 12:39:45 hello=world op_latency_ms{op=set} 22.656588 op_latency_ms{op=get} 2.173560 Configuration Here is the external probe configuration that makes use of this program:\nFull example in examples/external/cloudprober.cfg.\n# Run an external probe that executes a command from the current working # directory. probe { name: \u0026#34;redis_probe\u0026#34; type: EXTERNAL targets { dummy_targets {} } external_probe { mode: ONCE command: \u0026#34;./redis_probe\u0026#34; } } Note: To pass target information to your external program, you can send target information as arguments using the @label@ notation. Supported fields are: target, address, port, probe, and target labels like target.label.fqdn.\ncommand: \u0026quot;./redis_probe\u0026quot; -host=@address@ -port=@port@ Running it through cloudprober, you'll see the following output:\n# Launch cloudprober cloudprober --config_file=cloudprober.cfg cloudprober 1519..0 1519583408 labels=ptype=external,probe=redis_probe,dst= success=1 total=1 latency=12143.765 cloudprober 1519..1 1519583408 labels=ptype=external,probe=redis_probe,dst=,op=get op_latency_ms=0.516 get_latency_ms=0.491 cloudprober 1519..2 1519583410 labels=ptype=external,probe=redis_probe,dst= success=2 total=2 latency=30585.915 cloudprober 1519..3 1519583410 labels=ptype=external,probe=redis_probe,dst=,op=set op_latency_ms=0.636 get_latency_ms=0.994 cloudprober 1519..4 1519583412 labels=ptype=external,probe=redis_probe,dst= success=3 total=3 latency=42621.871 You can import this data in prometheus following the process outlined at: Running Prometheus. Before doing that, let's make it more interesting.\nDistributions How nice will it be if we could find distribution of the set and get latency. If tail latency was too high, it could explain the random timeouts in your application. Fortunately, it's very easy to create distributions in Cloudprober. You just need to add the following section to your probe definition:\nFull example in examples/external/cloudprober_aggregate.cfg.\n# Run an external probe and aggregate metrics in cloudprober. ... output_metrics_options { aggregate_in_cloudprober: true # Create distributions for get_latency_ms and set_latency_ms. dist_metric { key: \u0026#34;op_latency_ms\u0026#34; value: { explicit_buckets: \u0026#34;0.1,0.2,0.4,0.6,0.8,1.0,2.0\u0026#34; } } } This configuration adds options to aggregate the metrics in the cloudprober and configures \u0026ldquo;op_latency_ms\u0026rdquo; as a distribution metric with explicit buckets. Cloudprober will now build cumulative distributions using for these metrics. We can import this data in Stackdriver or Prometheus and get the percentiles of the \u0026ldquo;get\u0026rdquo; and \u0026ldquo;set\u0026rdquo; latencies. Following screenshot shows the grafana dashboard built using these metrics.\n\nServer Mode The probe that we created above forks out a new redis_probe process for every probe cycle. This can get expensive if probe frequency is high and the process is big (e.g. a Java binary). Also, what if you want to keep some state across probes, for example, lets say you want to monitor performance over HTTP/2 where you keep using the same TCP connection for multiple HTTP requests. A new process every time makes keeping state impossible.\nExternal probe's server mode provides a way to run the external probe process in daemon mode. Cloudprober communicates with this process over stdout/stdin (connected with OS pipes), using serialized protobuf messages. Cloudprober comes with a serverutils package that makes it easy to build external probe servers in Go.\nPlease see the code at examples/external/redis_probe.go for server mode implementation of the above probe. Here is the corresponding cloudprober config to run this probe in server mode: examples/external/cloudprober_server.cfg.\nIn server mode, if external probe process dies for reason, it's restarted by Cloudprober.\n","permalink":"https://cloudprober.org/how-to/external-probe/","tags":null,"title":"External Probe"},{"categories":null,"contents":"Cloudprober's main task is to run probes. A probe executes something, usually against a set of targets, to verify that the systems are working as expected from consumers\u0026rsquo; point of view. For example, an HTTP probe executes an HTTP request against a web server to verify that the web server is available. Cloudprober probes run repeatedly at a configured interval and export probe results as a set of metrics.\nA probe is defined as a set of the following fields:\n   Field Description     type Probe type, for example: HTTP, PING or UDP   name Probe name. Each probe should have a unique name.   interval_msec How often to run the probe (in milliseconds).   timeout_msec Probe timeout (in milliseconds).   targets Targets to run probe against.   validator Probe validators, further explained here.   \u0026lt;type\u0026gt;_probe Probe type specific configuration.    Please take a look at the ProbeDef protobuf for further details on various fields and options. All probe types export following metrics at a minimum:\n   Metric Description     total Total number of probes.   success Number of successful probes. Deficit between total and success indicates failures.   latency Cumulative probe latency (by default in microseconds). Latency can also be configured to be a distribution (histogram) metric through a config option (latency_distribution). By default it's just the sum of the latencies observed so far. Average latency can be computed using rate(latency) / rate(success).    Probe Types Cloudprober has built-in support for the following probe types:\n Ping HTTP UDP DNS External  More probe types can be added through cloudprober extensions (to be documented).\nPing Code | Config options\nPing probe type implements a fast ping prober, that can probe hundreds of targets in parallel. Probe results are reported as number of packets sent (total), received (success) and round-trip time (latency). It supports raw sockets (requires root access) as well as datagram sockets for ICMP (doesn't require root access).\nICMP datagram sockets are not enabled by default on most Linux systems. You can enable them by running the following command: sudo sysctl -w net.ipv4.ping_group_range=\u0026quot;0 5000\u0026quot;\nHTTP Code | Config options\nHTTP probe is be used to send HTTP(s) requests to a target and verify that a response is received. Apart from the core probe metrics (total, success, and latency), HTTP probes also export a map of response code counts. Requests are marked as failed if there is a timeout.\n SSL Certificate Expiry: If the target serves a SSL Certificate, cloudprober will walk the certificate chain and export the earliest expiry time in seconds as a metric. The metric is named ssl_earliest_cert_expiry_sec, and will only be exported when the expiry time in seconds is a positive number.  UDP Code | Config options\nUDP probe sends a UDP packet to the configured targets. UDP probe (and all other probes that use ports) provides more coverage for the network elements on the data path as most packet forwarding elements use 5-tuple hashing and using a new source port for each probe ensures that we hit different network element each time.\nDNS Code | Config options\nDNS probe type is implemented in a similar way as other probes except for that it sends DNS requests to the target.\nExternal Code | Config options\nExternal probe type allows running arbitrary probes through cloudprober. For an external probe, actual probe logic resides in an external program; cloudprober only manages the execution of that program and provides a way to export that data through the standard channel.\nExternal probe can be configured in two modes:\n  ONCE: In this mode, an external program is executed for each probe run. Exit status of the program determines the success or failure of the probe. External probe can optionally be configured to interpret external program's output as metrics. This is a simple model but it doesn't allow the external program to maintain state and multiple forks can be expensive depending on the frequency of the probes.\n  SERVER: In this mode, external program is expected to run in server mode. Cloudprober automatically starts the external program if it's not running at the time of the probe execution. Cloudprober and external probe process communicate with each other over stdin/stdout using protobuf messages defined in probes/external/proto/server.proto.\n  ","permalink":"https://cloudprober.org/concepts/probe/","tags":null,"title":"Probe"},{"categories":null,"contents":" \nNOTE: Cloudprober's active development has moved to github.com/cloudprober/cloudprober from github.com/google/cloudprober.\nCloudprober is a monitoring software that makes it super-easy to monitor availability and performance of various components of your system. Cloudprober employs the \u0026ldquo;active\u0026rdquo; monitoring model. It runs probes against (or on) your components to verify that they are working as expected. For example, it can run a probe to verify that your frontends can reach your backends. Similarly it can run a probe to verify that your in-Cloud VMs can actually reach your on-premise systems. This kind of monitoring makes it possible to monitor your systems\u0026rsquo; interfaces regardless of the implementation and helps you quickly pin down what's broken in your system.\nFeatures  Automated target discovery for Cloud targets. GCE and Kubernetes are supported out-of-the-box, other Cloud providers can be added easily. Integration with open source monitoring stack of Prometheus and Grafana. Cloudprober exports probe results as counter based metrics that work well with Prometheus and Grafana. Out of the box, config based integration with popular monitoring systems: Prometheus, DataDog, PostgreSQL, StackDriver, CloudWatch. Fast and efficient built-in implementations for the most common types of checks: PING (ICMP), HTTP, UDP, DNS. Especially PING and UDP probes are implemented in such a way that thousands of hosts can be probed with minimal resources. Arbitrary, complex probes can be run through the external probe type. For example, you could write a simple script to insert and delete a row in your database, and execute this script through the \u0026lsquo;EXTERNAL\u0026rsquo; probe type. Standard metrics - total, success, latency. Latency can be configured to be a distribution (histogram) metric, allowing calculations of percentiles. Strong focus on ease of deployment. Cloudprober is written entirely in Go, and compiles into a static binary. It can be easily deployed, either as a standalone binary or through docker containers. Thanks to the automated, continuous, target discovery, there is usually no need to re-deploy or re-configure cloudprober in response to most of the changes. Low footprint. Cloudprober docker image is small, containing just the statically compiled binary and it takes very little CPU and RAM to run even a large number of probes. Extensible architecture. Cloudprober can be easily extended along most of the dimensions. Adding support for other Cloud targets, monitoring systems and even a new probe type, is straight-forward and fairly easy.  Getting Started Visit Getting Started page to get started with Cloudprober.\nFeedback We'd love to hear your feedback. If you're using Cloudprober, would you please mind sharing how you use it by adding a comment here. It will be a great help in planning Cloudprober's future progression.\nJoin Cloudprober Slack or Github discussions for questions and discussion about Cloudprober.\n","permalink":"https://cloudprober.org/_readme/","tags":null,"title":""},{"categories":null,"contents":"Installation   From Source\nIf you have Go 1.9 or higher installed and GOPATH environment variable properly set up, you can download and install cloudprober using the following commands:\ngo get github.com/cloudprober/cloudprober GOBIN=$GOPATH/bin go install $GOPATH/src/github.com/cloudprober/cloudprober/cmd/cloudprober.go   Pre-built Binaries\nYou can download pre-built binaries for Linux, MacOS and Windows from the project's releases page.\n(See this page for how to download the unreleased binaries.)\n  Docker Image You can download and run the latest docker image using the following command:\ndocker run --net host cloudprober/cloudprober # Note: \u0026#34;--net host\u0026#34; provides better network performance and makes port # forwarding management easier, but is not required.   Configuration Without any config, cloudprober will run only the \u0026ldquo;sysvars\u0026rdquo; module (no probes) and write metrics to stdout in cloudprober's line protocol format (to be documented). It will also start a Prometheus exporter at: http://localhost:9313 (you can change the default port through the environment variable CLOUDPROBER_PORT and the default listening address through the environment variable CLOUDPROBER_HOST).\nSince sysvars variables are not very interesting themselves, lets add a simple config that probes Google's homepage:\n# Write config to a file in /tmp cat \u0026gt; /tmp/cloudprober.cfg \u0026lt;\u0026lt;EOF probe { name: \u0026#34;google_homepage\u0026#34; type: HTTP targets { host_names: \u0026#34;www.google.com\u0026#34; } interval_msec: 5000 # 5s timeout_msec: 1000 # 1s } EOF This config adds an HTTP probe that accesses the homepage of the target \u0026ldquo;www.google.com\u0026quot; every 5s with a timeout of 1s. Cloudprober configuration is specified in the text protobuf format, with config schema described by the proto file: config.proto.\nAssuming that you saved this file at /tmp/cloudprober.cfg (following the command above), you can have cloudprober use this config file using the following command line:\n./cloudprober --config_file /tmp/cloudprober.cfg You can have the standard docker image use this config using the following command:\ndocker run --net host -v /tmp/cloudprober.cfg:/etc/cloudprober.cfg \\  cloudprober/cloudprober Note: While running on GCE, cloudprober config can also be provided through a custom metadata attribute: cloudprober_config.\nVerification One quick way to verify that cloudprober got the correct config is to access the URL http://localhost:9313/config (through cURL or in browser). It returns the config that cloudprober is using. You can also look at its current status at the URL (replace localhost by the actual hostname if not running locally): http://localhost:9313/status.\nYou should be able to see the generated metrics at http://localhost:9313/metrics (prometheus format) and the stdout (cloudprober format):\ncloudprober 15.. 1500590520 labels=ptype=http,probe=google-http,dst=.. total=17 success=17 latency=180835 cloudprober 15.. 1500590530 labels=ptype=sysvars,probe=sysvars hostname=\u0026#34;manugarg-ws\u0026#34; uptime=100 cloudprober 15.. 1500590530 labels=ptype=http,probe=google-http,dst=.. total=19 success=19 latency=211644 This information is good for debugging monitoring issues, but to really make sense of this data, you'll need to feed this data to another monitoring system like Prometheus or StackDriver (see Surfacers for more details). Lets set up a Prometheus and Grafana stack to make pretty graphs for us.\nRunning Prometheus Download prometheus binary from its release page. You can use a config like the following to scrape a cloudprober instance running on the same host.\n# Write config to a file in /tmp cat \u0026gt; /tmp/prometheus.yml \u0026lt;\u0026lt;EOF scrape_configs: - job_name: \u0026#39;cloudprober\u0026#39; scrape_interval: 10s static_configs: - targets: [\u0026#39;localhost:9313\u0026#39;] EOF # Start prometheus: ./prometheus --config.file=/tmp/prometheus.yml Prometheus provides a web interface at http://localhost:9090. You can explore probe metrics and build useful graphs through this interface. All probes in cloudprober export at least 3 counters:\n total: Total number of probes. success: Number of successful probes. Difference between total and success indicates failures. latency: Total (cumulative) probe latency.  Using these counters, probe failure ratio and average latency can be calculated as:\nfailure_ratio = (rate(total) - rate(success)) / rate(total) avg_latency = rate(latency) / rate(success) Assuming that prometheus is running at localhost:9090, graphs depicting failure ratio and latency over time can be accessed in prometheus at: this url . Even though prometheus provides a graphing interface, Grafana provides much richer interface and has excellent support for prometheus.\nGrafana Grafana is a popular tool for building monitoring dashboards. Grafana has native support for prometheus and thanks to the excellent support for prometheus in Cloudprober itself, it's a breeze to build Grafana dashboards from Cloudprober's probe results.\nTo get started with Grafana, follow the Grafana-Prometheus integration guide.\n","permalink":"https://cloudprober.org/getting-started/","tags":null,"title":"Getting Started"}]
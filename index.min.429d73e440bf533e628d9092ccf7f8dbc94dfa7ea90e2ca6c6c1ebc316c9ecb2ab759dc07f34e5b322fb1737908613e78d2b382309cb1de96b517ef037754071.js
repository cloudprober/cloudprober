var suggestions=document.getElementById("suggestions"),search=document.getElementById("search");search!==null&&document.addEventListener("keydown",inputFocus);function inputFocus(e){e.ctrlKey&&e.key==="/"&&(e.preventDefault(),search.focus()),e.key==="Escape"&&(search.blur(),suggestions.classList.add("d-none"))}document.addEventListener("click",function(e){var t=suggestions.contains(e.target);t||suggestions.classList.add("d-none")}),document.addEventListener("keydown",suggestionFocus);function suggestionFocus(e){const s=suggestions.classList.contains("d-none");if(s)return;const t=[...suggestions.querySelectorAll("a")];if(t.length===0)return;const n=t.indexOf(document.activeElement);if(e.key==="ArrowUp"){e.preventDefault();const s=n>0?n-1:0;t[s].focus()}else if(e.key==="ArrowDown"){e.preventDefault();const s=n+1<t.length?n+1:n;t[s].focus()}}(function(){var e=new FlexSearch.Document({tokenize:"forward",cache:100,document:{id:"id",store:["href","title","description"],index:["title","description","content"]}});e.add({id:0,href:"/docs/overview/",title:"Get Started",description:"",content:""}),e.add({id:1,href:"/docs/how-to/",title:"How To",description:"",content:""}),e.add({id:2,href:"/docs/about/",title:"About Cloudprober",description:`Posted\u0026nbsp;on\u0026nbsp;Oct 05, 2023 by Manu Garg\u0026nbsp;\u0026hyphen;\u0026nbsp;3 min\u0026nbsp;read Origin #I started building Cloudprober in 2016, while I was at Google, leading the Cloud Networking SRE team there. Google Cloud was just beginning to grow big, and we were still grappling with some early growth issue. Our biggest problem was that our customers were discovering problems before us, which resulted in bad experience for our customers and huge time sink for my team in debugging those issues.`,content:`Posted\u0026nbsp;on\u0026nbsp;Oct 05, 2023 by Manu Garg\u0026nbsp;\u0026hyphen;\u0026nbsp;3 min\u0026nbsp;read Origin #I started building Cloudprober in 2016, while I was at Google, leading the Cloud Networking SRE team there. Google Cloud was just beginning to grow big, and we were still grappling with some early growth issue. Our biggest problem was that our customers were discovering problems before us, which resulted in bad experience for our customers and huge time sink for my team in debugging those issues.1.
Google\u0026rsquo;s existing monitoring tools didn\u0026rsquo;t work well in Cloud, necessitating the need to build things from ground up. And since probers are the cornerstone of monitoring and reliability at Google2, that\u0026rsquo;s where we decided to start. Thus began the journey of Cloudprober.
Even though the primary goal of Cloudprober at that time was to discover and alert on Cloud Networking availability and performance problems, we decided to develop it as a generic prober that could be used to monitor a wide variety of systems and services. We also decided to make Cloudprober open source so that a wider community could trust it, contribute to it, and run it on their own systems.
Scale, Efficiency #For scales as big as Google Cloud, horizontal scalability and efficiency become critical requirements, and for a monitoring software to be useful reliability is super important as well. Keeping these requirements in mind, our goal for Cloudprober was for it to be able to reliably monitor 100s of 1000s of endpoints (IPs, Ports, HTTP/S URLs, etc) from each instance, while keeping the resource requirements and management overhead very low3.
Cloudprober maximizes resources utilization by relying heavily on Go concurrency (resource efficiency), supports probing large number of targets in parallel at a high frequency (each instance does more), minimizes the need of frequent updates by supporting dynamic targets discovery (ease of management), has native implementations for common probe types (efficiency), and so on.
Beyond Google and Open-Source #We open-sourced Cloudprober in 2017. That brought in a new phase in its evolution. We added many features over time to make it more useful to the wider community, such as first-class Kubernetes support, a built-in probe status UI, PostgreSQL and Cloudwatch surfacers, OAuth support, Validators, and most recently, built-in alerting capability.
We used the same codebase for the internal and open-source versions, which was more work but it created a huge advantage \u0026ndash; our own extensive internal deployment provided a continuous testing platform for Cloudprober, particularly for its scalability and performance aspects, while we added all these features.
Move away from Google Github #I left Google in Nov 2021. To keep working on Cloudprober independently, I moved Cloudprober\u0026rsquo;s Github repository from github.com/google/cloudprober to github.com/cloudprober/cloudprober. This was a disruptive move and we lost a lot of Github stars in the process (1.4k - ðŸ˜ƒ), but overall it was a good move as Cloudprober has grown much faster after becoming independent.
While I can\u0026rsquo;t say this authoritatively now as I don\u0026rsquo;t work there anymore, from what I know, Google still uses Cloudprober, in fact, even more widely now.
Growth and stability #Throughout its journey, Cloudprober has continuously adapted and expanded to meet the evolving needs of its users4. To ensure that Cloudprober thrives and evolves robustly, we\u0026rsquo;ve been very diligent that it grows in a structured way, a commitment we\u0026rsquo;ll uphold in future as well.
A customer-reported infrastructure issue is much harder to debug than an issue discovered by your own monitoring.\u0026#160;\u0026#x21a9;\u0026#xfe0e;
Almost all of Google\u0026rsquo;s systems rely on probers to detect customer facing problems.\u0026#160;\u0026#x21a9;\u0026#xfe0e;
Hostinger was able to probe 1.8M targets using a single instance: blog.\u0026#160;\u0026#x21a9;\u0026#xfe0e;
I think it\u0026rsquo;s an essential trait for any software. Software that don\u0026rsquo;t evolve with time wither away.\u0026#160;\u0026#x21a9;\u0026#xfe0e;
`}),e.add({id:3,href:"/docs/how-to/alerting/",title:"Alerting",description:`You can configure Cloudprober to send alerts on probe failures. Alerts are configured per probe and each probe can have multiple alerts with independent configuration. Alert configuration consists of mainly two parts:
Alert condition Notification config Alert Condition #Alert condition is defined in terms of number of failures (failures) out of a number of attempts (total). For example, if alert condition is specified as: condition {failures: 3, total: 5}, an alert will be triggered if 3 probes have failed out of the last 5 attempts.`,content:`You can configure Cloudprober to send alerts on probe failures. Alerts are configured per probe and each probe can have multiple alerts with independent configuration. Alert configuration consists of mainly two parts:
Alert condition Notification config Alert Condition #Alert condition is defined in terms of number of failures (failures) out of a number of attempts (total). For example, if alert condition is specified as: condition {failures: 3, total: 5}, an alert will be triggered if 3 probes have failed out of the last 5 attempts.
If no condition is specified, both failures and total are assumed to be 1, i.e., alert will trigger on the first probe failure itself.
Alert condition definition lets you take care of both the cases: continuous failures and sporadic failures. For example, if you run a probe every 30s and alert condition is {failures: 4, total: 10}, alert will trigger either after 2 minutes of consecutive failures, or if probe has failed 4 times in last 5 minutes.
More examples and explanation:
F=failure, S=success
condition { failure: 3 } or condition { failures: 3, total: 3} Trigger an alert on 3 consecutive failures. A pattern like F F F will immediately trigger an alert, but S S F F S S F F S S will not, even though failures are happening quite regularly. You could catch the second pattern by configuring the alert condition as {failures: 3, total: 5}.
condition { failure: 3, total: 6 }: Alert will trigger on 3 failures in the last 6 attempts. A pattern like F F F will immediately trigger an alert, so if probe interval is 10s, you\u0026rsquo;ll get an alert within 20-30s of incident starting. A pattern like F S F S S F or F F S S S F will also trigger an alert.
Alerts Dashboard #Cloudprober comes with an alerts dashboard that you can access at the /alerts URL. Alerts dashboard shows currently firing and 20 historical alerts.
Notifications #When you add alerts, you\u0026rsquo;d probably also want to be notified when they fire. You can do that using the notify config block:
# This example is in YAML format. You can use the original textpb format too. # See https://cloudprober.org/docs/config/alerting/cloudprober_alerting_AlertConf probe: ... alert: notify: pager_duty: routing_key: \u0026quot;...\u0026quot; slack: webhook_url: \u0026quot;...\u0026quot; Above example configures two notifications: PagerDuty \u0026amp; Slack. Cloudprober currently supports the following notification targets:
Email PagerDuty Opsgenie Slack Command HTTP Configuration documentation (linked above) has more details on each of them.
Notification Fields #You can customize the information included in the alert notification. The following table shows the top-level notification parameters, corresponding config fields, and their default values.
Info Configuration Field Default Dashboard URL dashboard_url_template http://localhost:9313/status?probe=@probe@ Playbook URL playbook_url_template \u0026quot;\u0026quot; Summary summary_template Cloudprober alert \u0026ldquo;@alert@\u0026rdquo; for \u0026ldquo;@target@\u0026rdquo; Details details_template Cloudprober alert \u0026ldquo;@alert@\u0026rdquo; for \u0026ldquo;@target@\u0026rdquo;:Failures: @failures@ out of @total@ probesFailing since: @since@
Probe: @probe@ Dashboard: @dashboard_url@ Playbook: @playbook_url@ As you see here, you can embed alert information in notifications using placeholders like @field@. For example if you host your alert playbooks at https://playbook/, you can configure playbook_url to be https://playbook/@alert@. Cloudprober supports the following alert fields placeholders:
Placeholder Alert field @alert@ Alert name. Same as probe name if alert name is not configured @probe@ Probe name @target@ Target name @target.label.\u0026lt;label\u0026gt;@ Target label value, e.g. if target has a label: env=prod, @target.label.env@ will be replaced with prod @since@ Alert start time @failure@ Failure count that caused the alert @total@ Total number of probes @target_ip@ Target IP if available. It only works for targets discovered by Cloudprober @dashboard_url@, @playbook_url@, @summary@, @details@ See the table above. `}),e.add({id:4,href:"/docs/",title:"Docs",description:"",content:""}),e.add({id:5,href:"/docs/how-to/external-probe/",title:"External Probe",description:`External probe type allows you to run arbitrary, complex probes through Cloudprober. An external probe runs an independent external program for actual probing. Cloudprober calculates probe metrics based on program\u0026rsquo;s exit status and time elapsed in execution.
Cloudprober also allows external programs to provide additional metrics. Every message sent to stdout will be parsed as a new metrics to be emitted. For general logging you can use another I/O stream like stderr.`,content:`External probe type allows you to run arbitrary, complex probes through Cloudprober. An external probe runs an independent external program for actual probing. Cloudprober calculates probe metrics based on program\u0026rsquo;s exit status and time elapsed in execution.
Cloudprober also allows external programs to provide additional metrics. Every message sent to stdout will be parsed as a new metrics to be emitted. For general logging you can use another I/O stream like stderr.
Sample Probe #To understand how it works, lets create a sample probe that sets and gets a key in a redis server. Here is the main function of such a probe:
func main() { var client redis.Client var key = \u0026quot;hello\u0026quot; startTime := time.Now() client.Set(key, []byte(\u0026quot;world\u0026quot;)) fmt.Printf(\u0026quot;op_latency_ms{op=set} %f\\n\u0026quot;, float64(time.Since(startTime).Nanoseconds())/1e6) startTime = time.Now() val, _ := client.Get(\u0026quot;hello\u0026quot;) log.Printf(\u0026quot;%s=%s\u0026quot;, key, string(val)) fmt.Printf(\u0026quot;op_latency_ms{op=get} %f\\n\u0026quot;, float64(time.Since(startTime).Nanoseconds())/1e6) } (Full listing: https://github.com/cloudprober/cloudprober/blob/master/examples/external/redis_probe.go) This program sets and gets a key in redis and prints the time taken for both operations. op_latency_ms{op=get|set} will be emitted as metrics. You could also define your own labels using this format:
Cloudprober can use this program as an external probe, to verify the availability and performance of the redis server. This program assumes that redis server is running locally, at its default port. For the sake of demonstration, lets run a local redis server (you can also easily modify this program to use a different server.)
#!bash OS=\$(uname) [[ \u0026quot;\$OS\u0026quot; == \u0026quot;Darwin\u0026quot; ]] \u0026amp;\u0026amp; brew install redis [[\u0026quot;\$OS\u0026quot; == \u0026quot;Linux\u0026quot;]] \u0026amp;\u0026amp; sudo apt install redis Let\u0026rsquo;s compile our probe program (redis_probe.go) and verify that it\u0026rsquo;s working as expected:
#!bash CGO_ENABLED=0 go build -ldflags â€œ-extldflags=-staticâ€ examples/external/redis_probe.go ./redis_probe 2022/02/24 12:39:45 hello=world op_latency_ms{op=set} 22.656588 op_latency_ms{op=get} 2.173560 Configuration #Here is the external probe configuration that makes use of this program:
Full example in examples/external/cloudprober.cfg.
# Run an external probe that executes a command from the current working # directory. probe { name: \u0026quot;redis_probe\u0026quot; type: EXTERNAL targets { dummy_targets {} } external_probe { mode: ONCE command: \u0026quot;./redis_probe\u0026quot; } } Note: To pass target information to your external program as arguments use the @label@ notation. Supported fields are: @target@, @address@, @port@, @probe@, and target labels like @target.label.fqdn@.
command: \u0026quot;./redis_probe\u0026quot; -host=@address@ -port=@port@ Running it through cloudprober, you\u0026rsquo;ll see the following output:
# Launch cloudprober cloudprober --config_file=cloudprober.cfg cloudprober 1519..0 1519583408 labels=ptype=external,probe=redis_probe,dst= success=1 total=1 latency=12143.765 cloudprober 1519..1 1519583408 labels=ptype=external,probe=redis_probe,dst=,op=get op_latency_ms=0.516 get_latency_ms=0.491 cloudprober 1519..2 1519583410 labels=ptype=external,probe=redis_probe,dst= success=2 total=2 latency=30585.915 cloudprober 1519..3 1519583410 labels=ptype=external,probe=redis_probe,dst=,op=set op_latency_ms=0.636 get_latency_ms=0.994 cloudprober 1519..4 1519583412 labels=ptype=external,probe=redis_probe,dst= success=3 total=3 latency=42621.871 You can import this data in prometheus following the process outlined at: Running Prometheus. Before doing that, let\u0026rsquo;s make it more interesting.
Distributions #How nice will it be if we could find distribution of the set and get latency. If tail latency was too high, it could explain the random timeouts in your application. Fortunately, it\u0026rsquo;s very easy to create distributions in Cloudprober. You just need to add the following section to your probe definition:
Full example in examples/external/cloudprober_aggregate.cfg.
# Run an external probe and aggregate metrics in cloudprober. ... output_metrics_options { aggregate_in_cloudprober: true # Create distributions for get_latency_ms and set_latency_ms. dist_metric { key: \u0026quot;op_latency_ms\u0026quot; value: { explicit_buckets: \u0026quot;0.1,0.2,0.4,0.6,0.8,1.0,2.0\u0026quot; } } } This configuration adds options to aggregate the metrics in the cloudprober and configures \u0026ldquo;op_latency_ms\u0026rdquo; as a distribution metric with explicit buckets. Cloudprober will now build cumulative distributions using for these metrics. We can import this data in Stackdriver or Prometheus and get the percentiles of the \u0026ldquo;get\u0026rdquo; and \u0026ldquo;set\u0026rdquo; latencies. Following screenshot shows the grafana dashboard built using these metrics.
Server Mode #The probe that we created above forks out a new redis_probe process for every probe cycle. This can get expensive if probe frequency is high and the process is big (e.g. a Java binary). Also, what if you want to keep some state across probes, for example, lets say you want to monitor performance over HTTP/2 where you keep using the same TCP connection for multiple HTTP requests. A new process every time makes keeping state impossible.
External probe\u0026rsquo;s server mode provides a way to run the external probe process in daemon mode. Cloudprober communicates with this process over stdout/stdin (connected with OS pipes), using serialized protobuf messages. Cloudprober comes with a serverutils package that makes it easy to build external probe servers in Go.
Please see the code at examples/external/redis_probe.go for server mode implementation of the above probe. Here is the corresponding cloudprober config to run this probe in server mode: examples/external/cloudprober_server.cfg.
In server mode, if external probe process dies for reason, it\u0026rsquo;s restarted by Cloudprober.
`}),e.add({id:6,href:"/docs/how-to/k8s_targets/",title:"Kubernetes Targets",description:`Cloudprober supports dynamic discovery of Kubernetes resources (e.g. pods, endpoints, ingresses, etc) through the targets type k8s.
For example, the following config adds an HTTP probe for the endpoints named cloudprober (equivalent to running kubectl get ep cloudprober).
probe { name: \u0026quot;pod-to-endpoints\u0026quot; type: HTTP targets { # Equivalent to kubectl get ep cloudprober k8s { endpoints: \u0026quot;cloudprober\u0026quot; } } # Note that the following http_probe automatically uses target's discovered # port.`,content:`Cloudprober supports dynamic discovery of Kubernetes resources (e.g. pods, endpoints, ingresses, etc) through the targets type k8s.
For example, the following config adds an HTTP probe for the endpoints named cloudprober (equivalent to running kubectl get ep cloudprober).
probe { name: \u0026quot;pod-to-endpoints\u0026quot; type: HTTP targets { # Equivalent to kubectl get ep cloudprober k8s { endpoints: \u0026quot;cloudprober\u0026quot; } } # Note that the following http_probe automatically uses target's discovered # port. http_probe { relative_url: \u0026quot;/status\u0026quot; } } Supported Resource and Filters #Cloudprober supports discovery for the following k8s resources:
Services Endpoints Pods Ingresses Filters #You can filter k8s resources using the following options:
name: (regex) Resource name filter. It can be a regex. Example: # Endpoints with names ending in \u0026quot;service\u0026quot; targets { k8s { endpoints: \u0026quot;.*-service\u0026quot; } } namespace: Namespace filter. Example: # Ingresses in \u0026quot;prod\u0026quot; namespace, ending in \u0026quot;lb\u0026quot; targets { k8s { namespace: \u0026quot;prod\u0026quot; ingresses: \u0026quot;.*-lb\u0026quot; } } # Kube-DNS service targets { k8s { namespace: \u0026quot;kube-system\u0026quot; services: \u0026quot;kube-dns\u0026quot; } } labelSelector: Label based selector. It can be repeated, and works similar to the kubectl\u0026rsquo;s \u0026ndash;selector/-l flag. Example: targets { k8s { pods: \u0026quot;.*\u0026quot; labelSelector: \u0026quot;k8s-app\u0026quot; # k8a-app label exists labelSelector: \u0026quot;role=frontend\u0026quot; # label \u0026quot;role\u0026quot; is set to \u0026quot;frontend\u0026quot; labelSelector: \u0026quot;!no-monitoring\u0026quot; # label \u0026quot;no-monitoring is not set\u0026quot; } } portFilter: (regex) Filter resources by port name or number (if port name is not set). This is useful for resources like endpoints and services, where each resource may have multiple ports. Example: targets { k8s { endpoints: \u0026quot;.*-service\u0026quot; portFilter: \u0026quot;http-.*\u0026quot; } } Cluster Resources Access #Note: If you\u0026rsquo;ve installed Cloudprober using Helm Chart, this step is automatically taken care of.
Cloudprober discovers k8s resources using kubernetes APIs. It assumes that we are interested in the cluster we are running it in, and uses in-cluster config to talk to the kubernetes API server. For this set up to work, we need to give our container read-only access to kubernetes resources:
# Define a ClusterRole (resource-reader) for read-only access to the cluster # resources and bind this ClusterRole to the default service account. cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: v1 kind: ServiceAccount metadata: name: cloudprober --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \u0026quot;true\u0026quot; name: resource-reader namespace: default rules: - apiGroups: [\u0026quot;\u0026quot;] resources: [\u0026quot;*\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;] - apiGroups: - extensions - \u0026quot;networking.k8s.io\u0026quot; # k8s 1.14+ resources: - ingresses - ingresses/status verbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;] --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: default-resource-reader namespace: default subjects: - kind: ServiceAccount name: cloudprober namespace: default roleRef: kind: ClusterRole name: resource-reader apiGroup: rbac.authorization.k8s.io EOF This will create a new service account cloudprober and will give it read-only access to the cluster resources.
`}),e.add({id:7,href:"/docs/how-to/run-on-kubernetes/",title:"Running On Kubernetes",description:`Kubernetes is a popular platform for running containers, and Cloudprober container runs on Kubernetes right out of the box. This document shows how you can run Cloudprober on kubernetes, use ConfigMap for config, and discover kubernetes targets automatically.
â“˜ If you use helm charts for k8s installations, Cloudprober helm chart provides the most convenient way to run Cloudprober on k8s.ConfigMap #In Kubernetes, a convenient way to provide config to containers is to use config maps.`,content:`Kubernetes is a popular platform for running containers, and Cloudprober container runs on Kubernetes right out of the box. This document shows how you can run Cloudprober on kubernetes, use ConfigMap for config, and discover kubernetes targets automatically.
â“˜ If you use helm charts for k8s installations, Cloudprober helm chart provides the most convenient way to run Cloudprober on k8s.ConfigMap #In Kubernetes, a convenient way to provide config to containers is to use config maps. Let\u0026rsquo;s create a config that specifies a probe to monitor \u0026ldquo;google.com\u0026rdquo;.
probe { name: \u0026quot;google-http\u0026quot; type: HTTP targets { host_names: \u0026quot;www.google.com\u0026quot; } http_probe {} interval_msec: 15000 timeout_msec: 1000 } Save this config in cloudprober.cfg, create a config map using the following command:
kubectl create configmap cloudprober-config \\ --from-file=cloudprober.cfg=cloudprober.cfg If you change the config, you can update the config map using the following command:
kubectl create configmap cloudprober-config \\ --from-file=cloudprober.cfg=cloudprober.cfg -o yaml --dry-run | \\ kubectl replace -f - Deployment Map #Now let\u0026rsquo;s add a deployment.yaml to add the config volume and cloudprober container:
apiVersion: apps/v1 kind: Deployment metadata: name: cloudprober spec: replicas: 1 selector: matchLabels: app: cloudprober template: metadata: annotations: checksum/config: \u0026quot;\${CONFIG_CHECKSUM}\u0026quot; labels: app: cloudprober spec: volumes: - name: cloudprober-config configMap: name: cloudprober-config containers: - name: cloudprober image: cloudprober/cloudprober command: [\u0026quot;/cloudprober\u0026quot;] args: [\u0026quot;--config_file\u0026quot;, \u0026quot;/cfg/cloudprober.cfg\u0026quot;, \u0026quot;--logtostderr\u0026quot;] volumeMounts: - name: cloudprober-config mountPath: /cfg ports: - name: http containerPort: 9313 --- apiVersion: v1 kind: Service metadata: name: cloudprober labels: app: cloudprober spec: ports: - port: 9313 protocol: TCP targetPort: 9313 selector: app: cloudprober type: NodePort Note that we added an annotation to the deployment spec; this annotation allows us to update the deployment whenever cloudprober config changes. We can update this annotation based on the local cloudprober config content, and update the deployment using the following one-liner:
# Update the config checksum annotation in deployment.yaml before running # kubectl apply. export CONFIG_CHECKSUM=\$(kubectl get cm/cloudprober-config -o yaml | sha256sum) \u0026amp;\u0026amp; \\ cat deployment.yaml | envsubst | kubectl apply -f - (Note: If you use Helm for Kubernetes deployments, Helm provides a more native way to include config checksums in deployments.)
Applying the above yaml file, should create a deployment with a service at port 9313:
\$ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE cloudprober 1/1 1 1 94m \$ kubectl get service cloudprober NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE cloudprober NodePort 10.31.249.108 \u0026lt;none\u0026gt; 9313:31367/TCP 94m Now you should be able to access various cloudprober URLs (/status for status,/config for config, /metrics for prometheus-format metrics) from within the cluster. For quick verification you can also set up a port forwarder and access these URLs locally at localhost:9313:
kubectl port-forward svc/cloudprober 9313:9313 Once you\u0026rsquo;ve verified that everything is working as expected, you can go on setting up metrics collection through prometheus (or stackdriver) in usual ways.
Kubernetes Targets #If you\u0026rsquo;re running on Kubernetes, you\u0026rsquo;d probably want to monitor Kubernetes resources (e.g. pods, endpoints, etc) as well. Cloudprober supports dynamic discovery of Kubernetes resources through the targets type k8s.
For example, the following config adds an HTTP probe for the endpoints named cloudprober (equivalent to running kubectl get ep cloudprober).
probe { name: \u0026quot;pod-to-endpoints\u0026quot; type: HTTP targets { # Equivalent to kubectl get ep cloudprober k8s { endpoints: \u0026quot;cloudprober\u0026quot; } } # Note that the following http_probe automatically uses target's discovered # port. http_probe { relative_url: \u0026quot;/status\u0026quot; } } See Kubernetes Targets for more details on Kubernetes targets.
Push Config Update #To push new cloudprober config to the cluster:
# Update the config map kubectl create configmap cloudprober-config \\ --from-file=cloudprober.cfg=cloudprober.cfg -o yaml --dry-run | \\ kubectl replace -f - # Update deployment export CONFIG_CHECKSUM=\$(kubectl get cm/cloudprober-config -o yaml | sha256sum) \u0026amp;\u0026amp; \\ cat deployment.yaml | envsubst | kubectl apply -f - Cloudprober should now start monitoring cloudprober endpoints. To verify:
# Set up port fowarding such that you can access cloudprober:9313 through # localhost:9313. kubectl port-forward svc/cloudprober 9313:9313 \u0026amp; # Check status curl localhost:9313/status # Check metrics (prometheus data format) curl localhost:9313/metrics If you\u0026rsquo;re running on GKE and have not disabled cloud logging, you\u0026rsquo;ll also see logs in Stackdriver Logging.
`}),e.add({id:8,href:"/docs/how-to/additional-labels/",title:"Additional Labels",description:`You can add additional labels to probe metrics using a probe-level field: additional_label. An additional label\u0026rsquo;s value can be static, or it can be determined at the run-time: from the environment that the probe is running in (e.g. GCE instance labels), or target\u0026rsquo;s labels.
Example config here demonstrates adding various types of additional labels to probe metrics. For this config (also listed below for quick rerefence):
if ingress target has label \u0026ldquo;fqdn:app.`,content:`You can add additional labels to probe metrics using a probe-level field: additional_label. An additional label\u0026rsquo;s value can be static, or it can be determined at the run-time: from the environment that the probe is running in (e.g. GCE instance labels), or target\u0026rsquo;s labels.
Example config here demonstrates adding various types of additional labels to probe metrics. For this config (also listed below for quick rerefence):
if ingress target has label \u0026ldquo;fqdn:app.example.com\u0026rdquo;, and prober is running in the GCE zone us-east1-c, and prober\u0026rsquo;s GCE instance has label env:prod. Probe metrics will look like the following:
total{probe=\u0026quot;my_ingress\u0026quot;,ptype=\u0026quot;http\u0026quot;,metrictype=\u0026quot;prober\u0026quot;,env=\u0026quot;prod\u0026quot;,src_zone=\u0026quot;us-east1-c\u0026quot;,host=\u0026quot;app.example.com\u0026quot;}: 90 success{probe=\u0026quot;my_ingress\u0026quot;,ptype=\u0026quot;http\u0026quot;,metrictype=\u0026quot;prober\u0026quot;,env=\u0026quot;prod\u0026quot;,src_zone=\u0026quot;us-east1-c\u0026quot;,host=\u0026quot;app.example.com\u0026quot;}: 80 probe { name: \u0026quot;my_ingress\u0026quot; type: HTTP targets { rds_targets { resource_path: \u0026quot;k8s://ingresses\u0026quot; filter { key: \u0026quot;namespace\u0026quot; value: \u0026quot;default\u0026quot; } } } # Static label additional_label { key: \u0026quot;metrictype\u0026quot; value: \u0026quot;prober\u0026quot; } # Label is configured at the run-time, based on the prober instance label (GCE). additional_label { key: \u0026quot;env\u0026quot; value: \u0026quot;{{.label_env}}\u0026quot; } # Label is configured at the run-time, based on the prober environment (GCE). additional_label { key: \u0026quot;src_zone\u0026quot; value: \u0026quot;{{.zone}}\u0026quot; } # Label is configured based on the target's labels. additional_label { key: \u0026quot;host\u0026quot; value: \u0026quot;@target.label.fqdn@\u0026quot; } http_probe {} } (Listing source: examples/additional_label/cloudprober.cfg)
Adding your own metrics #For external probes, Cloudprober also allows external programs to provide additional metrics. See External Probe for more details.
`}),e.add({id:9,href:"/docs/how-to/validators/",title:"Validators",description:`Validators allow you to run checks on the probe request output (if any). For example, you can specify if you expect the probe output to match a certain regex or return a certain status code (for HTTP). You can configure more than one validators and all validators should succeed for the probe to be marked as success.
probe { name: \u0026quot;google_homepage\u0026quot; type: HTTP targets { host_names: \u0026quot;www.google.com\u0026quot; } interval_msec: 10000 # Probe every 10s # This validator should succeed.`,content:`Validators allow you to run checks on the probe request output (if any). For example, you can specify if you expect the probe output to match a certain regex or return a certain status code (for HTTP). You can configure more than one validators and all validators should succeed for the probe to be marked as success.
probe { name: \u0026quot;google_homepage\u0026quot; type: HTTP targets { host_names: \u0026quot;www.google.com\u0026quot; } interval_msec: 10000 # Probe every 10s # This validator should succeed. validator { name: \u0026quot;status_code_2xx\u0026quot; http_validator { success_status_codes: \u0026quot;200-299\u0026quot; } } # This validator will fail, notice missing 'o' in our regex. validator { name: \u0026quot;gogle_re\u0026quot; regex: \u0026quot;gogle\u0026quot; } } (Full listing: https://github.com/cloudprober/cloudprober/blob/master/examples/validators/cloudprober_validator.cfg)
To make the debugging easier, validation failures are logged and exported as an independent map counter \u0026ndash; validation_failure, with validator key. For example, the above example will result in the following counters being exported after 5 runs:
total{probe=\u0026quot;google_homepage\u0026quot;,dst=\u0026quot;www.google.com\u0026quot;} 5 success{probe=\u0026quot;google_homepage\u0026quot;,dst=\u0026quot;www.google.com\u0026quot;} 0 validation_failure{validator=\u0026quot;status_code_2xx\u0026quot;,probe=\u0026quot;google_homepage\u0026quot;,dst=\u0026quot;www.google.com\u0026quot;} 0 validation_failure{validator=\u0026quot;gogle_re\u0026quot;,probe=\u0026quot;google_homepage\u0026quot;,dst=\u0026quot;www.google.com\u0026quot;} 5 Note that validator counter will not go up if probe fails for other reasons, for example web server timing out. That\u0026rsquo;s why you typically don\u0026rsquo;t want to alert only on validation failures. That said, in some cases, validation failures could be the only thing you\u0026rsquo;re interested in, for example, if you\u0026rsquo;re trying to make sure that a certain copyright is always present in your web pages or you want to catch data integrity issues in your network.
Let\u0026rsquo;s take a look at the types of validators you can configure.
Regex Validator #Regex validator simply checks for a regex in the probe request output. It works for all probe types except for UDP and UDP_LISTENER - these probe types don\u0026rsquo;t support any validators at the moment.
HTTP Validator #HTTP response validator works only for the HTTP probe type. You can currently use HTTP validator to define success and failure status codes (represented by success_status_codes and failure_stauts_codes in the config):
If failure_status_codes is defined and response status code falls within that range, validator is considered to have failed. If success_status_codes is defined and response status code does not fall within that range, validator is considered to have failed. If failure_header is defined and HTTP response include specified header and there are matching values, validator is considered to have failed. Leaving value_regex empty checks only for header name. If success_header is defined and HTTP response does not include specified header with matching values, validator is considered to have failed. Leaving value_regex empty checks only for header name. Data Integrity Validator #Data integrity validator is designed to catch the packet corruption issues in the network. We have a basic check that verifies that the probe output is made up purely of a pattern repeated many times over.
`}),e.add({id:10,href:"/docs/how-to/percentiles/",title:"Percentiles, Histograms, and Distributions",description:"Percentiles give you a deeper insight into how your system is behaving. For example, if your application\u0026rsquo;s response latency is very low 94 times out 100 but very high for the remaining 6 times, your average latency will still be low but it won\u0026rsquo;t be a great experience for your users. In other words, this is the case where your 95th percentile latency is high, even though your average and median (50th-%ile) latency is very low.",content:`Percentiles give you a deeper insight into how your system is behaving. For example, if your application\u0026rsquo;s response latency is very low 94 times out 100 but very high for the remaining 6 times, your average latency will still be low but it won\u0026rsquo;t be a great experience for your users. In other words, this is the case where your 95th percentile latency is high, even though your average and median (50th-%ile) latency is very low.
A typical way to measure percentiles from continuous monitoring data, which you may have to aggregate across various sources, is to use histograms (also called, distributions). In a histogram, you assign the incoming data points (samples) to pre-defined buckets. Each data point increases the count for the bucket that it falls into; data point itself is discarded after that. You can take a look at the bucket counts at any point of time and get an estimate of the percentiles. Histograms make it easy to aggregate data across multiple entities, for example, from probes running on multiple machines.
Following diagram shows distribution of latencies into 9 equal sized histogram buckets:
(Above diagram shows histogram for the following samples: 5.1, 6.2, 9.0, 12.1, 8.3, 9.7, 9.4, 10.3, 14.1, 11.2, 16.6, 9.9, 10.6, 14.1, 0.9, 7.1, 17.7)
Histograms in Cloudprober (Distributions) #Cloudprober uses a metric type called \u0026lsquo;distribution\u0026rsquo; to create and export histograms. Cloudprober supports creating distributions for probe latencies, and for metrics generated from external probe payloads. To create distributions, you have to specify how the data should be bucketed \u0026ndash; you can either explicitly specify all bucket bounds, or use exponential buckets type which generates bucket bounds from only a few variables.
Here is an example of using explicit buckets for latencies:
probe { name: \u0026quot;...\u0026quot; type: HTTP targets { host_names: \u0026quot;...\u0026quot; } latency_unit: \u0026quot;ms\u0026quot; latency_distribution { explicit_buckets: \u0026quot;0.01,0.1,0.15,0.2,0.25,0.35,0.5,0.75,1.0,1.5,2.0,3.0,4.0,5.0,10.0,15.0,20.0\u0026quot; } } Configuring distributions #As seen in the example above, for latencies you configure distribution at the probe level by adding a field called latency_distribution. Without this field, cloudprober exports only cumulative latencies. To create distributions from an external probe\u0026rsquo;s data, take a look at the external probe\u0026rsquo;s documentation.
Format for the distribution field is in turn defined in dist.proto.
// Dist defines a Distribution data type. message Dist { oneof buckets { // Comma-separated list of lower bounds, where each lower bound is a float // value. Example: 0.5,1,2,4,8. string explicit_buckets = 1; // Exponentially growing buckets ExponentialBuckets exponential_buckets = 2; } } // ExponentialBucket defines a set of num_buckets+2 buckets: // bucket[0] covers (âˆ’Inf, 0) // bucket[1] covers [0, scale_factor) // bucket[2] covers [scale_factor, scale_factor*base) // ... // bucket[i] covers [scale_factor*base^(iâˆ’2), scale_factor*base^(iâˆ’1)) // ... // bucket[num_buckets+1] covers [scale_factor*base^(num_bucketsâˆ’1), +Inf) // Note: Base must be at least 1.01. message ExponentialBuckets { optional float scale_factor = 1 [default = 1.0]; optional float base = 2 [default = 2]; optional uint32 num_buckets = 3 [default = 20]; } Percentiles and Heatmap #Now that we\u0026rsquo;ve configured cloudprober to generate distributions, how do we make use of this new information. This depends on the monitoring system (prometheus, stackdriver, postgres, etc) you\u0026rsquo;re exporting your data to.
Both prometheus and stackdriver support computing and plotting percentiles from the distributions data. Stackdriver can natively create heatmaps from distributions while for prometheus you need to use grafana to create heatmaps.
Stackdriver (Google Cloud Monitoring) #Stackdriver automatically shows percentile aggregator for distribution metrics in metrics explorer (example). You can also use Stackdriver MQL to create percentiles (see stackdriver documentation for other usages of MQL for cloudprober metrics):
fetch gce_instance | metric 'custom.googleapis.com/cloudprober/http/google_homepage/latency' | filter (resource.zone == 'us-central1-a') | align delta(1m) | every 1m | group_by [resource.zone], [value_latency_percentile: percentile(value.latency, 95)] Stackdriver has detailed documentation on charting distributions.
Prometheus #Cloudprober surfaces distributions to prometheus as prometheus metric type histogram. Here is an example of prometheus metrics page created by cloudprober:
# TYPE latency histogram latency_sum{ptype=\u0026quot;http\u0026quot;,probe=\u0026quot;my_probe\u0026quot;,dst=\u0026quot;hostA\u0026quot;} 77557.14022499947 1607766316442 latency_count{ptype=\u0026quot;http\u0026quot;,probe=\u0026quot;my_probe\u0026quot;,dst=\u0026quot;hostA\u0026quot;} 172150 1607766316442 latency_bucket{ptype=\u0026quot;http\u0026quot;,probe=\u0026quot;my_probe\u0026quot;,dst=\u0026quot;hostA\u0026quot;,le=\u0026quot;0.01\u0026quot;} 0 1607766316442 latency_bucket{ptype=\u0026quot;http\u0026quot;,probe=\u0026quot;my_probe\u0026quot;,dst=\u0026quot;hostA\u0026quot;,le=\u0026quot;0.1\u0026quot;} 0 1607766316442 ... ... latency_bucket{ptype=\u0026quot;http\u0026quot;,probe=\u0026quot;my_probe\u0026quot;,dst=\u0026quot;hostA\u0026quot;,le=\u0026quot;75\u0026quot;} 172150 1607766316442 latency_bucket{ptype=\u0026quot;http\u0026quot;,probe=\u0026quot;my_probe\u0026quot;,dst=\u0026quot;hostA\u0026quot;,le=\u0026quot;100\u0026quot;} 172150 1607766316442 latency_bucket{ptype=\u0026quot;http\u0026quot;,probe=\u0026quot;my_probe\u0026quot;,dst=\u0026quot;hostA\u0026quot;,le=\u0026quot;+Inf\u0026quot;} 172150 1607766316442 Fortunately there is already a plenty of good documentation on how to make use of histograms in prometheus and grafana:
Grafana blog on how to visualize prometheus histograms in grafana. Prometheus documentation on histrograms. More Resources #The Problem with Percentiles â€“ Aggregation brings Aggravation. Why percentiles don\u0026rsquo;t work the way you think. `}),e.add({id:11,href:"/docs/how-to/targets/",title:"Targets",description:`Cloudprober probes usually run against some targets1 to check those targets' status, such as an HTTP probe to your APIs servers, or PING/TCP probes to a third-party provider to verify network connectivity to them. Each probe can have multiple targets. If a probe has multiple targets, Cloudprober runs parallel probes for each target. This page further explains how targets work in Cloudprober.
Dynamically Discovered Targets #One of the core features of Cloudprober is the automatic and continuous discovery of targets.`,content:`Cloudprober probes usually run against some targets1 to check those targets' status, such as an HTTP probe to your APIs servers, or PING/TCP probes to a third-party provider to verify network connectivity to them. Each probe can have multiple targets. If a probe has multiple targets, Cloudprober runs parallel probes for each target. This page further explains how targets work in Cloudprober.
Dynamically Discovered Targets #One of the core features of Cloudprober is the automatic and continuous discovery of targets. This feature is especially important for the dynamic environments that today\u0026rsquo;s cloud based deployments make possible. For example in a kubernetes cluster the number of pods and their IPs can change on the fly, either in response to replica count changes or node failures. Automated targets discovery makes sure that we don\u0026rsquo;t have to reconfigure Cloudprober in response to such events.
Targets Configuration #Cloudprober provides multiple ways to configure targets for a probe.
Static targets #Static targets are the easiest and most straight-forward to configure:
probe { ... targets { host_names: \u0026quot;www.google.com,www.yahoo.com,cloudprober:9313\u0026quot; } .. } In the above config, probe will run against 3 hosts in parallel: www.google.com, www.yahoo.com, and cloudprober:9313 (yes, you can specify ports here for port-aware probes).
You can specify more detailed targets using the endpoints field. Using endpoints, you can even specify the URL directly in target definition; this method is particularly useful if you want to run an HTTP probe for multiple similar targets.
probe { type: HTTP ... targets { endpoint { # This will probe https://web.example.com/url1, target will show up as # \u0026quot;frontend_main\u0026quot; in metrics. name: \u0026quot;frontend_main\u0026quot; url: \u0026quot;https://web.example.com/url1\u0026quot; } endpoint { # This will probe http://cms.example.com, target will show up as # \u0026quot;cms.example.com\u0026quot; in metrics. name: \u0026quot;cms.example.com\u0026quot; } } .. } File based targets #You can define your targets in a file and refer to them in Cloudprober through that file. This file can be modified independently, and whenever that happens cloudprober will reload it automatically.
Example configuration:
targets { file_targets { file_path: \u0026quot;/var/run/cloudprober/vips.json\u0026quot; } } In the targets file, resources should be specified in a specific format. Here is an example of targets in JSON format:
{ \u0026quot;resource\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;switch-xx-1\u0026quot;, \u0026quot;ip\u0026quot;: \u0026quot;10.1.1.1\u0026quot;, \u0026quot;port\u0026quot;: 8080, \u0026quot;labels\u0026quot;: { \u0026quot;device_type\u0026quot;: \u0026quot;switch\u0026quot;, \u0026quot;cluster\u0026quot;: \u0026quot;xx\u0026quot; } }, { \u0026quot;name\u0026quot;: \u0026quot;switch-xx-2\u0026quot;, \u0026quot;ip\u0026quot;: \u0026quot;10.1.1.2\u0026quot;, \u0026quot;port\u0026quot;: 8081, \u0026quot;labels\u0026quot;: { \u0026quot;cluster\u0026quot;: \u0026quot;xx\u0026quot; } } ] } (You can also define targets in the textproto format: example. Full example with cloudprober.cfg: file_based_targets)
Even if you don\u0026rsquo;t intend to use the auto-reload feature of the file targets, they can still be quite useful over static targets as they allow you to specify additional details for targets. For example, specifying target\u0026rsquo;s IP address in the example above lets you tackle the case where you want to specify target\u0026rsquo;s name, let\u0026rsquo;s say for better identification or for HTTP requests to work, but don\u0026rsquo;t want to rely on DNS for resolving its IP address.
K8s targets #K8s targets are explained at Kubernetes Targets.
GCP targets #Since Cloudprober started at GCP, it\u0026rsquo;s no surprise that Cloudprober has great support for GCP targets. Cloudprober supports the following GCP resources:
GCE Instances Forwarding Rules (regional and global) Cloud pub/sub (list of hostnames over cloud pub/sub) TODO: Add more details on GCP targets.
Probe configuration through target fields #Field Probe Type Configuration port Port aware probes (HTTP, DNS, TCP, UDP, etc) If a target has an associated port, for example, a Kubernetes endpoint, it will automatically be used for probing unless a port has been explicitly configured in the probe. label:relative_url HTTP If an explicit relative URL is not set, HTTP probe will use relative_url label\u0026rsquo;s value if set. label:fqdn HTTP HTTP probe will use target\u0026rsquo;s fqdn label as the URL-host (host part of the URL) and Host header if available and if Host header has not been configured explicitly. Metrics #Target name: All metrics generated by Cloudprober have a dst label which is set to the target name. Target labels: See additional labels for how resource labels can be used to set additional labels on the metrics. Scaling targets discovery and other features #If you run a lot of Cloudprober instances with targets discovery, you may end up overwhelming the API servers, or running out of your API quota in case of Cloud resources. To avoid that, Cloudprober allows centralizing the targets discovery through the Resource Discovery Service (RDS) mechanism. See Resource Discovery Service for more details on that.
Other salient features of the cloudprober\u0026rsquo;s targets discovery:
Continuous discovery. We don\u0026rsquo;t just discover targets in the beginning, but keep refreshing them at a regular interval. Protection against the upstream provider failures. If refreshing of the targets fails during one of the refresh cycles, we continue using the existing set of targets. There are some cases where there is no explicit target, for example, you may run a probe to measure your CI system\u0026rsquo;s performance, or run a complex probe that touches many endpoints.\u0026#160;\u0026#x21a9;\u0026#xfe0e;
`}),e.add({id:12,href:"/docs/config/alerting/",title:"Alerting Config",description:"Configs:\u0026nbsp;\u0026nbsp;main | alerting | metrics | oauth | probes | rds | servers | surfacer | targets | tlsconfig | utils | validators YAML|TextPB cloudprober.alerting.AlertConf # # Name of the alert. Default is to use the probe name. If you have multiple # alerts for the same probe, you must specify a name for each alert.name: \u0026lt;string\u003e # Condition for the alert. Default is to alert on any failure. # Example: # # Alert if 6 out of 10 probes fail.",content:" Configs:\u0026nbsp;\u0026nbsp;main | alerting | metrics | oauth | probes | rds | servers | surfacer | targets | tlsconfig | utils | validators YAML|TextPB cloudprober.alerting.AlertConf # # Name of the alert. Default is to use the probe name. If you have multiple # alerts for the same probe, you must specify a name for each alert.name: \u0026lt;string\u003e # Condition for the alert. Default is to alert on any failure. # Example: # # Alert if 6 out of 10 probes fail. # condition { # failures: 6 # total: 10 # }condition: \u0026lt;cloudprober.alerting.Condition\u003e # How to notify in case of alert.notify: \u0026lt;cloudprober.alerting.NotifyConfig\u003e # Dashboard URL template. # Default: http://localhost:9313/status?probe=@probe@dashboard_url_template: \u0026lt;string\u003e playbook_url_template: \u0026lt;string\u003e # Default: Cloudprober alert \u0026#34;@alert@\u0026#34; for \u0026#34;@target@\u0026#34;summary_template: \u0026lt;string\u003e # Default: # Cloudprober alert \u0026#34;@alert@\u0026#34; for \u0026#34;@target@\u0026#34;: # Failures: @failures@ out of @total@ probes # Failing since: @since@ # Probe: @probe@ # Dashboard: @dashboard_url@ # Playbook: @playbook_url@details_template: \u0026lt;string\u003e # Key values to be included in the alert. These fields are expanded # using the same template expansion rules as summary_template and # details_template (see above).other_info: \u0026lt;cloudprober.alerting.AlertConf.OtherInfoEntry\u003e severity: (UNKNOWN_SEVERITY|CRITICAL|ERROR|WARNING|INFO): \u0026lt;enum\u003e # How often to repeat notification for the same alert. Default is 1hr. # To disable any kind of notification throttling, set this to 0.repeat_interval_sec: \u0026lt;int32\u003e cloudprober.alerting.AlertConf.OtherInfoEntry # key: \u0026lt;string\u003e value: \u0026lt;string\u003e cloudprober.alerting.Condition # failures: \u0026lt;int32\u003e total: \u0026lt;int32\u003e cloudprober.alerting.Email # # Email addresses to send the alert to.to: \u0026lt;string\u003e # From address in the alert email. # If not set, defaults to the value of smtp_user if smtp_user is set, # otherwise defaults to cloudprober-alert@\u0026lt;hostname\u0026gt;.from: \u0026lt;string\u003e # Default: Environment variable SMTP_SERVER smtp_server: \u0026lt;string\u003e # Default: Environment variable SMTP_USERNAMEsmtp_username: \u0026lt;string\u003e # Default: Environment variable SMTP_PASSWORDsmtp_password: \u0026lt;string\u003e cloudprober.alerting.NotifyConfig # # Command to run when alert is fired. You can use this command to do # various things, e.g.: # - Send a notification using a method not supported by Cloudprober. # - Collect more information, e.g. send mtr report on ping failures. # - Attempt fix the issue, e.g. restart a pod or clear cache. # # In the command line following fields are substituted: # @alert@: Alert name # @probe@: Probe name # @target@: Target name, or target and port if port is specified. # @target.label.\u0026lt;label\u0026gt;@: Label \u0026lt;label\u0026gt; value, e.g. target.label.role. # @failures@: Count of failures. # @total@: Out of. # @since@: Time since the alert condition started. # @json@: JSON representation of the alert fields. # # For example, if you want to send an email when an alert is fired, you can # use the following command: # command: \u0026#34;/usr/bin/mail -s \u0026#39;Alert @alert@ fired for @target@\u0026#39; manu@a.b\u0026#34;command: \u0026lt;string\u003e # Email notification configuration.email: \u0026lt;cloudprober.alerting.Email\u003e # PagerDuty configuration.pager_duty: \u0026lt;cloudprober.alerting.PagerDuty\u003e # Slack configuration.slack: \u0026lt;cloudprober.alerting.Slack\u003e # Opsgenie configuration.opsgenie: \u0026lt;cloudprober.alerting.Opsgenie\u003e # Notify using an HTTP request. HTTP request fields are expanded using the # same template expansion rules as \u0026#34;command\u0026#34; above: # For example, to send a notification using rest API: # http_notify { # url: \u0026#34;http://localhost:8080/alert\u0026#34; # method: POST # header { # key: \u0026#34;Authorization\u0026#34; # value: \u0026#34;Bearer {{env \u0026#39;AUTH_TOKEN\u0026#39;}}\u0026#34; # } # data: \u0026#34;{\\\u0026#34;message\\\u0026#34;: \\\u0026#34;@alert@ fired for @target@\\\u0026#34;, \\\u0026#34;details\\\u0026#34;: \\\u0026#34;name\\\u0026#34;}\u0026#34; # }http_notify: \u0026lt;cloudprober.utils.httpreq.HTTPRequest\u003e cloudprober.alerting.Opsgenie # # API key to access Opsgenie. It\u0026#39;s usually tied to a team and is # obtained by creating a new API integration or using an existing one.api_key: \u0026lt;string\u003e # Environment variable name Default: OPSGENIE_API_KEYapi_key_env_var: \u0026lt;string\u003e # Opsgenie responders. Opsgenie uses the responders to route the alerts if # API key doesn\u0026#39;t belong to a team integration. # Example: # responders { # id: \u0026#34;4513b7ea-3b91-438f-b7e4-e3e54af9147c\u0026#34; # type: TEAM # }responders: \u0026lt;cloudprober.alerting.Opsgenie.Responder\u003e # Opsgenie API URL. # Default: https://api.opsgenie.com/v2/alertsapi_url: \u0026lt;string\u003e # Whether to send resolve notifications or not. Default is to send resolve # notifications.disable_send_resolved: \u0026lt;bool\u003e cloudprober.alerting.Opsgenie.Responder # [id \u0026lt;string\u0026gt; | name \u0026lt;string\u0026gt;]: \u0026lt;oneof\u003e type: (UNKNOWN_RESPONDER|USER|TEAM|ESCALATION|SCHEDULE): \u0026lt;enum\u003e cloudprober.alerting.PagerDuty # # PagerDuty Routing Key. # The routing key is used to authenticate to PagerDuty and is tied to a # service. You can obtain the routing key from the service page, under the # integrations tab. # Note: set either routing_key or routing_key_env_var. routing_key # takes precedence over routing_key_env_var.routing_key: \u0026lt;string\u003e # The environment variable containing the pagerduty routing key. # Default: PAGERDUTY_ROUTING_KEY;routing_key_env_var: \u0026lt;string\u003e # PagerDuty API URL. # Used to overwrite the default PagerDuty API URL.api_url: \u0026lt;string\u003e # Whether to send resolve notifications or not. Default is to send resolve # notifications.disable_send_resolved: \u0026lt;bool\u003e cloudprober.alerting.Slack # # Webhook URL # The Slack notifications use a webhook URL to send the notifications to # a Slack channel. The webhook URL can be found in the Slack console under # the \u0026#34;Incoming Webhooks\u0026#34; section. # https://api.slack.com/messaging/webhooks # Note: set either webhook_url or webhook_url_env_var. webhook_url # takes precedence over webhook_url_env_var.webhook_url: \u0026lt;string\u003e # The environment variable that is used to contain the slack webhook URL.webhook_url_env_var: \u0026lt;string\u003e cloudprober.alerting.AlertConf # # Name of the alert. Default is to use the probe name. If you have multiple # alerts for the same probe, you must specify a name for each alert.name: \u0026lt;string\u003e # Condition for the alert. Default is to alert on any failure. # Example: # # Alert if 6 out of 10 probes fail. # condition { # failures: 6 # total: 10 # }condition: \u0026lt;cloudprober.alerting.Condition\u003e # How to notify in case of alert.notify: \u0026lt;cloudprober.alerting.NotifyConfig\u003e # Dashboard URL template. # Default: http://localhost:9313/status?probe=@probe@dashboard_url_template: \u0026lt;string\u003e playbook_url_template: \u0026lt;string\u003e # Default: Cloudprober alert \u0026#34;@alert@\u0026#34; for \u0026#34;@target@\u0026#34;summary_template: \u0026lt;string\u003e # Default: # Cloudprober alert \u0026#34;@alert@\u0026#34; for \u0026#34;@target@\u0026#34;: # Failures: @failures@ out of @total@ probes # Failing since: @since@ # Probe: @probe@ # Dashboard: @dashboard_url@ # Playbook: @playbook_url@details_template: \u0026lt;string\u003e # Key values to be included in the alert. These fields are expanded # using the same template expansion rules as summary_template and # details_template (see above).other_info: \u0026lt;cloudprober.alerting.AlertConf.OtherInfoEntry\u003e severity: (UNKNOWN_SEVERITY|CRITICAL|ERROR|WARNING|INFO): \u0026lt;enum\u003e # How often to repeat notification for the same alert. Default is 1hr. # To disable any kind of notification throttling, set this to 0.repeat_interval_sec: \u0026lt;int32\u003e cloudprober.alerting.AlertConf.OtherInfoEntry # key: \u0026lt;string\u003e value: \u0026lt;string\u003e cloudprober.alerting.Condition # failures: \u0026lt;int32\u003e total: \u0026lt;int32\u003e cloudprober.alerting.Email # # Email addresses to send the alert to.to: \u0026lt;string\u003e # From address in the alert email. # If not set, defaults to the value of smtp_user if smtp_user is set, # otherwise defaults to cloudprober-alert@\u0026lt;hostname\u0026gt;.from: \u0026lt;string\u003e # Default: Environment variable SMTP_SERVER smtp_server: \u0026lt;string\u003e # Default: Environment variable SMTP_USERNAMEsmtp_username: \u0026lt;string\u003e # Default: Environment variable SMTP_PASSWORDsmtp_password: \u0026lt;string\u003e cloudprober.alerting.NotifyConfig # # Command to run when alert is fired. You can use this command to do # various things, e.g.: # - Send a notification using a method not supported by Cloudprober. # - Collect more information, e.g. send mtr report on ping failures. # - Attempt fix the issue, e.g. restart a pod or clear cache. # # In the command line following fields are substituted: # @alert@: Alert name # @probe@: Probe name # @target@: Target name, or target and port if port is specified. # @target.label.\u0026lt;label\u0026gt;@: Label \u0026lt;label\u0026gt; value, e.g. target.label.role. # @failures@: Count of failures. # @total@: Out of. # @since@: Time since the alert condition started. # @json@: JSON representation of the alert fields. # # For example, if you want to send an email when an alert is fired, you can # use the following command: # command: \u0026#34;/usr/bin/mail -s \u0026#39;Alert @alert@ fired for @target@\u0026#39; manu@a.b\u0026#34;command: \u0026lt;string\u003e # Email notification configuration.email: \u0026lt;cloudprober.alerting.Email\u003e # PagerDuty configuration.pager_duty: \u0026lt;cloudprober.alerting.PagerDuty\u003e # Slack configuration.slack: \u0026lt;cloudprober.alerting.Slack\u003e # Opsgenie configuration.opsgenie: \u0026lt;cloudprober.alerting.Opsgenie\u003e # Notify using an HTTP request. HTTP request fields are expanded using the # same template expansion rules as \u0026#34;command\u0026#34; above: # For example, to send a notification using rest API: # http_notify { # url: \u0026#34;http://localhost:8080/alert\u0026#34; # method: POST # header { # key: \u0026#34;Authorization\u0026#34; # value: \u0026#34;Bearer {{env \u0026#39;AUTH_TOKEN\u0026#39;}}\u0026#34; # } # data: \u0026#34;{\\\u0026#34;message\\\u0026#34;: \\\u0026#34;@alert@ fired for @target@\\\u0026#34;, \\\u0026#34;details\\\u0026#34;: \\\u0026#34;name\\\u0026#34;}\u0026#34; # }http_notify: \u0026lt;cloudprober.utils.httpreq.HTTPRequest\u003e cloudprober.alerting.Opsgenie # # API key to access Opsgenie. It\u0026#39;s usually tied to a team and is # obtained by creating a new API integration or using an existing one.api_key: \u0026lt;string\u003e # Environment variable name Default: OPSGENIE_API_KEYapi_key_env_var: \u0026lt;string\u003e # Opsgenie responders. Opsgenie uses the responders to route the alerts if # API key doesn\u0026#39;t belong to a team integration. # Example: # responders { # id: \u0026#34;4513b7ea-3b91-438f-b7e4-e3e54af9147c\u0026#34; # type: TEAM # }responders: \u0026lt;cloudprober.alerting.Opsgenie.Responder\u003e # Opsgenie API URL. # Default: https://api.opsgenie.com/v2/alertsapi_url: \u0026lt;string\u003e # Whether to send resolve notifications or not. Default is to send resolve # notifications.disable_send_resolved: \u0026lt;bool\u003e cloudprober.alerting.Opsgenie.Responder # [id \u0026lt;string\u0026gt; | name \u0026lt;string\u0026gt;]: \u0026lt;oneof\u003e type: (UNKNOWN_RESPONDER|USER|TEAM|ESCALATION|SCHEDULE): \u0026lt;enum\u003e cloudprober.alerting.PagerDuty # # PagerDuty Routing Key. # The routing key is used to authenticate to PagerDuty and is tied to a # service. You can obtain the routing key from the service page, under the # integrations tab. # Note: set either routing_key or routing_key_env_var. routing_key # takes precedence over routing_key_env_var.routing_key: \u0026lt;string\u003e # The environment variable containing the pagerduty routing key. # Default: PAGERDUTY_ROUTING_KEY;routing_key_env_var: \u0026lt;string\u003e # PagerDuty API URL. # Used to overwrite the default PagerDuty API URL.api_url: \u0026lt;string\u003e # Whether to send resolve notifications or not. Default is to send resolve # notifications.disable_send_resolved: \u0026lt;bool\u003e cloudprober.alerting.Slack # # Webhook URL # The Slack notifications use a webhook URL to send the notifications to # a Slack channel. The webhook URL can be found in the Slack console under # the \u0026#34;Incoming Webhooks\u0026#34; section. # https://api.slack.com/messaging/webhooks # Note: set either webhook_url or webhook_url_env_var. webhook_url # takes precedence over webhook_url_env_var.webhook_url: \u0026lt;string\u003e # The environment variable that is used to contain the slack webhook URL.webhook_url_env_var: \u0026lt;string\u003e "}),e.add({id:13,href:"/docs/how-to/built-in-servers/",title:"Built-in Servers",description:`Cloudprober comes with some custom servers that can be enabled through configuration. These servers can act as targets for the other probes \u0026ndash; for example, you can run two Cloudprober instances on two different machines and have one instance\u0026rsquo;s servers act as targets and other instance probe those targets. These servers can come in handy when the goal is to monitor the underlying infrastructure: e.g. network or load balancers. Cloudprober (probes) ===(Network)===\u003e Cloudprober (servers) HTTP Server #server { type: HTTP http_server { port: 8080 } } This creates an HTTP server that responds on the port 8080.`,content:`Cloudprober comes with some custom servers that can be enabled through configuration. These servers can act as targets for the other probes \u0026ndash; for example, you can run two Cloudprober instances on two different machines and have one instance\u0026rsquo;s servers act as targets and other instance probe those targets.
These servers can come in handy when the goal is to monitor the underlying infrastructure: e.g. network or load balancers.
Cloudprober (probes) ===(Network)===\u003e Cloudprober (servers) HTTP Server #server { type: HTTP http_server { port: 8080 } } This creates an HTTP server that responds on the port 8080. This HTTP server supports the following two endpoints by default:
/healthcheck - returns \u0026lsquo;OK\u0026rsquo; if instance is not in the lameduck mode. /lameduck - returns the lameduck status (true/false). Lameduck mode is a mode in which a server is still running but is signaling that it is about to go down for maintenance so new requests should not be sent to it. This is typically used with load balancers to take out a backend for maintenance without returning any actual errors.
TODO(manugarg): Document how a Cloudprober can be put in the lameduck mode.
Data Handlers #You can also add custom data handlers to the above HTTP server:
server { type: HTTP http_server { port: 8080 pattern_data_handler { response_size: 1024 } pattern_data_handler { response_size: 4 pattern: \u0026quot;four\u0026quot; } } } Above configuration adds the following two URLs to the HTTP server:
/data_1024 which responds with 1024 bytes of cloudprobercloudprober...(repeated). /data_4 which responds with four. These endpoints are useful to monitor other aspects of the underlying network like MTU, and consistency (make sure data is not getting corrupted), etc.
See this for all HTTP server configuration options.
UDP #UDP server can either echo packets back or completely ignore them. In echo mode, you can use it along with the UDP probe type.
server { type: UDP udp_server { port: 85 type: ECHO } } server { type: UDP udp_server { port: 90 type: DISCARD } } See ServerConf for all UDP server configuration options.
GRPC #See ServerConf for all GRPC server configuration options.
`}),e.add({id:14,href:"/docs/surfacers/cloudwatch/",title:"Cloudwatch (AWS)",description:`Cloudprober can natively export metrics to AWS Cloudwatch using the cloudwatch surfacer. Adding the cloudwatch surfacer to cloudprover is as simple as adding the following stanza to the config:
surfacer { type: CLOUDWATCH } Authentication #The cloudwatch surfacer uses the AWS Go SDK, and supports the default credential chain:
Environment variables. Shared credentials file. If your application uses an ECS task definition or RunTask API operation, IAM role for tasks.`,content:`Cloudprober can natively export metrics to AWS Cloudwatch using the cloudwatch surfacer. Adding the cloudwatch surfacer to cloudprover is as simple as adding the following stanza to the config:
surfacer { type: CLOUDWATCH } Authentication #The cloudwatch surfacer uses the AWS Go SDK, and supports the default credential chain:
Environment variables. Shared credentials file. If your application uses an ECS task definition or RunTask API operation, IAM role for tasks. If your application is running on an Amazon EC2 instance, IAM role for Amazon EC2. Cloudwatch Region #The list below is the order of precedence that will be used to determine the AWS region that Cloudprober will publish metrics to.
Region configuration EC2 metadata. AWS_REGION environment variable. AWS_DEFAULT_REGION environment variable, if AWS_SDK_LOAD_CONFIG is set (See AWS package documentation for more details). Authorization #In order to permit Cloudprober to publish metric data to cloudwatch, ensure the profile being used for authentication has the following permissions, where the \u0026ldquo;cloudwatch:namespace\u0026rdquo; is the metric namespace used by Cloudprober.
If the default metric namespace is changed, also change the condition in the IAM policy below to match the same value.
{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Condition\u0026quot;: { \u0026quot;StringEqualsIgnoreCase\u0026quot;: { \u0026quot;cloudwatch:namespace\u0026quot;: \u0026quot;cloudprober\u0026quot; } }, \u0026quot;Action\u0026quot;: [ \u0026quot;cloudwatch:PutMetricData\u0026quot; ], \u0026quot;Resource\u0026quot;: [ \u0026quot;*\u0026quot; ], \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Sid\u0026quot;: \u0026quot;PutMetrics\u0026quot; } ] } Metric Namespace #The metric namespace used to publish metrics to by default is set to cloudprober. This can be changed by expanding the surfacer configuration:
surfacer { type: CLOUDWATCH cloudwatch_surfacer { namespace: \u0026quot;/cloudprober/website/probes\u0026quot; } } Note: If the namespace is modified, also modify the IAM policy condition for the namespace PutMetricData call.
Configuration Options #The full list of configuration options for the cloudwatch surfacer is:
// The cloudwatch metric namespace optional string namespace = 1 [default = \u0026quot;cloudprober\u0026quot;]; // The cloudwatch resolution value, lowering this below 60 will incur // additional charges as the metrics will be charged at a high resolution rate. optional int64 resolution = 2 [default=60]; // The AWS Region, used to create a CloudWatch session. // The order of fallback for evaluating the AWS Region: // 1. This config value. // 2. EC2 metadata endpoint, via cloudprober sysvars. // 3. AWS_REGION environment value. // 4. AWS_DEFAULT_REGION environment value, if AWS_SDK_LOAD_CONFIG is set. // https://docs.aws.amazon.com/sdk-for-go/api/aws/session/ optional string region = 3; // The maximum number of metrics that will be published at one // time. Metrics will be stored locally in a cache until this // limit is reached. 1000 is the maximum number of metrics // supported by the Cloudwatch PutMetricData API. // Metrics will be published when the timer expires, or the buffer is // full, whichever happens first. optional int32 metrics_batch_size = 4 [default = 1000]; // The maximum amount of time to hold metrics in the buffer (above). // Metrics will be published when the timer expires, or the buffer is // full, whichever happens first. optional int32 batch_timer_sec = 5 [default = 30]; (All config options: SurfacerConf
Calculating the metric delta with Cloudwatch Metric Maths #The metrics produced by Cloudprober are cumulative. Most services producing metrics into cloudwatch produce snapshot data whereby the metrics are recorded for a specific point in time.
In order to achieve a similar effect here, the Cloudwatch Metric Maths RATE and PERIOD functions can be used to determine the delta values.
RATE(m1) * PERIOD(m1) Whereby m1 is the metric id for the Cloudprober metrics, for example:
namespace: cloudprober metric name: latency dst: google.com ptype: http probe: probe name `}),e.add({id:15,href:"/docs/config/",title:"Configuration",description:" ",content:" "}),e.add({id:16,href:"/docs/config/overview/",title:"Configuration",description:"Configs:\u0026nbsp;\u0026nbsp;main | alerting | metrics | oauth | probes | rds | servers | surfacer | targets | tlsconfig | utils | validators YAML|TextPB # Probes to run.probe \u0026lt;cloudprober.probes.ProbeDef\u003e: # Probe name. It should be unique across all probes. - name: \u0026lt;string\u003e type: (PING|HTTP|DNS|EXTERNAL|UDP|UDP_LISTENER|GRPC|TCP|EXTENSION|USER_DEFINED): \u0026lt;enum\u003e # Interval between two probe runs in milliseconds. # Only one of \u0026#34;interval\u0026#34; and \u0026#34;inteval_msec\u0026#34; should be defined. # Default interval is 2s. interval_msec: \u0026lt;int32\u003e # Interval between two probe runs in string format, e.",content:" Configs:\u0026nbsp;\u0026nbsp;main | alerting | metrics | oauth | probes | rds | servers | surfacer | targets | tlsconfig | utils | validators YAML|TextPB # Probes to run.probe \u0026lt;cloudprober.probes.ProbeDef\u003e: # Probe name. It should be unique across all probes. - name: \u0026lt;string\u003e type: (PING|HTTP|DNS|EXTERNAL|UDP|UDP_LISTENER|GRPC|TCP|EXTENSION|USER_DEFINED): \u0026lt;enum\u003e # Interval between two probe runs in milliseconds. # Only one of \u0026#34;interval\u0026#34; and \u0026#34;inteval_msec\u0026#34; should be defined. # Default interval is 2s. interval_msec: \u0026lt;int32\u003e # Interval between two probe runs in string format, e.g. 10s. # Only one of \u0026#34;interval\u0026#34; and \u0026#34;inteval_msec\u0026#34; should be defined. # Default interval is 2s. interval: \u0026lt;string\u003e # Timeout for each probe in milliseconds # Only one of \u0026#34;timeout\u0026#34; and \u0026#34;timeout_msec\u0026#34; should be defined. # Default timeout is 1s. timeout_msec: \u0026lt;int32\u003e # Timeout for each probe in string format, e.g. 10s. # Only one of \u0026#34;timeout\u0026#34; and \u0026#34;timeout_msec\u0026#34; should be defined. # Default timeout is 1s. timeout: \u0026lt;string\u003e # Targets for the probe. Targets are required for all probes except # for external, user_defined, and extension probe types. targets: \u0026lt;cloudprober.targets.TargetsDef\u003e # Latency distribution. If specified, latency is stored as a distribution. latency_distribution: \u0026lt;cloudprober.metrics.Dist\u003e # Latency unit. Any string that\u0026#39;s parseable by time.ParseDuration. # Valid values: \u0026#34;ns\u0026#34;, \u0026#34;us\u0026#34; (or \u0026#34;Âµs\u0026#34;), \u0026#34;ms\u0026#34;, \u0026#34;s\u0026#34;, \u0026#34;m\u0026#34;, \u0026#34;h\u0026#34;. latency_unit: \u0026lt;string\u003e | default: us # Latency metric name. You may want to change the latency metric name, if: # you\u0026#39;re using latency_distribution for some probes, and regular metric for # other probes, and you want to differentiate between the two. # For example: # probe { # name: \u0026#34;web1_latency\u0026#34; # latency_distribution: {...} # latency_metric_name: \u0026#34;latency_dist\u0026#34; # ... # } # probe { # name: \u0026#34;app1\u0026#34; # ... # } latency_metric_name: \u0026lt;string\u003e | default: latency # Validators for this probe. Validators are run on the data returned by the # probe. See https://cloudprober.org/docs/how-to/validators/ for more info. validator: \u0026lt;cloudprober.validators.Validator\u003e # Set the source IP to send packets from, either by providing an IP address # directly, or a network interface. [source_ip \u0026lt;string\u0026gt; | source_interface \u0026lt;string\u0026gt;]: \u0026lt;oneof\u003e ip_version: (IP_VERSION_UNSPECIFIED|IPV4|IPV6): \u0026lt;enum\u003e # How often to export stats. Probes usually run at a higher frequency (e.g. # every second); stats from individual probes are aggregated within # cloudprober until exported. In most cases, users don\u0026#39;t need to change the # default. # # By default this field is set in the following way: # For all probes except UDP: # stats_export_interval=max(interval, 10s) # For UDP: # stats_export_interval=max(2*max(interval, timeout), 10s) stats_export_interval_msec: \u0026lt;int32\u003e # Additional labels to add to the probe results. Label\u0026#39;s value can either be # static or can be derived from target\u0026#39;s labels. # # Example: # additional_label { # key: \u0026#34;app\u0026#34; # value: \u0026#34;@target.label.app@\u0026#34; # } # (More detailed example at: examples/additional_label/cloudprober.cfg) additional_label: \u0026lt;cloudprober.probes.AdditionalLabel\u003e # (Experimental) If set, test is inversed, i.e. we count it as success if # target doesn\u0026#39;t respond. This is useful, for example, that your firewall is # working as expected. # # This is currently implemented only by PING and TCP probes. # Note: This field is currently experimental, and may change in future. negative_test: \u0026lt;bool\u003e # Alerts configuration. If specified, cloudprober will generate alerts on # probe failures. You can specify multiple alerts. # Example: # alert { # name: \u0026#34;alert1\u0026#34; # condition {...} # notify { # pagerduty { ...} # } # } # alert { # name: \u0026#34;alert2\u0026#34; # notify { ... } # } alert: \u0026lt;cloudprober.alerting.AlertConf\u003e [ping_probe \u0026lt;cloudprober.probes.ping.ProbeConf\u0026gt; | http_probe \u0026lt;cloudprober.probes.http.ProbeConf\u0026gt; | \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;dns_probe \u0026lt;cloudprober.probes.dns.ProbeConf\u0026gt; | external_probe \u0026lt;cloudprober.probes.external.ProbeConf\u0026gt; | \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;udp_probe \u0026lt;cloudprober.probes.udp.ProbeConf\u0026gt; | udp_listener_probe \u0026lt;cloudprober.probes.udplistener.ProbeConf\u0026gt; | \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;grpc_probe \u0026lt;cloudprober.probes.grpc.ProbeConf\u0026gt; | tcp_probe \u0026lt;cloudprober.probes.tcp.ProbeConf\u0026gt; | \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;user_defined_probe \u0026lt;string\u0026gt;]: \u0026lt;oneof\u003e # Which machines this probe should run on. If defined, cloudprober will run # this probe only if machine\u0026#39;s hostname matches this value. This is useful # for large deployments, where you may want to use the same prober config # everywhere but run this probe only on a subset of machines. run_on: \u0026lt;string\u003e # Schedule for the probe. You can use a schedule to specify when a probe # should or should not run. This is useful for running probes only during # business hours. # # You can specify multiple schedules. Probe will not run if any of the # \u0026#34;DISABLE\u0026#34; schedules are active. If both \u0026#34;ENABLE\u0026#34; and \u0026#34;DISABLE\u0026#34; schedules # overlap, \u0026#34;DISABLE\u0026#34; takes precedence. # # For example, to disable a probe during weekends and on Tuesday between 7pm # and 8pm, e.g. for rollouts: # schdule { # type: DISABLE # start_weekday: FRIDAY # start_time: \u0026#34;20:00\u0026#34; # end_weekday: SUNDAY # end_time: \u0026#34;17:00\u0026#34; # timezone: \u0026#34;America/New_York\u0026#34; # } # schdule { # type: DISABLE # start_weekday: TUESDAY # start_time: \u0026#34;19:00\u0026#34; # end_weekday: TUESDAY # end_time: \u0026#34;20:00\u0026#34; # timezone: \u0026#34;America/New_York\u0026#34; # } schedule: \u0026lt;cloudprober.probes.Schedule\u003e # Debug options. Currently only used to enable logging metrics. debug_options: \u0026lt;cloudprober.probes.DebugOptions\u003e # Surfacers are used to export probe results for further processing. # If no surfacer is configured, a prometheus and a file surfacer are # initialized: # - Prometheus makes probe results available at http://\u0026lt;host\u0026gt;:9313/metrics. # - File surfacer writes results to stdout. # # You can disable default surfacers (in case you want no surfacer at all), by # adding the following to your config: # surfacer {}surfacer \u0026lt;cloudprober.surfacer.SurfacerDef\u003e: # This name is used for logging. If not defined, it\u0026#39;s derived from the type. # Note that this field is required for the USER_DEFINED surfacer type and # should match with the name that you used while registering the user defined # surfacer. - name: \u0026lt;string\u003e type: (NONE|PROMETHEUS|STACKDRIVER|FILE|POSTGRES|PUBSUB|CLOUDWATCH|DATADOG|PROBESTATUS|BIGQUERY|OTEL|USER_DEFINED): \u0026lt;enum\u003e # How many metrics entries (EventMetrics) to buffer. This is the buffer # between incoming metrics and the metrics that are being processed. Default # value should work in most cases. You may need to increase it on a busy # system, but that\u0026#39;s usually a sign that you metrics processing pipeline is # slow for some reason, e.g. slow writes to a remote file. # Note: Only file and pubsub surfacer supports this option right now. metrics_buffer_size: \u0026lt;int64\u003e | default: 10000 # If specified, only allow metrics that match any of these label filters. # Example: # allow_metrics_with_label { # key: \u0026#34;probe\u0026#34;, # value: \u0026#34;check_homepage\u0026#34;, # } allow_metrics_with_label: \u0026lt;cloudprober.surfacer.LabelFilter\u003e # Ignore metrics that match any of these label filters. Ignore has precedence # over allow filters. # Example: # ignore_metrics_with_label { # key: \u0026#34;probe\u0026#34;, # value: \u0026#34;sysvars\u0026#34;, # } ignore_metrics_with_label: \u0026lt;cloudprober.surfacer.LabelFilter\u003e # Allow and ignore metrics based on their names. You can specify regexes # here. Ignore has precendence over allow. # Examples: # ignore_metrics_with_name: \u0026#34;validation_failure\u0026#34; # allow_metrics_with_name: \u0026#34;(total|success|latency)\u0026#34; # # For efficiency reasons, filtering by metric name has to be implemented by # individual surfacers (while going through metrics within an EventMetrics). # As FILE and PUBSUB surfacers export eventmetrics as is, they don\u0026#39;t support # this option. allow_metrics_with_name: \u0026lt;string\u003e ignore_metrics_with_name: \u0026lt;string\u003e # Whether to add failure metric or not. This option is enabled by default # for all surfacers except FILE and PUBSUB. add_failure_metric: \u0026lt;bool\u003e # If set to true, cloudprober will export all metrics as gauge metrics. Note # that cloudprober inherently generates only cumulative metrics. To create # gauge metrics from cumulative metrics, we keep a copy of the old metrics # and subtract new metrics from the previous metrics. This transformation in # metrics has an increased memory-overhead because extra copies required. # However, it should not be noticeable unless you\u0026#39;re producing large number # of metrics (say \u0026gt; 10000 metrics per second). export_as_gauge: \u0026lt;bool\u003e # Matching surfacer specific configuration (one for each type in the above # enum) [prometheus_surfacer \u0026lt;cloudprober.surfacer.prometheus.SurfacerConf\u0026gt; | stackdriver_surfacer \u0026lt;cloudprober.surfacer.stackdriver.SurfacerConf\u0026gt; | \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;file_surfacer \u0026lt;cloudprober.surfacer.file.SurfacerConf\u0026gt; | postgres_surfacer \u0026lt;cloudprober.surfacer.postgres.SurfacerConf\u0026gt; | \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;pubsub_surfacer \u0026lt;cloudprober.surfacer.pubsub.SurfacerConf\u0026gt; | cloudwatch_surfacer \u0026lt;cloudprober.surfacer.cloudwatch.SurfacerConf\u0026gt; | \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;datadog_surfacer \u0026lt;cloudprober.surfacer.datadog.SurfacerConf\u0026gt; | probestatus_surfacer \u0026lt;cloudprober.surfacer.probestatus.SurfacerConf\u0026gt; | \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;bigquery_surfacer \u0026lt;cloudprober.surfacer.bigquery.SurfacerConf\u0026gt; | otel_surfacer \u0026lt;cloudprober.surfacer.otel.SurfacerConf\u0026gt;]: \u0026lt;oneof\u003e # Servers to run inside cloudprober. These servers can serve as targets for # other probes.server \u0026lt;cloudprober.servers.ServerDef\u003e: - type: (HTTP|UDP|GRPC|EXTERNAL): \u0026lt;enum\u003e [http_server \u0026lt;cloudprober.servers.http.ServerConf\u0026gt; | udp_server \u0026lt;cloudprober.servers.udp.ServerConf\u0026gt; | \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;grpc_server \u0026lt;cloudprober.servers.grpc.ServerConf\u0026gt; | external_server \u0026lt;cloudprober.servers.external.ServerConf\u0026gt;]: \u0026lt;oneof\u003e # Shared targets allow you to re-use the same targets copy across multiple # probes. # Example: # shared_targets { # name: \u0026#34;internal-vms\u0026#34; # targets { # rds_targets {...} # } # } # # probe { # name: \u0026#34;vm-ping\u0026#34; # type: PING # targets { # shared_targets: \u0026#34;internal-vms\u0026#34; # } # } # # probe { # name: \u0026#34;vm-http\u0026#34; # type: HTTP # targets { # shared_targets: \u0026#34;internal-vms\u0026#34; # } # }shared_targets \u0026lt;cloudprober.SharedTargets\u003e: - name: \u0026lt;string\u003e targets: \u0026lt;cloudprober.targets.TargetsDef\u003e # Resource discovery serverrds_server \u0026lt;cloudprober.rds.ServerConf\u003e: # List of providers that server supports. provider: \u0026lt;cloudprober.rds.Provider\u003e # Port for the default HTTP server. This port is also used for prometheus # exporter (URL /metrics). Default port is 9313. If not specified in the # config, default port can be overridden by the environment variable # CLOUDPROBER_PORT.port: \u0026lt;int32\u003e # Port to run the default gRPC server on. If not specified, and if # environment variable CLOUDPROBER_GRPC_PORT is set, CLOUDPROBER_GRPC_PORT is # used for the default gRPC server. If CLOUDPROBER_GRPC_PORT is not set as # well, default gRPC server is not started.grpc_port: \u0026lt;int32\u003e # TLS config, it can be used to: # - Specify client\u0026#39;s CA cert for client cert verification: # grpc_tls_config { # ca_cert_file: \u0026#34;....\u0026#34; # } # # - Specify TLS cert and key: # grpc_tls_config { # tls_cert_file: \u0026#34;...\u0026#34; # tls_key_file: \u0026#34;...\u0026#34; # }grpc_tls_config \u0026lt;cloudprober.tlsconfig.TLSConfig\u003e: # CA certificate file to verify certificates provided by the other party. ca_cert_file: \u0026lt;string\u003e # Local certificate file. tls_cert_file: \u0026lt;string\u003e # Private key file corresponding to the certificate above. tls_key_file: \u0026lt;string\u003e # Whether to ignore the cert validation. disable_cert_validation: \u0026lt;bool\u003e # ServerName override server_name: \u0026lt;string\u003e # Certificate reload interval in seconds. If configured, the TLS cert will # be reloaded every reload_interval_sec seconds. This is useful when # certificates are generated and refreshed dynamically. reload_interval_sec: \u0026lt;int32\u003e # Host for the default HTTP server. Default listens on all addresses. If not # specified in the config, default port can be overridden by the environment # variable CLOUDPROBER_HOST.host: \u0026lt;string\u003e # Probes are staggered across time to avoid executing all of them at the # same time. This behavior can be disabled by setting the following option # to true.disable_jitter: \u0026lt;bool\u003e | default: false # How often to export system variables. To learn more about system variables: # http://godoc.org/github.com/cloudprober/cloudprober/internal/sysvars.sysvars_interval_msec: \u0026lt;int32\u003e | default: 10000 # Variables specified in this environment variable are exported as it is. # This is specifically useful to export information about system environment, # for example, docker image tag/digest-id, OS version etc. See # tools/cloudprober_startup.sh in the cloudprober directory for an example on # how to use these variables.sysvars_env_var: \u0026lt;string\u003e | default: SYSVARS # Time between triggering cancelation of various goroutines and exiting the # process. If --stop_time flag is also configured, that gets priority. # You may want to set it to 0 if cloudprober is running as a backend for # the probes and you don\u0026#39;t want time lost in stop and start.stop_time_sec: \u0026lt;int32\u003e | default: 5 # Global targets options. Per-probe options are specified within the probe # stanza.global_targets_options \u0026lt;cloudprober.targets.GlobalTargetsOptions\u003e: # RDS server address # Deprecated: This option is now deprecated, please use rds_server_options # instead. rds_server_address: \u0026lt;string\u003e # RDS server options, for example: # rds_server_options { # server_address: \u0026#34;rds-server.xyz:9314\u0026#34; # oauth_config: { # ... # } # } rds_server_options: \u0026lt;cloudprober.rds.ClientConf.ServerOptions\u003e # GCE targets options. global_gce_targets_options: \u0026lt;cloudprober.targets.gce.GlobalOptions\u003e # Lame duck options. If provided, targets module checks for the lame duck # targets and removes them from the targets list. lame_duck_options: \u0026lt;cloudprober.targets.lameduck.Options\u003e # Probes to run.probe \u0026lt;cloudprober.probes.ProbeDef\u003e { # Probe name. It should be unique across all probes. name: \u0026lt;string\u003e type: (PING|HTTP|DNS|EXTERNAL|UDP|UDP_LISTENER|GRPC|TCP|EXTENSION|USER_DEFINED): \u0026lt;enum\u003e # Interval between two probe runs in milliseconds. # Only one of \u0026#34;interval\u0026#34; and \u0026#34;inteval_msec\u0026#34; should be defined. # Default interval is 2s. interval_msec: \u0026lt;int32\u003e # Interval between two probe runs in string format, e.g. 10s. # Only one of \u0026#34;interval\u0026#34; and \u0026#34;inteval_msec\u0026#34; should be defined. # Default interval is 2s. interval: \u0026lt;string\u003e # Timeout for each probe in milliseconds # Only one of \u0026#34;timeout\u0026#34; and \u0026#34;timeout_msec\u0026#34; should be defined. # Default timeout is 1s. timeout_msec: \u0026lt;int32\u003e # Timeout for each probe in string format, e.g. 10s. # Only one of \u0026#34;timeout\u0026#34; and \u0026#34;timeout_msec\u0026#34; should be defined. # Default timeout is 1s. timeout: \u0026lt;string\u003e # Targets for the probe. Targets are required for all probes except # for external, user_defined, and extension probe types. targets: \u0026lt;cloudprober.targets.TargetsDef\u003e # Latency distribution. If specified, latency is stored as a distribution. latency_distribution: \u0026lt;cloudprober.metrics.Dist\u003e # Latency unit. Any string that\u0026#39;s parseable by time.ParseDuration. # Valid values: \u0026#34;ns\u0026#34;, \u0026#34;us\u0026#34; (or \u0026#34;Âµs\u0026#34;), \u0026#34;ms\u0026#34;, \u0026#34;s\u0026#34;, \u0026#34;m\u0026#34;, \u0026#34;h\u0026#34;. latency_unit: \u0026lt;string\u003e | default: us # Latency metric name. You may want to change the latency metric name, if: # you\u0026#39;re using latency_distribution for some probes, and regular metric for # other probes, and you want to differentiate between the two. # For example: # probe { # name: \u0026#34;web1_latency\u0026#34; # latency_distribution: {...} # latency_metric_name: \u0026#34;latency_dist\u0026#34; # ... # } # probe { # name: \u0026#34;app1\u0026#34; # ... # } latency_metric_name: \u0026lt;string\u003e | default: latency # Validators for this probe. Validators are run on the data returned by the # probe. See https://cloudprober.org/docs/how-to/validators/ for more info. validator: \u0026lt;cloudprober.validators.Validator\u003e # Set the source IP to send packets from, either by providing an IP address # directly, or a network interface. [source_ip \u0026lt;string\u0026gt; | source_interface \u0026lt;string\u0026gt;]: \u0026lt;oneof\u003e ip_version: (IP_VERSION_UNSPECIFIED|IPV4|IPV6): \u0026lt;enum\u003e # How often to export stats. Probes usually run at a higher frequency (e.g. # every second); stats from individual probes are aggregated within # cloudprober until exported. In most cases, users don\u0026#39;t need to change the # default. # # By default this field is set in the following way: # For all probes except UDP: # stats_export_interval=max(interval, 10s) # For UDP: # stats_export_interval=max(2*max(interval, timeout), 10s) stats_export_interval_msec: \u0026lt;int32\u003e # Additional labels to add to the probe results. Label\u0026#39;s value can either be # static or can be derived from target\u0026#39;s labels. # # Example: # additional_label { # key: \u0026#34;app\u0026#34; # value: \u0026#34;@target.label.app@\u0026#34; # } # (More detailed example at: examples/additional_label/cloudprober.cfg) additional_label: \u0026lt;cloudprober.probes.AdditionalLabel\u003e # (Experimental) If set, test is inversed, i.e. we count it as success if # target doesn\u0026#39;t respond. This is useful, for example, that your firewall is # working as expected. # # This is currently implemented only by PING and TCP probes. # Note: This field is currently experimental, and may change in future. negative_test: \u0026lt;bool\u003e # Alerts configuration. If specified, cloudprober will generate alerts on # probe failures. You can specify multiple alerts. # Example: # alert { # name: \u0026#34;alert1\u0026#34; # condition {...} # notify { # pagerduty { ...} # } # } # alert { # name: \u0026#34;alert2\u0026#34; # notify { ... } # } alert: \u0026lt;cloudprober.alerting.AlertConf\u003e [ping_probe \u0026lt;cloudprober.probes.ping.ProbeConf\u0026gt; | http_probe \u0026lt;cloudprober.probes.http.ProbeConf\u0026gt; | \u0026nbsp;\u0026nbsp;\u0026nbsp;dns_probe \u0026lt;cloudprober.probes.dns.ProbeConf\u0026gt; | external_probe \u0026lt;cloudprober.probes.external.ProbeConf\u0026gt; | \u0026nbsp;\u0026nbsp;\u0026nbsp;udp_probe \u0026lt;cloudprober.probes.udp.ProbeConf\u0026gt; | udp_listener_probe \u0026lt;cloudprober.probes.udplistener.ProbeConf\u0026gt; | \u0026nbsp;\u0026nbsp;\u0026nbsp;grpc_probe \u0026lt;cloudprober.probes.grpc.ProbeConf\u0026gt; | tcp_probe \u0026lt;cloudprober.probes.tcp.ProbeConf\u0026gt; | \u0026nbsp;\u0026nbsp;\u0026nbsp;user_defined_probe \u0026lt;string\u0026gt;]: \u0026lt;oneof\u003e # Which machines this probe should run on. If defined, cloudprober will run # this probe only if machine\u0026#39;s hostname matches this value. This is useful # for large deployments, where you may want to use the same prober config # everywhere but run this probe only on a subset of machines. run_on: \u0026lt;string\u003e # Schedule for the probe. You can use a schedule to specify when a probe # should or should not run. This is useful for running probes only during # business hours. # # You can specify multiple schedules. Probe will not run if any of the # \u0026#34;DISABLE\u0026#34; schedules are active. If both \u0026#34;ENABLE\u0026#34; and \u0026#34;DISABLE\u0026#34; schedules # overlap, \u0026#34;DISABLE\u0026#34; takes precedence. # # For example, to disable a probe during weekends and on Tuesday between 7pm # and 8pm, e.g. for rollouts: # schdule { # type: DISABLE # start_weekday: FRIDAY # start_time: \u0026#34;20:00\u0026#34; # end_weekday: SUNDAY # end_time: \u0026#34;17:00\u0026#34; # timezone: \u0026#34;America/New_York\u0026#34; # } # schdule { # type: DISABLE # start_weekday: TUESDAY # start_time: \u0026#34;19:00\u0026#34; # end_weekday: TUESDAY # end_time: \u0026#34;20:00\u0026#34; # timezone: \u0026#34;America/New_York\u0026#34; # } schedule: \u0026lt;cloudprober.probes.Schedule\u003e # Debug options. Currently only used to enable logging metrics. debug_options: \u0026lt;cloudprober.probes.DebugOptions\u003e } # Surfacers are used to export probe results for further processing. # If no surfacer is configured, a prometheus and a file surfacer are # initialized: # - Prometheus makes probe results available at http://\u0026lt;host\u0026gt;:9313/metrics. # - File surfacer writes results to stdout. # # You can disable default surfacers (in case you want no surfacer at all), by # adding the following to your config: # surfacer {}surfacer \u0026lt;cloudprober.surfacer.SurfacerDef\u003e { # This name is used for logging. If not defined, it\u0026#39;s derived from the type. # Note that this field is required for the USER_DEFINED surfacer type and # should match with the name that you used while registering the user defined # surfacer. name: \u0026lt;string\u003e type: (NONE|PROMETHEUS|STACKDRIVER|FILE|POSTGRES|PUBSUB|CLOUDWATCH|DATADOG|PROBESTATUS|BIGQUERY|OTEL|USER_DEFINED): \u0026lt;enum\u003e # How many metrics entries (EventMetrics) to buffer. This is the buffer # between incoming metrics and the metrics that are being processed. Default # value should work in most cases. You may need to increase it on a busy # system, but that\u0026#39;s usually a sign that you metrics processing pipeline is # slow for some reason, e.g. slow writes to a remote file. # Note: Only file and pubsub surfacer supports this option right now. metrics_buffer_size: \u0026lt;int64\u003e | default: 10000 # If specified, only allow metrics that match any of these label filters. # Example: # allow_metrics_with_label { # key: \u0026#34;probe\u0026#34;, # value: \u0026#34;check_homepage\u0026#34;, # } allow_metrics_with_label: \u0026lt;cloudprober.surfacer.LabelFilter\u003e # Ignore metrics that match any of these label filters. Ignore has precedence # over allow filters. # Example: # ignore_metrics_with_label { # key: \u0026#34;probe\u0026#34;, # value: \u0026#34;sysvars\u0026#34;, # } ignore_metrics_with_label: \u0026lt;cloudprober.surfacer.LabelFilter\u003e # Allow and ignore metrics based on their names. You can specify regexes # here. Ignore has precendence over allow. # Examples: # ignore_metrics_with_name: \u0026#34;validation_failure\u0026#34; # allow_metrics_with_name: \u0026#34;(total|success|latency)\u0026#34; # # For efficiency reasons, filtering by metric name has to be implemented by # individual surfacers (while going through metrics within an EventMetrics). # As FILE and PUBSUB surfacers export eventmetrics as is, they don\u0026#39;t support # this option. allow_metrics_with_name: \u0026lt;string\u003e ignore_metrics_with_name: \u0026lt;string\u003e # Whether to add failure metric or not. This option is enabled by default # for all surfacers except FILE and PUBSUB. add_failure_metric: \u0026lt;bool\u003e # If set to true, cloudprober will export all metrics as gauge metrics. Note # that cloudprober inherently generates only cumulative metrics. To create # gauge metrics from cumulative metrics, we keep a copy of the old metrics # and subtract new metrics from the previous metrics. This transformation in # metrics has an increased memory-overhead because extra copies required. # However, it should not be noticeable unless you\u0026#39;re producing large number # of metrics (say \u0026gt; 10000 metrics per second). export_as_gauge: \u0026lt;bool\u003e # Matching surfacer specific configuration (one for each type in the above # enum) [prometheus_surfacer \u0026lt;cloudprober.surfacer.prometheus.SurfacerConf\u0026gt; | stackdriver_surfacer \u0026lt;cloudprober.surfacer.stackdriver.SurfacerConf\u0026gt; | \u0026nbsp;\u0026nbsp;\u0026nbsp;file_surfacer \u0026lt;cloudprober.surfacer.file.SurfacerConf\u0026gt; | postgres_surfacer \u0026lt;cloudprober.surfacer.postgres.SurfacerConf\u0026gt; | \u0026nbsp;\u0026nbsp;\u0026nbsp;pubsub_surfacer \u0026lt;cloudprober.surfacer.pubsub.SurfacerConf\u0026gt; | cloudwatch_surfacer \u0026lt;cloudprober.surfacer.cloudwatch.SurfacerConf\u0026gt; | \u0026nbsp;\u0026nbsp;\u0026nbsp;datadog_surfacer \u0026lt;cloudprober.surfacer.datadog.SurfacerConf\u0026gt; | probestatus_surfacer \u0026lt;cloudprober.surfacer.probestatus.SurfacerConf\u0026gt; | \u0026nbsp;\u0026nbsp;\u0026nbsp;bigquery_surfacer \u0026lt;cloudprober.surfacer.bigquery.SurfacerConf\u0026gt; | otel_surfacer \u0026lt;cloudprober.surfacer.otel.SurfacerConf\u0026gt;]: \u0026lt;oneof\u003e } # Servers to run inside cloudprober. These servers can serve as targets for # other probes.server \u0026lt;cloudprober.servers.ServerDef\u003e { type: (HTTP|UDP|GRPC|EXTERNAL): \u0026lt;enum\u003e [http_server \u0026lt;cloudprober.servers.http.ServerConf\u0026gt; | udp_server \u0026lt;cloudprober.servers.udp.ServerConf\u0026gt; | \u0026nbsp;\u0026nbsp;\u0026nbsp;grpc_server \u0026lt;cloudprober.servers.grpc.ServerConf\u0026gt; | external_server \u0026lt;cloudprober.servers.external.ServerConf\u0026gt;]: \u0026lt;oneof\u003e } # Shared targets allow you to re-use the same targets copy across multiple # probes. # Example: # shared_targets { # name: \u0026#34;internal-vms\u0026#34; # targets { # rds_targets {...} # } # } # # probe { # name: \u0026#34;vm-ping\u0026#34; # type: PING # targets { # shared_targets: \u0026#34;internal-vms\u0026#34; # } # } # # probe { # name: \u0026#34;vm-http\u0026#34; # type: HTTP # targets { # shared_targets: \u0026#34;internal-vms\u0026#34; # } # }shared_targets \u0026lt;cloudprober.SharedTargets\u003e { name: \u0026lt;string\u003e targets: \u0026lt;cloudprober.targets.TargetsDef\u003e } # Resource discovery serverrds_server \u0026lt;cloudprober.rds.ServerConf\u003e { # List of providers that server supports. provider: \u0026lt;cloudprober.rds.Provider\u003e } # Port for the default HTTP server. This port is also used for prometheus # exporter (URL /metrics). Default port is 9313. If not specified in the # config, default port can be overridden by the environment variable # CLOUDPROBER_PORT.port: \u0026lt;int32\u003e # Port to run the default gRPC server on. If not specified, and if # environment variable CLOUDPROBER_GRPC_PORT is set, CLOUDPROBER_GRPC_PORT is # used for the default gRPC server. If CLOUDPROBER_GRPC_PORT is not set as # well, default gRPC server is not started.grpc_port: \u0026lt;int32\u003e # TLS config, it can be used to: # - Specify client\u0026#39;s CA cert for client cert verification: # grpc_tls_config { # ca_cert_file: \u0026#34;....\u0026#34; # } # # - Specify TLS cert and key: # grpc_tls_config { # tls_cert_file: \u0026#34;...\u0026#34; # tls_key_file: \u0026#34;...\u0026#34; # }grpc_tls_config \u0026lt;cloudprober.tlsconfig.TLSConfig\u003e { # CA certificate file to verify certificates provided by the other party. ca_cert_file: \u0026lt;string\u003e # Local certificate file. tls_cert_file: \u0026lt;string\u003e # Private key file corresponding to the certificate above. tls_key_file: \u0026lt;string\u003e # Whether to ignore the cert validation. disable_cert_validation: \u0026lt;bool\u003e # ServerName override server_name: \u0026lt;string\u003e # Certificate reload interval in seconds. If configured, the TLS cert will # be reloaded every reload_interval_sec seconds. This is useful when # certificates are generated and refreshed dynamically. reload_interval_sec: \u0026lt;int32\u003e } # Host for the default HTTP server. Default listens on all addresses. If not # specified in the config, default port can be overridden by the environment # variable CLOUDPROBER_HOST.host: \u0026lt;string\u003e # Probes are staggered across time to avoid executing all of them at the # same time. This behavior can be disabled by setting the following option # to true.disable_jitter: \u0026lt;bool\u003e | default: false # How often to export system variables. To learn more about system variables: # http://godoc.org/github.com/cloudprober/cloudprober/internal/sysvars.sysvars_interval_msec: \u0026lt;int32\u003e | default: 10000 # Variables specified in this environment variable are exported as it is. # This is specifically useful to export information about system environment, # for example, docker image tag/digest-id, OS version etc. See # tools/cloudprober_startup.sh in the cloudprober directory for an example on # how to use these variables.sysvars_env_var: \u0026lt;string\u003e | default: SYSVARS # Time between triggering cancelation of various goroutines and exiting the # process. If --stop_time flag is also configured, that gets priority. # You may want to set it to 0 if cloudprober is running as a backend for # the probes and you don\u0026#39;t want time lost in stop and start.stop_time_sec: \u0026lt;int32\u003e | default: 5 # Global targets options. Per-probe options are specified within the probe # stanza.global_targets_options \u0026lt;cloudprober.targets.GlobalTargetsOptions\u003e { # RDS server address # Deprecated: This option is now deprecated, please use rds_server_options # instead. rds_server_address: \u0026lt;string\u003e # RDS server options, for example: # rds_server_options { # server_address: \u0026#34;rds-server.xyz:9314\u0026#34; # oauth_config: { # ... # } # } rds_server_options: \u0026lt;cloudprober.rds.ClientConf.ServerOptions\u003e # GCE targets options. global_gce_targets_options: \u0026lt;cloudprober.targets.gce.GlobalOptions\u003e # Lame duck options. If provided, targets module checks for the lame duck # targets and removes them from the targets list. lame_duck_options: \u0026lt;cloudprober.targets.lameduck.Options\u003e } "}),e.add({id:17,href:"/docs/surfacers/overview/",title:"Exporting Metrics (Surfacers)",description:"One of the biggest strengths of cloudprober is that it can export data to multiple monitoring systems, even simultaneously, just based on simple configuration. Cloudprober does that using a built-in mechanism, called surfacers. Each surfacer type implements interface for a specific monitoring system, for example, cloudwatch surfacer publishes data to AWS Cloudwatch. You can configure multiple surfacers at the same time. If you don\u0026rsquo;t specify any surfacer, prometheus and file surfacers are enabled automatically.",content:`One of the biggest strengths of cloudprober is that it can export data to multiple monitoring systems, even simultaneously, just based on simple configuration. Cloudprober does that using a built-in mechanism, called surfacers. Each surfacer type implements interface for a specific monitoring system, for example, cloudwatch surfacer publishes data to AWS Cloudwatch. You can configure multiple surfacers at the same time. If you don\u0026rsquo;t specify any surfacer, prometheus and file surfacers are enabled automatically.
Cloudprober currently supports following surfacer types:
Prometheus (config) OpenTelemetry (OTEL) (config) [New in v0.13.2] Stackdriver (Google Cloud Monitoring) Google Pub/Sub (config) Postgres (config) File (config) Cloudwatch (AWS Cloud Monitoring) Overall surfacers config.
It\u0026rsquo;s easy to add more surfacers without having to understand the internals of cloudprober. You only need to implement the Surfacer interface.
Configuration #Adding surfacers to cloudprober is as easy as adding \u0026ldquo;surfacer\u0026rdquo; config stanzas to your config, like the following:
# Enable prometheus and stackdriver surfacers. # Make probe metrics available at the URL :\u0026lt;cloudprober_port\u0026gt;/metrics, for # scraping by prometheus. surfacer { type: PROMETHEUS prometheus_surfacer { # Following option adds a prefix to exported metrics, for example, # \u0026quot;total\u0026quot; metric is exported as \u0026quot;cloudprober_total\u0026quot;. metrics_prefix: \u0026quot;cloudprober_\u0026quot; } } # Stackdriver (Google Cloud Monitoring) surfacer_ No other configuration # is necessary if running on GCP. surfacer { type: STACKDRIVER } Filtering Metrics #You can control which metrics are published to a surfacer using the filtering mechanisms. For example, you may want to publish only specific metrics to AWS Cloudwatch to save on the costs.
Filtering by Label #To filter metrics by labels, use one of the following options in the surfacers config:
allow_metrics_with_label (allowMetricsWithLabel in yaml) ignore_metrics_with_label (ignoreMetricsWithLabel in yaml) Note: ignore_metrics_with_label takes precedence over allow_metrics_with_label.
For example, to ignore all sysvar metrics:
surfacer { type: PROMETHEUS ignore_metrics_with_label { key: \u0026quot;probe\u0026quot;, value: \u0026quot;sysvars\u0026quot;, } } Or to only allow metrics from http probes:
surfacer { type: PROMETHEUS allow_metrics_with_label { key: \u0026quot;ptype\u0026quot;, value: \u0026quot;http\u0026quot;, } } Filtering by Metric Name #To filter metrics by name, use one of the following options in the surfacers config:
allow_metrics_with_name (allowMetricsWithName in yaml) ignore_metrics_with_name (ignoreMetricsWithName in yaml) Note: ignore_metrics_with_name takes precedence over allow_metrics_with_name.
To filter out all validation_failure metrics by name:
surfacer { type: PROMETHEUS ignore_metrics_with_name: \u0026quot;validation_failure\u0026quot; } Modifying Metrics #You can configure surfacers to modify the metrics before they are sent to the backend monitoring system:
add_failure_metric: Export failure count along with the default total and success metrics:
surfacer { type: ... add_failure_metric = true .. } NOTE: This option is now enabled by default for all surfacers, except for FILE and PUBSUB surfacers.
export_as_gauge: Export gauge metrics instead of cumulative. Cloudprober exports cumulative metrics (sum of values so far) by default, but you can configure it to export gauge metrics instead. Gauge metrics make point-in-time calculations easier (e.g. you can just divide latency by success to get the average latency), but we lose the historical information if metrics are not received for a few intervals for some reason.
surfacer { type: ... export_as_gauge = true .. } `}),e.add({id:18,href:"/docs/how-to/extensions/",title:"Extending Cloudprober",description:"Cloudprober allows you to extend it across \u0026ldquo;probe\u0026rdquo; and \u0026ldquo;target\u0026rdquo; dimensions, that is, you can add new probe and target types to it without having to fork the entire codebase. Note that to extend cloudprober in this way, you will have to maintain your own cloudprober binary (which is mostly a wrapper around the \u0026ldquo;cloudprober package\u0026rdquo;), but you\u0026rsquo;ll be able to use rest of the cloudprober code from the common location.",content:`Cloudprober allows you to extend it across \u0026ldquo;probe\u0026rdquo; and \u0026ldquo;target\u0026rdquo; dimensions, that is, you can add new probe and target types to it without having to fork the entire codebase. Note that to extend cloudprober in this way, you will have to maintain your own cloudprober binary (which is mostly a wrapper around the \u0026ldquo;cloudprober package\u0026rdquo;), but you\u0026rsquo;ll be able to use rest of the cloudprober code from the common location.
Sample probe type #To demonstrate how it works, let\u0026rsquo;s add a new probe-type to Cloudprober. We\u0026rsquo;ll take the sample redis probe that we added in the external probe how-to, and convert it into a probe type that one can easily re-use. Let\u0026rsquo;s say that this probe-type provides a way to test redis server functionality and it takes the following options - operation (GET vs SET vs DELETE), key, value. This probe\u0026rsquo;s configuration looks like this:
probe { name: \u0026quot;redis_set\u0026quot; type: EXTENSION targets { host_names: \u0026quot;localhost:6379\u0026quot; } redis_probe { op: \u0026quot;set\u0026quot; key: \u0026quot;testkey\u0026quot; value: \u0026quot;testval\u0026quot; } } To make cloudprober understand this config, we\u0026rsquo;ll have to do a few things:
Define the probe config in a protobuf (.proto) file and mark it as an extension of the overall config.
Implement the probe type, possibly as a Go package, even though it can be embedded directly into the top-level binary.
Create a new cloudprober binary that includes the new probe type package.
Protobuf for the new probe type #Let\u0026rsquo;s create a new directory for our code: \$GOPATH/src/myprober.
// File: \$GOPATH/src/myprober/myprobe/myprobe.proto syntax = \u0026quot;proto2\u0026quot;; import \u0026quot;github.com/cloudprober/cloudprober/probes/proto/config.proto\u0026quot;; package myprober; message ProbeConf { // Redis operation required string op = 1; // Key and value for the redis operation required string key = 2; optional string value = 3; } extend cloudprober.probes.ProbeDef { optional ProbeConf redis_probe = 200; } Let\u0026rsquo;s generate Go code for this protobuf:
# From the myprober directory protoc --go_out=.,import_path=myprobe:. --proto_path=\$GOPATH/src:. myprobe/*.proto \$ ls myprobe/ myprobe.pb.go myprobe.proto Implement the probe type #Now let\u0026rsquo;s implement our probe type. Our probe type should implement the probes.Probe interface.
package myprobe // Probe holds aggregate information about all probe runs, per-target. type Probe struct { name string c *configpb.ProbeConf targets []string opts *options.Options ... } // Init initializes the probe with the given params. func (p *Probe) Init(name string, opts *options.Options) error { c, ok := opts.ProbeConf.(*ProbeConf) if !ok { return fmt.Errorf(\u0026quot;not a my probe config\u0026quot;) } // initialize p fields, p.name = name, etc. } // Start runs the probe indefinitely, at the configured interval. func (p *Probe) Start(ctx context.Context, dataChan chan *metrics.EventMetrics) { probeTicker := time.NewTicker(p.opts.Interval) for { select { case \u0026lt;-ctx.Done(): probeTicker.Stop() return case \u0026lt;-probeTicker.C: for _, em := range p.res { dataChan \u0026lt;- em } p.targets = p.opts.Targets.List() ... probeCtx, cancelFunc := context.WithDeadline(ctx, time.Now().Add(p.opts.Timeout)) p.runProbe(probeCtx) cancelFunc() } } } // runProbe runs probe for all targets and update EventMetrics. func (p *Probe) runProbe(ctx context.Context) { p.targets = p.opts.Targets.List() var wg sync.WaitGroup for _, target := range p.targets { wg.Add(1) go func(target string, em *metrics.EventMetrics) { defer wg.Done() em.Metric(\u0026quot;total\u0026quot;).(*metrics.Int).Inc() start := time.Now() err := p.runProbeForTarget(ctx, target) // run probe just for a single target if err != nil { p.l.Errorf(err.Error()) return } em.Metric(\u0026quot;success\u0026quot;).(*metrics.Int).Inc() em.Metric(\u0026quot;latency\u0026quot;).(metrics.LatencyValue).AddFloat64( time.Since(start). Seconds() / p.opts.LatencyUnit.Seconds()) }(target, p.res[target]) } wg.Wait() } Full example in examples/extensions/myprober/myprobe/myprobe.go.
This probe type sets or gets (depending on the configuration) a key-valye in redis and records success and time taken (latency) if operation is successful.
Implement a cloudprober binary that includes support for our probe #package main ... func main() { flag.Parse() // Register our probe type probes.RegisterProbeType(int(myprobe.E_RedisProbe.TypeDescriptor().Number()), func() probes.Probe { return \u0026amp;myprobe.Probe{} }) err := cloudprober.Init() // getConfig not shown here. if err != nil { glog.Exitf(\u0026quot;Error initializing cloudprober. Err: %v\u0026quot;, err) } // web.Init sets up web UI for cloudprober. web.Init() cloudprober.Start(context.Background()) // Wait forever select {} } Full example in examples/extensions/myprober/myprober.go.
Let\u0026rsquo;s write a test config that uses the newly defined probe type:
probe { name: \u0026quot;redis_set\u0026quot; type: EXTENSION interval_msec: 10000 timeout_msec: 5000 targets { host_names: \u0026quot;localhost:6379\u0026quot; } [myprober.redis_probe] { op: \u0026quot;set\u0026quot; key: \u0026quot;testkey\u0026quot; value: \u0026quot;testval\u0026quot; } } Full example in examples/extensions/myprober/myprober.cfg.
Let\u0026rsquo;s compile our prober and run it with the above config:
go run ./myprober.go --config_file=myprober.cfg you should see an output like the following:
cloudprober 1540848577649139842 1540848587 labels=ptype=redis,probe=redis_set,dst=localhost:6379 total=31 success=31 latency=70579.823 cloudprober 1540848577649139843 1540848887 labels=ptype=sysvars,probe=sysvars hostname=\u0026quot;manugarg-macbookpro5.roam.corp.google.com\u0026quot; start_timestamp=\u0026quot;1540848577\u0026quot; cloudprober 1540848577649139844 1540848887 labels=ptype=sysvars,probe=sysvars uptime_msec=310007.784 gc_time_msec=0.000 mallocs=14504 frees=826 cloudprober 1540848577649139845 1540848887 labels=ptype=sysvars,probe=sysvars goroutines=12 mem_stats_sys_bytes=7211256 cloudprober 1540848577649139846 1540848587 labels=ptype=redis,probe=redis_set,dst=localhost:6379 total=32 success=32 latency=72587.981 cloudprober 1540848577649139847 1540848897 labels=ptype=sysvars,probe=sysvars hostname=\u0026quot;manugarg-macbookpro5.roam.corp.google.com\u0026quot; start_timestamp=\u0026quot;1540848577\u0026quot; cloudprober 1540848577649139848 1540848897 labels=ptype=sysvars,probe=sysvars uptime_msec=320006.541 gc_time_msec=0.000 mallocs=14731 frees=844 cloudprober 1540848577649139849 1540848897 labels=ptype=sysvars,probe=sysvars goroutines=12 mem_stats_sys_bytes=7211256 You can import this data in prometheus following the process outlined at: Running Prometheus.
Conclusion #The article shows how to add a new probe type to cloudprober. Extending cloudprober allows you to implement new probe types that may make sense for your organization, but not for the open source community. You have to implement the logic for the probe type, but other cloudprober features work as it is \u0026ndash; targets, metrics (e.g. latency distribution if you configure it), surfacers - data can be multiple systems simultaneously, etc.
`}),e.add({id:19,href:"/docs/overview/getting-started/",title:"Getting Started",description:`Installation #From Source
If you have Go 1.9 or higher installed and GOPATH environment variable properly set up, you can download and install cloudprober using the following commands:
go get github.com/cloudprober/cloudprober GOBIN=\$GOPATH/bin go install \$GOPATH/src/github.com/cloudprober/cloudprober/cmd/cloudprober.go Pre-built Binaries
You can download pre-built binaries for Linux, MacOS and Windows from the project\u0026rsquo;s releases page.
(See this page for how to download the unreleased binaries.)
Docker Image You can download and run the latest docker image using the following command:`,content:`Installation #From Source
If you have Go 1.9 or higher installed and GOPATH environment variable properly set up, you can download and install cloudprober using the following commands:
go get github.com/cloudprober/cloudprober GOBIN=\$GOPATH/bin go install \$GOPATH/src/github.com/cloudprober/cloudprober/cmd/cloudprober.go Pre-built Binaries
You can download pre-built binaries for Linux, MacOS and Windows from the project\u0026rsquo;s releases page.
(See this page for how to download the unreleased binaries.)
Docker Image You can download and run the latest docker image using the following command:
docker run ghcr.io/cloudprober/cloudprober Configuration #Without any config, cloudprober will run only the \u0026ldquo;sysvars\u0026rdquo; module (no probes) and write metrics to stdout in cloudprober\u0026rsquo;s line protocol format (to be documented). It will also start a Prometheus exporter at: http://localhost:9313 (you can change the default port through the environment variable CLOUDPROBER_PORT and the default listening address through the environment variable CLOUDPROBER_HOST).
Since sysvars variables are not very interesting themselves, lets add a simple config that probes Google\u0026rsquo;s homepage:
# Write config to a file in /tmp cat \u0026gt; /tmp/cloudprober.cfg \u0026lt;\u0026lt;EOF probe { name: \u0026quot;google_homepage\u0026quot; type: HTTP targets { host_names: \u0026quot;www.google.com\u0026quot; } interval_msec: 5000 # 5s timeout_msec: 1000 # 1s } EOF This config adds an HTTP probe that accesses the homepage of the target \u0026ldquo;www.google.com\u0026rdquo; every 5s with a timeout of 1s. Cloudprober configuration is specified in the text protobuf format, with config schema described by the proto file: config.proto.
Assuming that you saved this file at /tmp/cloudprober.cfg (following the command above), you can have cloudprober use this config file using the following command line:
./cloudprober --config_file /tmp/cloudprober.cfg You can have the standard docker image use this config using the following command:
docker run -v /tmp/cloudprober.cfg:/etc/cloudprober.cfg \\ ghcr.io/cloudprober/cloudprober Note: While running on GCE, cloudprober config can also be provided through a custom metadata attribute: cloudprober_config.
Verification #One quick way to verify that cloudprober got the correct config is to access the URL http://localhost:9313/config (through cURL or in browser). It returns the config that cloudprober is using. You can also look at its current status at the URL (replace localhost by the actual hostname if not running locally): http://localhost:9313/status.
You should be able to see the generated metrics at http://localhost:9313/metrics (prometheus format) and the stdout (cloudprober format):
cloudprober 15.. 1500590520 labels=ptype=http,probe=google-http,dst=.. total=17 success=17 latency=180835 cloudprober 15.. 1500590530 labels=ptype=sysvars,probe=sysvars hostname=\u0026quot;manugarg-ws\u0026quot; uptime=100 cloudprober 15.. 1500590530 labels=ptype=http,probe=google-http,dst=.. total=19 success=19 latency=211644 This information is good for debugging monitoring issues, but to really make sense of this data, you\u0026rsquo;ll need to feed this data to another monitoring system like Prometheus or StackDriver (see Surfacers for more details). Lets set up a Prometheus and Grafana stack to make pretty graphs for us.
Running Prometheus #Download prometheus binary from its release page. You can use a config like the following to scrape a cloudprober instance running on the same host.
# Write config to a file in /tmp cat \u0026gt; /tmp/prometheus.yml \u0026lt;\u0026lt;EOF scrape_configs: - job_name: 'cloudprober' scrape_interval: 10s static_configs: - targets: ['localhost:9313'] EOF # Start prometheus: ./prometheus --config.file=/tmp/prometheus.yml Prometheus provides a web interface at http://localhost:9090. You can explore probe metrics and build useful graphs through this interface. All probes in cloudprober export at least 3 counters:
total: Total number of probes. success: Number of successful probes. Difference between total and success indicates failures. latency: Total (cumulative) probe latency. Using these counters, probe failure ratio and average latency can be calculated as:
failure_ratio = (rate(total) - rate(success)) / rate(total) avg_latency = rate(latency) / rate(success) Assuming that prometheus is running at localhost:9090, graphs depicting failure ratio and latency over time can be accessed in prometheus at: this url . Even though prometheus provides a graphing interface, Grafana provides much richer interface and has excellent support for prometheus.
Grafana #Grafana is a popular tool for building monitoring dashboards. Grafana has native support for prometheus and thanks to the excellent support for prometheus in Cloudprober itself, it\u0026rsquo;s a breeze to build Grafana dashboards from Cloudprober\u0026rsquo;s probe results.
To get started with Grafana, follow the Grafana-Prometheus integration guide.
`}),e.add({id:20,href:"/docs/config/metrics/",title:"Metrics Config",description:"Configs:\u0026nbsp;\u0026nbsp;main | alerting | metrics | oauth | probes | rds | servers | surfacer | targets | tlsconfig | utils | validators YAML|TextPB cloudprober.metrics.Dist # [explicit_buckets \u0026lt;string\u0026gt; | exponential_buckets \u0026lt;cloudprober.metrics.ExponentialBuckets\u0026gt;]: \u0026lt;oneof\u003e cloudprober.metrics.ExponentialBuckets # scale_factor: \u0026lt;float\u003e base: \u0026lt;float\u003e num_buckets: \u0026lt;uint32\u003e cloudprober.metrics.payload.OutputMetricsOptions # metrics_kind: (UNDEFINED|GAUGE|CUMULATIVE): \u0026lt;enum\u003e # Additional labels (comma-separated) to attach to the output metrics, e.g. # \u0026#34;region=us-east1,zone=us-east1-d\u0026#34;. ptype=\u0026#34;external\u0026#34; and probe=\u0026#34;\u0026lt;probeName\u0026gt;\u0026#34; # are attached automatically.additional_labels: \u0026lt;string\u003e # Whether to aggregate metrics in Cloudprober.",content:" Configs:\u0026nbsp;\u0026nbsp;main | alerting | metrics | oauth | probes | rds | servers | surfacer | targets | tlsconfig | utils | validators YAML|TextPB cloudprober.metrics.Dist # [explicit_buckets \u0026lt;string\u0026gt; | exponential_buckets \u0026lt;cloudprober.metrics.ExponentialBuckets\u0026gt;]: \u0026lt;oneof\u003e cloudprober.metrics.ExponentialBuckets # scale_factor: \u0026lt;float\u003e base: \u0026lt;float\u003e num_buckets: \u0026lt;uint32\u003e cloudprober.metrics.payload.OutputMetricsOptions # metrics_kind: (UNDEFINED|GAUGE|CUMULATIVE): \u0026lt;enum\u003e # Additional labels (comma-separated) to attach to the output metrics, e.g. # \u0026#34;region=us-east1,zone=us-east1-d\u0026#34;. ptype=\u0026#34;external\u0026#34; and probe=\u0026#34;\u0026lt;probeName\u0026gt;\u0026#34; # are attached automatically.additional_labels: \u0026lt;string\u003e # Whether to aggregate metrics in Cloudprober. If enabled, Cloudprober # aggregates the metrics returned by the external probe process -- external # probe process should return metrics only since the last probe run. # Note that this option is mutually exclusive with GAUGE metrics and # cloudprober will fail during initialization if both options are enabled.aggregate_in_cloudprober: \u0026lt;bool\u003e | default: false # Metrics that should be treated as distributions. These metrics are exported # by the external probe program as comma-separated list of values, for # example: \u0026#34;op_latency 4.7,5.6,5.9,6.1,4.9\u0026#34;. To be able to build distribution # from these values, these metrics should be pre-configured in external # probe: # dist_metric { # key: \u0026#34;op_latency\u0026#34; # value { # explicit_buckets: \u0026#34;1,2,4,8,16,32,64,128,256\u0026#34; # } # }dist_metric: \u0026lt;cloudprober.metrics.payload.OutputMetricsOptions.DistMetricEntry\u003e cloudprober.metrics.payload.OutputMetricsOptions.DistMetricEntry # key: \u0026lt;string\u003e value: \u0026lt;cloudprober.metrics.Dist\u003e cloudprober.metrics.Dist # [explicit_buckets \u0026lt;string\u0026gt; | exponential_buckets \u0026lt;cloudprober.metrics.ExponentialBuckets\u0026gt;]: \u0026lt;oneof\u003e cloudprober.metrics.ExponentialBuckets # scale_factor: \u0026lt;float\u003e base: \u0026lt;float\u003e num_buckets: \u0026lt;uint32\u003e cloudprober.metrics.payload.OutputMetricsOptions # metrics_kind: (UNDEFINED|GAUGE|CUMULATIVE): \u0026lt;enum\u003e # Additional labels (comma-separated) to attach to the output metrics, e.g. # \u0026#34;region=us-east1,zone=us-east1-d\u0026#34;. ptype=\u0026#34;external\u0026#34; and probe=\u0026#34;\u0026lt;probeName\u0026gt;\u0026#34; # are attached automatically.additional_labels: \u0026lt;string\u003e # Whether to aggregate metrics in Cloudprober. If enabled, Cloudprober # aggregates the metrics returned by the external probe process -- external # probe process should return metrics only since the last probe run. # Note that this option is mutually exclusive with GAUGE metrics and # cloudprober will fail during initialization if both options are enabled.aggregate_in_cloudprober: \u0026lt;bool\u003e | default: false # Metrics that should be treated as distributions. These metrics are exported # by the external probe program as comma-separated list of values, for # example: \u0026#34;op_latency 4.7,5.6,5.9,6.1,4.9\u0026#34;. To be able to build distribution # from these values, these metrics should be pre-configured in external # probe: # dist_metric { # key: \u0026#34;op_latency\u0026#34; # value { # explicit_buckets: \u0026#34;1,2,4,8,16,32,64,128,256\u0026#34; # } # }dist_metric: \u0026lt;cloudprober.metrics.payload.OutputMetricsOptions.DistMetricEntry\u003e cloudprober.metrics.payload.OutputMetricsOptions.DistMetricEntry # key: \u0026lt;string\u003e value: \u0026lt;cloudprober.metrics.Dist\u003e "}),e.add({id:21,href:"/docs/config/oauth/",title:"OAuth Config",description:"Configs:\u0026nbsp;\u0026nbsp;main | alerting | metrics | oauth | probes | rds | servers | surfacer | targets | tlsconfig | utils | validators YAML|TextPB cloudprober.oauth.BearerToken # [file \u0026lt;string\u0026gt; | cmd \u0026lt;string\u0026gt; | \u0026nbsp;gce_service_account \u0026lt;string\u0026gt; | k8s_local_token \u0026lt;bool\u0026gt;]: \u0026lt;oneof\u003e # If above sources return JSON tokens with an expiry, we use that info to # determine when to refresh tokens and refresh_interval_sec is completely # ignored. If above sources return a string, we refresh from the source # every 30s by default.",content:" Configs:\u0026nbsp;\u0026nbsp;main | alerting | metrics | oauth | probes | rds | servers | surfacer | targets | tlsconfig | utils | validators YAML|TextPB cloudprober.oauth.BearerToken # [file \u0026lt;string\u0026gt; | cmd \u0026lt;string\u0026gt; | \u0026nbsp;gce_service_account \u0026lt;string\u0026gt; | k8s_local_token \u0026lt;bool\u0026gt;]: \u0026lt;oneof\u003e # If above sources return JSON tokens with an expiry, we use that info to # determine when to refresh tokens and refresh_interval_sec is completely # ignored. If above sources return a string, we refresh from the source # every 30s by default. To disable this behavior set refresh_interval_sec to # zero.refresh_interval_sec: \u0026lt;float\u003e cloudprober.oauth.Config # [http_request \u0026lt;cloudprober.oauth.HTTPRequest\u0026gt; | bearer_token \u0026lt;cloudprober.oauth.BearerToken\u0026gt; | \u0026nbsp;google_credentials \u0026lt;cloudprober.oauth.GoogleCredentials\u0026gt;]: \u0026lt;oneof\u003e # How long before the expiry do we refresh. Default is 60 (1m). This applies # only to http_request and bearer_token types, and only if token presents # expiry in some way. # TODO(manugarg): Consider setting default based on probe interval.refresh_expiry_buffer_sec: \u0026lt;int32\u003e cloudprober.oauth.GoogleCredentials # json_file: \u0026lt;string\u003e scope: \u0026lt;string\u003e # Use encoded JWT directly as access token, instead of implementing the whole # OAuth2.0 flow.jwt_as_access_token: \u0026lt;bool\u003e # Audience works only if jwt_as_access_token is true.audience: \u0026lt;string\u003e cloudprober.oauth.HTTPRequest # token_url: \u0026lt;string\u003e method: \u0026lt;string\u003e # Data to be sent as request body. If there are multiple \u0026#34;data\u0026#34; fields, we combine # their values with a \u0026#39;\u0026amp;\u0026#39; in between. Note: 1) If data appears to be a valid json, # we automatically set the content-type header to \u0026#34;application/json\u0026#34;, 2) If data # appears to be a query string we set content-type to # \u0026#34;application/x-www-form-urlencoded\u0026#34;. Content type header can still be overridden # using the header field below.data: \u0026lt;string\u003e # HTTP request headersheader: \u0026lt;cloudprober.oauth.HTTPRequest.HeaderEntry\u003e cloudprober.oauth.HTTPRequest.HeaderEntry # key: \u0026lt;string\u003e value: \u0026lt;string\u003e cloudprober.oauth.BearerToken # [file \u0026lt;string\u0026gt; | cmd \u0026lt;string\u0026gt; | \u0026nbsp;gce_service_account \u0026lt;string\u0026gt; | k8s_local_token \u0026lt;bool\u0026gt;]: \u0026lt;oneof\u003e # If above sources return JSON tokens with an expiry, we use that info to # determine when to refresh tokens and refresh_interval_sec is completely # ignored. If above sources return a string, we refresh from the source # every 30s by default. To disable this behavior set refresh_interval_sec to # zero.refresh_interval_sec: \u0026lt;float\u003e cloudprober.oauth.Config # [http_request \u0026lt;cloudprober.oauth.HTTPRequest\u0026gt; | bearer_token \u0026lt;cloudprober.oauth.BearerToken\u0026gt; | \u0026nbsp;google_credentials \u0026lt;cloudprober.oauth.GoogleCredentials\u0026gt;]: \u0026lt;oneof\u003e # How long before the expiry do we refresh. Default is 60 (1m). This applies # only to http_request and bearer_token types, and only if token presents # expiry in some way. # TODO(manugarg): Consider setting default based on probe interval.refresh_expiry_buffer_sec: \u0026lt;int32\u003e cloudprober.oauth.GoogleCredentials # json_file: \u0026lt;string\u003e scope: \u0026lt;string\u003e # Use encoded JWT directly as access token, instead of implementing the whole # OAuth2.0 flow.jwt_as_access_token: \u0026lt;bool\u003e # Audience works only if jwt_as_access_token is true.audience: \u0026lt;string\u003e cloudprober.oauth.HTTPRequest # token_url: \u0026lt;string\u003e method: \u0026lt;string\u003e # Data to be sent as request body. If there are multiple \u0026#34;data\u0026#34; fields, we combine # their values with a \u0026#39;\u0026amp;\u0026#39; in between. Note: 1) If data appears to be a valid json, # we automatically set the content-type header to \u0026#34;application/json\u0026#34;, 2) If data # appears to be a query string we set content-type to # \u0026#34;application/x-www-form-urlencoded\u0026#34;. Content type header can still be overridden # using the header field below.data: \u0026lt;string\u003e # HTTP request headersheader: \u0026lt;cloudprober.oauth.HTTPRequest.HeaderEntry\u003e cloudprober.oauth.HTTPRequest.HeaderEntry # key: \u0026lt;string\u003e value: \u0026lt;string\u003e "}),e.add({id:22,href:"/docs/overview/cloudprober/",title:"Overview",description:`Cloudprober provides a reliable and easy-to-use solution to monitor the availability and performance of your systems. Employing an \u0026ldquo;active\u0026rdquo; monitoring approach, Cloudprober executes probes on or against these systems to verify their proper functioning.
For example, you could use Cloudprober to run a probe to verify that your users can access your website and your APIs, your microservices can talk to each other, your kubernetes clusters can schedule pods, your CI/CD pipelines are functioning as expected, or VPN connectivity with your partners is working as expected, and much more.`,content:` Cloudprober provides a reliable and easy-to-use solution to monitor the availability and performance of your systems. Employing an \u0026ldquo;active\u0026rdquo; monitoring approach, Cloudprober executes probes on or against these systems to verify their proper functioning.
For example, you could use Cloudprober to run a probe to verify that your users can access your website and your APIs, your microservices can talk to each other, your kubernetes clusters can schedule pods, your CI/CD pipelines are functioning as expected, or VPN connectivity with your partners is working as expected, and much more.
This kind of monitoring makes it possible to monitor your systems\u0026rsquo; interfaces regardless of the implementation, and helps you quickly pin down what\u0026rsquo;s broken in your systems.
Features #Out of the box, config based, integration with many popular monitoring systems:
Prometheus/Grafana DataDog PostgreSQL AWS CloudWatch StackDriver / Google Cloud Monitoring Multiple options for checks:
Efficient, highly scalable, built-in probes: HTTP, PING, TCP, DNS, gRPC, UDP. Run custom checks through the \u0026quot;external\u0026quot; probe type. Automated targets discovery to make Cloud deployments as painless as possible:
Kubernetes resources. GCP instances, forwarding rules, and pub/sub messages. File based targets. Deployment friendly:
Written entirely in Go, and compiles into a static binary. Deploy as a standalone binary, or through docker containers. Continuous, automated target discovery, to ensure that most infrastructure changes don\u0026rsquo;t require re-deployment. Low footprint. Cloudprober takes advantage of the Go\u0026rsquo;s concurrency paradigms, and makes most of the available processing power. Configurable metrics:
Configurable metrics labels, based on the resource labels. Latency histograms for percentile calculations. Extensible architecture. Cloudprober can be easily extended along most of the dimensions. Adding support for other Cloud targets, monitoring systems and even a new probe type, is straight-forward and fairly easy. Getting Started #Visit Getting Started page to get started with Cloudprober.
Feedback #We\u0026rsquo;d love to hear your feedback. If you\u0026rsquo;re using Cloudprober, would you please mind sharing how you use it by adding a comment here. It will be a great help in planning Cloudprober\u0026rsquo;s future progression.
Join Cloudprober Slack or Github discussions for questions and discussion about Cloudprober.
`}),e.add({id:23,href:"/docs/config/probes/",title:"Probes Config",description:"Configs:\u0026nbsp;\u0026nbsp;main | alerting | metrics | oauth | probes | rds | servers | surfacer | targets | tlsconfig | utils | validators YAML|TextPB cloudprober.probes.AdditionalLabel # key: \u0026lt;string\u003e # Value can either be a static value or can be derived from target\u0026#39;s labels. # To get value from target\u0026#39;s labels, use target.labels.\u0026lt;target\u0026#39;s label key\u0026gt; # as value.value: \u0026lt;string\u003e cloudprober.probes.DebugOptions # # Whether to log metrics or not.log_metrics: \u0026lt;bool\u003e cloudprober.probes.ProbeDef # # Probe name.",content:" Configs:\u0026nbsp;\u0026nbsp;main | alerting | metrics | oauth | probes | rds | servers | surfacer | targets | tlsconfig | utils | validators YAML|TextPB cloudprober.probes.AdditionalLabel # key: \u0026lt;string\u003e # Value can either be a static value or can be derived from target\u0026#39;s labels. # To get value from target\u0026#39;s labels, use target.labels.\u0026lt;target\u0026#39;s label key\u0026gt; # as value.value: \u0026lt;string\u003e cloudprober.probes.DebugOptions # # Whether to log metrics or not.log_metrics: \u0026lt;bool\u003e cloudprober.probes.ProbeDef # # Probe name. It should be unique across all probes.name: \u0026lt;string\u003e type: (PING|HTTP|DNS|EXTERNAL|UDP|UDP_LISTENER|GRPC|TCP|EXTENSION|USER_DEFINED): \u0026lt;enum\u003e # Interval between two probe runs in milliseconds. # Only one of \u0026#34;interval\u0026#34; and \u0026#34;inteval_msec\u0026#34; should be defined. # Default interval is 2s.interval_msec: \u0026lt;int32\u003e # Interval between two probe runs in string format, e.g. 10s. # Only one of \u0026#34;interval\u0026#34; and \u0026#34;inteval_msec\u0026#34; should be defined. # Default interval is 2s.interval: \u0026lt;string\u003e # Timeout for each probe in milliseconds # Only one of \u0026#34;timeout\u0026#34; and \u0026#34;timeout_msec\u0026#34; should be defined. # Default timeout is 1s.timeout_msec: \u0026lt;int32\u003e # Timeout for each probe in string format, e.g. 10s. # Only one of \u0026#34;timeout\u0026#34; and \u0026#34;timeout_msec\u0026#34; should be defined. # Default timeout is 1s.timeout: \u0026lt;string\u003e # Targets for the probe. Targets are required for all probes except # for external, user_defined, and extension probe types.targets: \u0026lt;cloudprober.targets.TargetsDef\u003e # Latency distribution. If specified, latency is stored as a distribution.latency_distribution: \u0026lt;cloudprober.metrics.Dist\u003e # Latency unit. Any string that\u0026#39;s parseable by time.ParseDuration. # Valid values: \u0026#34;ns\u0026#34;, \u0026#34;us\u0026#34; (or \u0026#34;Âµs\u0026#34;), \u0026#34;ms\u0026#34;, \u0026#34;s\u0026#34;, \u0026#34;m\u0026#34;, \u0026#34;h\u0026#34;.latency_unit: \u0026lt;string\u003e | default: us # Latency metric name. You may want to change the latency metric name, if: # you\u0026#39;re using latency_distribution for some probes, and regular metric for # other probes, and you want to differentiate between the two. # For example: # probe { # name: \u0026#34;web1_latency\u0026#34; # latency_distribution: {...} # latency_metric_name: \u0026#34;latency_dist\u0026#34; # ... # } # probe { # name: \u0026#34;app1\u0026#34; # ... # }latency_metric_name: \u0026lt;string\u003e | default: latency # Validators for this probe. Validators are run on the data returned by the # probe. See https://cloudprober.org/docs/how-to/validators/ for more info.validator: \u0026lt;cloudprober.validators.Validator\u003e # Set the source IP to send packets from, either by providing an IP address # directly, or a network interface.[source_ip \u0026lt;string\u0026gt; | source_interface \u0026lt;string\u0026gt;]: \u0026lt;oneof\u003e ip_version: (IP_VERSION_UNSPECIFIED|IPV4|IPV6): \u0026lt;enum\u003e # How often to export stats. Probes usually run at a higher frequency (e.g. # every second); stats from individual probes are aggregated within # cloudprober until exported. In most cases, users don\u0026#39;t need to change the # default. # # By default this field is set in the following way: # For all probes except UDP: # stats_export_interval=max(interval, 10s) # For UDP: # stats_export_interval=max(2*max(interval, timeout), 10s)stats_export_interval_msec: \u0026lt;int32\u003e # Additional labels to add to the probe results. Label\u0026#39;s value can either be # static or can be derived from target\u0026#39;s labels. # # Example: # additional_label { # key: \u0026#34;app\u0026#34; # value: \u0026#34;@target.label.app@\u0026#34; # } # (More detailed example at: examples/additional_label/cloudprober.cfg)additional_label: \u0026lt;cloudprober.probes.AdditionalLabel\u003e # (Experimental) If set, test is inversed, i.e. we count it as success if # target doesn\u0026#39;t respond. This is useful, for example, that your firewall is # working as expected. # # This is currently implemented only by PING and TCP probes. # Note: This field is currently experimental, and may change in future.negative_test: \u0026lt;bool\u003e # Alerts configuration. If specified, cloudprober will generate alerts on # probe failures. You can specify multiple alerts. # Example: # alert { # name: \u0026#34;alert1\u0026#34; # condition {...} # notify { # pagerduty { ...} # } # } # alert { # name: \u0026#34;alert2\u0026#34; # notify { ... } # }alert: \u0026lt;cloudprober.alerting.AlertConf\u003e [ping_probe \u0026lt;cloudprober.probes.ping.ProbeConf\u0026gt; | http_probe \u0026lt;cloudprober.probes.http.ProbeConf\u0026gt; | \u0026nbsp;dns_probe \u0026lt;cloudprober.probes.dns.ProbeConf\u0026gt; | external_probe \u0026lt;cloudprober.probes.external.ProbeConf\u0026gt; | \u0026nbsp;udp_probe \u0026lt;cloudprober.probes.udp.ProbeConf\u0026gt; | udp_listener_probe \u0026lt;cloudprober.probes.udplistener.ProbeConf\u0026gt; | \u0026nbsp;grpc_probe \u0026lt;cloudprober.probes.grpc.ProbeConf\u0026gt; | tcp_probe \u0026lt;cloudprober.probes.tcp.ProbeConf\u0026gt; | \u0026nbsp;user_defined_probe \u0026lt;string\u0026gt;]: \u0026lt;oneof\u003e # Which machines this probe should run on. If defined, cloudprober will run # this probe only if machine\u0026#39;s hostname matches this value. This is useful # for large deployments, where you may want to use the same prober config # everywhere but run this probe only on a subset of machines.run_on: \u0026lt;string\u003e # Schedule for the probe. You can use a schedule to specify when a probe # should or should not run. This is useful for running probes only during # business hours. # # You can specify multiple schedules. Probe will not run if any of the # \u0026#34;DISABLE\u0026#34; schedules are active. If both \u0026#34;ENABLE\u0026#34; and \u0026#34;DISABLE\u0026#34; schedules # overlap, \u0026#34;DISABLE\u0026#34; takes precedence. # # For example, to disable a probe during weekends and on Tuesday between 7pm # and 8pm, e.g. for rollouts: # schdule { # type: DISABLE # start_weekday: FRIDAY # start_time: \u0026#34;20:00\u0026#34; # end_weekday: SUNDAY # end_time: \u0026#34;17:00\u0026#34; # timezone: \u0026#34;America/New_York\u0026#34; # } # schdule { # type: DISABLE # start_weekday: TUESDAY # start_time: \u0026#34;19:00\u0026#34; # end_weekday: TUESDAY # end_time: \u0026#34;20:00\u0026#34; # timezone: \u0026#34;America/New_York\u0026#34; # }schedule: \u0026lt;cloudprober.probes.Schedule\u003e # Debug options. Currently only used to enable logging metrics.debug_options: \u0026lt;cloudprober.probes.DebugOptions\u003e cloudprober.probes.Schedule # type: (ScheduleType_UNSPECIFIED|ENABLE|DISABLE): \u0026lt;enum\u003e # Period start weekday. If not specified, it defaults to EVERYDAY.start_weekday: (EVERYDAY|SUNDAY|MONDAY|TUESDAY|WEDNESDAY|THURSDAY|FRIDAY|SATURDAY): \u0026lt;enum\u003e # Start time in 24 hour HH:MM format.start_time: \u0026lt;string\u003e | default: 00:00 # Period end weekday. If not specified, it defaults to EVERYDAY.end_weekday: (EVERYDAY|SUNDAY|MONDAY|TUESDAY|WEDNESDAY|THURSDAY|FRIDAY|SATURDAY): \u0026lt;enum\u003e # End time in 24 hour HH:MM format.end_time: \u0026lt;string\u003e | default: 23:59 # Timezone in which the probe should run. If not specified, it defaults to # UTC. Example: \u0026#34;America/New_York\u0026#34;timezone: \u0026lt;string\u003e | default: UTC cloudprober.probes.dns.ProbeConf # # Domain to use when making DNS queriesresolved_domain: \u0026lt;string\u003e | default: www.google.com. # DNS Query Typequery_type: (NONE|A|NS|CNAME|SOA|PTR|MX|TXT|RP|AFSDB|SIG|KEY|AAAA|LOC|SRV|NAPTR|KX|CERT|DNAME|APL|DS|SSHFP|IPSECKEY|RRSIG|NSEC|DNSKEY|DHCID|NSEC3|NSEC3PARAM|TLSA|HIP|CDS|CDNSKEY|OPENPGPKEY|TKEY|TSIG|URI|CAA|TA|DLV): \u0026lt;enum\u003e # Minimum number of answers expected. Default behavior is to return success # if DNS response status is NOERROR.min_answers: \u0026lt;uint32\u003e | default: 0 # Whether to resolve the target (target is DNS server here) before making # the request. If set to false, we hand over the target directly to the DNS # client. Otherwise, we resolve the target first to an IP address. By # default we resolve first if it\u0026#39;s a discovered resource, e.g., a k8s # endpoint.resolve_first: \u0026lt;bool\u003e # Requests per probe. # Number of DNS requests per probe. Requests are executed concurrently and # each DNS request contributes to probe results. For example, if you run two # requests per probe, \u0026#34;total\u0026#34; counter will be incremented by 2.requests_per_probe: \u0026lt;int32\u003e | default: 1 # How long to wait between two requests to the same target. Only relevant # if requests_per_probe is also configured. # # This value should be less than (interval - timeout) / requests_per_probe. # This is to ensure that all requests are executed within one probe interval # and all of them get sufficient time. For example, if probe interval is 2s, # timeout is 1s, and requests_per_probe is 10, requests_interval_msec # should be less than 10ms.requests_interval_msec: \u0026lt;int32\u003e | default: 0 cloudprober.probes.external.ProbeConf # mode: (ONCE|SERVER): \u0026lt;enum\u003e # Command. For ONCE probes, arguments are processed for the following field # substitutions: # @probe@ Name of the probe # @target@ Hostname of the target # @address@ IP address of the target # # For example, for target ig-us-central1-a, /tools/recreate_vm -vm @target@ # will get converted to: /tools/recreate_vm -vm ig-us-central1-acommand: \u0026lt;string\u003e # Command environment variables. These are passed on to the external probe # process as environment variables.env_var: \u0026lt;cloudprober.probes.external.ProbeConf.EnvVarEntry\u003e options: \u0026lt;cloudprober.probes.external.ProbeConf.Option\u003e # Export output as metrics, where output is the output returned by the # external probe process, over stdout for ONCE probes, and through ProbeReply # for SERVER probes. Cloudprober expects variables to be in the following # format in the output: # var1 value1 (for example: total_errors 589)output_as_metrics: \u0026lt;bool\u003e | default: true output_metrics_options: \u0026lt;cloudprober.metrics.payload.OutputMetricsOptions\u003e cloudprober.probes.external.ProbeConf.EnvVarEntry # key: \u0026lt;string\u003e value: \u0026lt;string\u003e cloudprober.probes.external.ProbeConf.Option # name: \u0026lt;string\u003e value: \u0026lt;string\u003e cloudprober.probes.grpc.GenericRequest # # Protoset contains descriptor source protos generated from the *.proto # files. You can use protoc to generate protoset files: # protoc --proto_path=. --descriptor_set_out=myservice.protoset \\ # --include_imports my/custom/server/service.protoprotoset_file: \u0026lt;string\u003e # Note first 3 methods are valid only if descriptor source is not set.[list_services \u0026lt;bool\u0026gt; | list_service_methods \u0026lt;string\u0026gt; | \u0026nbsp;describe_service_method \u0026lt;string\u0026gt; | call_service_method \u0026lt;string\u0026gt;]: \u0026lt;oneof\u003e # Request data (in JSON format) for the call_service_method request.body: \u0026lt;string\u003e cloudprober.probes.grpc.ProbeConf # # Optional oauth config. For GOOGLE_DEFAULT_CREDENTIALS, use: # oauth_config: { bearer_token { gce_service_account: \u0026#34;default\u0026#34; } }oauth_config: \u0026lt;cloudprober.oauth.Config\u003e # If alts_config is provided, gRPC client uses ALTS for authentication and # encryption. For default alts configs, use: # alts_config: {}alts_config: \u0026lt;cloudprober.probes.grpc.ProbeConf.ALTSConfig\u003e # If TLSConfig is specified, it\u0026#39;s used for authentication. # Note that only one of ALTSConfig and TLSConfig can be enabled at a time.tls_config: \u0026lt;cloudprober.tlsconfig.TLSConfig\u003e # if insecure_transport is set to true, TLS will not be used.insecure_transport: \u0026lt;bool\u003e method: (ECHO|READ|WRITE|HEALTH_CHECK|GENERIC): \u0026lt;enum\u003e # Blob size for ECHO, READ, and WRITE methods.blob_size: \u0026lt;int32\u003e | default: 1024 # For HEALTH_CHECK, name of the service to health check.health_check_service: \u0026lt;string\u003e # For HEALTH_CHECK, ignore status. By default, HEALTH_CHECK test passes # only if response-status is SERVING. Setting the following option makes # HEALTH_CHECK pass regardless of the response-status.health_check_ignore_status: \u0026lt;bool\u003e # Request definition for the GENERIC method.request: \u0026lt;cloudprober.probes.grpc.GenericRequest\u003e num_conns: \u0026lt;int32\u003e | default: 2 keep_alive: \u0026lt;bool\u003e | default: true # If connect_timeout is not specified, reuse probe timeout.connect_timeout_msec: \u0026lt;int32\u003e # URI scheme allows gRPC to use different resolvers # Example URI scheme: \u0026#34;google-c2p:///\u0026#34; # See https://github.com/grpc/grpc/blob/master/doc/naming.md for more detailsuri_scheme: \u0026lt;string\u003e headers: \u0026lt;cloudprober.probes.grpc.ProbeConf.Header\u003e cloudprober.probes.grpc.ProbeConf.ALTSConfig # # If provided, ALTS verifies that peer is using one of the given service # accounts.target_service_account: \u0026lt;string\u003e # Handshaker service address. Default is to use the local metadata server. # For most of the ALTS use cases, default address should be okay.handshaker_service_address: \u0026lt;string\u003e cloudprober.probes.grpc.ProbeConf.Header # name: \u0026lt;string\u003e value: \u0026lt;string\u003e cloudprober.probes.http.ProbeConf # # HTTP request scheme (Corresponding target label: \u0026#34;scheme\u0026#34;). If not set, we # use taget\u0026#39;s \u0026#39;scheme\u0026#39; label if present. # Note: protocol is deprecated, use scheme instead.[protocol: (HTTP|HTTPS) | scheme: (HTTP|HTTPS)]: \u0026lt;oneof\u003e # Relative URL (Corresponding target label: \u0026#34;path\u0026#34;). We construct the final # URL like this: # \u0026lt;scheme\u0026gt;://\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;/\u0026lt;relative_url\u0026gt;. # # Note that the relative_url should start with a \u0026#39;/\u0026#39;.relative_url: \u0026lt;string\u003e # Port for HTTP requests (Corresponding target field: port) # Default is to use the scheme specific port, but if this field is not # set and discovered target has a port (e.g., k8s services, ingresses), # we use target\u0026#39;s port.port: \u0026lt;int32\u003e # Whether to resolve the target before making the request. If set to true, # we resolve the target first to an IP address and make a request using # that while passing target name (or \u0026#39;host\u0026#39; label if present) as Host # header. # # This behavior is automatic for discovered targets if they have an IP # address associated with them. Usually you don\u0026#39;t need to worry about this # field and you can left it unspecified. We\u0026#39;ll ty to do the right thing.resolve_first: \u0026lt;bool\u003e # Export response (body) count as a metricexport_response_as_metrics: \u0026lt;bool\u003e | default: false # HTTP request methodmethod: (GET|POST|PUT|HEAD|DELETE|PATCH|OPTIONS): \u0026lt;enum\u003e # HTTP request headers # It is recommended to use \u0026#34;header\u0026#34; instead of \u0026#34;headers\u0026#34; for new configs. # header { # key: \u0026#34;Authorization\u0026#34; # value: \u0026#34;Bearer {{env \u0026#34;AUTH_TOKEN\u0026#34;}}\u0026#34; # } headers: \u0026lt;cloudprober.probes.http.ProbeConf.Header\u003e header: \u0026lt;cloudprober.probes.http.ProbeConf.HeaderEntry\u003e # Request body. This field works similar to the curl\u0026#39;s data flag. If there # are multiple \u0026#34;body\u0026#34; fields, we combine their values with a \u0026#39;\u0026amp;\u0026#39; in between. # # Also, we try to guess the content-type header based on the data: # 1) If data appears to be a valid json, we automatically set the # content-type header to \u0026#34;application/json\u0026#34;. # 2) If the final data string appears to be a valid query string, we # set content-type to \u0026#34;application/x-www-form-urlencoded\u0026#34;. Content type # header can still be overridden using the header field above. # Example: # body: \u0026#34;grant_type=client_credentials\u0026#34; # body: \u0026#34;scope=transferMoney\u0026#34; # body: \u0026#34;clientId=aweseomeClient\u0026#34; # body: \u0026#34;clientSecret=noSecret\u0026#34;body: \u0026lt;string\u003e # Enable HTTP keep-alive. If set to true, underlying connection is reused # for further probes. Default is to close the connection after every request.keep_alive: \u0026lt;bool\u003e # OAuth Configoauth_config: \u0026lt;cloudprober.oauth.Config\u003e # Disable HTTP2 # Golang HTTP client automatically enables HTTP/2 if server supports it. This # option disables that behavior to enforce HTTP/1.1 for testing purpose.disable_http2: \u0026lt;bool\u003e # Disable TLS certificate validation. If set to true, any certificate # presented by the server for any host name will be accepted # Deprecation: This option is now subsumed by the tls_config below. To # disable cert validation use: # tls_config { # disable_cert_validation: true # }disable_cert_validation: \u0026lt;bool\u003e # TLS configtls_config: \u0026lt;cloudprober.tlsconfig.TLSConfig\u003e # Proxy URL, e.g. http://myproxy:3128proxy_url: \u0026lt;string\u003e # User agent. Default user agent is Go\u0026#39;s default user agent.user_agent: \u0026lt;string\u003e # Maximum idle connections to keep alivemax_idle_conns: \u0026lt;int32\u003e | default: 256 # The maximum amount of redirects the HTTP client will follow. # To disable redirects, use max_redirects: 0.max_redirects: \u0026lt;int32\u003e # Interval between targets.interval_between_targets_msec: \u0026lt;int32\u003e | default: 10 # Requests per probe. # Number of HTTP requests per probe. Requests are executed concurrently and # each HTTP re contributes to probe results. For example, if you run two # requests per probe, \u0026#34;total\u0026#34; counter will be incremented by 2.requests_per_probe: \u0026lt;int32\u003e | default: 1 # How long to wait between two requests to the same target. Only relevant # if requests_per_probe is also configured. # # This value should be less than (interval - timeout) / requests_per_probe. # This is to ensure that all requests are executed within one probe interval # and all of them get sufficient time. For example, if probe interval is 2s, # timeout is 1s, and requests_per_probe is 10, requests_interval_msec # should be less than 10ms.requests_interval_msec: \u0026lt;int32\u003e | default: 0 cloudprober.probes.http.ProbeConf.Header # name: \u0026lt;string\u003e value: \u0026lt;string\u003e cloudprober.probes.http.ProbeConf.HeaderEntry # key: \u0026lt;string\u003e value: \u0026lt;string\u003e cloudprober.probes.ping.ProbeConf # # Packets per probepackets_per_probe: \u0026lt;int32\u003e | default: 2 # How long to wait between two packets to the same targetpackets_interval_msec: \u0026lt;int32\u003e | default: 25 # Resolve targets after these many probesresolve_targets_interval: \u0026lt;int32\u003e | default: 5 # Ping payload size in bytes. It cannot be smaller than 8, number of bytes # required for the nanoseconds timestamp.payload_size: \u0026lt;int32\u003e | default: 56 # Use datagram socket for ICMP. # This option enables unprivileged pings (that is, you don\u0026#39;t require root # privilege to send ICMP packets). Note that most of the Linux distributions # don\u0026#39;t allow unprivileged pings by default. To enable unprivileged pings on # some Linux distributions, you may need to run the following command: # # sudo sysctl -w net.ipv4.ping_group_range=\u0026#34;0 \u0026lt;large valid group id\u0026gt;\u0026#34; # # net.ipv4.ping_group_range system setting takes two integers that specify # the group id range that is allowed to execute the unprivileged pings. Note # that the same setting (with ipv4 in the path) applies to IPv6 as well. # # Note: This option is not supported on Windows and is automatically # disabled there.use_datagram_socket: \u0026lt;bool\u003e | default: true # Disable integrity checks. To detect data courruption in the network, we # craft the outgoing ICMP packet payload in a certain format and verify that # the reply payload matches the same format.disable_integrity_check: \u0026lt;bool\u003e | default: false # Do not allow OS-level fragmentation, only works on Linux systems.disable_fragmentation: \u0026lt;bool\u003e | default: false cloudprober.probes.tcp.ProbeConf # # Port for TCP requests. If not specfied, and port is provided by the # targets (e.g. kubernetes endpoint or service), that port is used.port: \u0026lt;int32\u003e # Whether to resolve the target before making the request. If set to false, # we hand over the target golang\u0026#39;s net.Dial module, Otherwise, we resolve # the target first to an IP address and make a request using that. By # default we resolve first if it\u0026#39;s a discovered resource, e.g., a k8s # endpoint.resolve_first: \u0026lt;bool\u003e # Interval between targets.interval_between_targets_msec: \u0026lt;int32\u003e | default: 10 cloudprober.probes.udp.ProbeConf # # Port to send UDP Ping to (UDP Echo). If running with the UDP server that # comes with cloudprober, it should be same as # ProberConfig.udp_echo_server_port.port: \u0026lt;int32\u003e | default: 31122 # Number of sending side ports to use.num_tx_ports: \u0026lt;int32\u003e | default: 16 # message max to account for MTU.max_length: \u0026lt;int32\u003e | default: 1300 # Payload sizepayload_size: \u0026lt;int32\u003e # Changes the exported monitoring streams to be per port: # 1. Changes the streams names to total-per-port, success-per-port etc. # 2. Adds src_port and dst_port as stream labels. # Note that the field name is experimental and may change in the future.export_metrics_by_port: \u0026lt;bool\u003e | default: false # Whether to use all transmit ports per probe, per target. # Default is to probe each target once per probe and round-robin through the # source ports. # Setting this field to true changes the behavior to send traffic from all # ports to all targets in each probe. # For example, if num_tx_ports is set to 16, in every probe cycle, we\u0026#39;ll send # 16 packets to every target (1 per tx port). # Note that setting this field to true will increase the probe traffic.use_all_tx_ports_per_probe: \u0026lt;bool\u003e | default: false # maxTargets is the maximum number of targets supported by this probe type. # If there are more targets, they are pruned from the list to bring targets # list under maxTargets. A large number of targets has impact on resource # consumption.max_targets: \u0026lt;int32\u003e | default: 500 cloudprober.probes.udplistener.ProbeConf # # Port to listen.port: \u0026lt;int32\u003e | default: 32212 type: (INVALID|ECHO|DISCARD): \u0026lt;enum\u003e # Number of packets sent in a single probe.packets_per_probe: \u0026lt;int32\u003e | default: 1 cloudprober.probes.AdditionalLabel # key: \u0026lt;string\u003e # Value can either be a static value or can be derived from target\u0026#39;s labels. # To get value from target\u0026#39;s labels, use target.labels.\u0026lt;target\u0026#39;s label key\u0026gt; # as value.value: \u0026lt;string\u003e cloudprober.probes.DebugOptions # # Whether to log metrics or not.log_metrics: \u0026lt;bool\u003e cloudprober.probes.ProbeDef # # Probe name. It should be unique across all probes.name: \u0026lt;string\u003e type: (PING|HTTP|DNS|EXTERNAL|UDP|UDP_LISTENER|GRPC|TCP|EXTENSION|USER_DEFINED): \u0026lt;enum\u003e # Interval between two probe runs in milliseconds. # Only one of \u0026#34;interval\u0026#34; and \u0026#34;inteval_msec\u0026#34; should be defined. # Default interval is 2s.interval_msec: \u0026lt;int32\u003e # Interval between two probe runs in string format, e.g. 10s. # Only one of \u0026#34;interval\u0026#34; and \u0026#34;inteval_msec\u0026#34; should be defined. # Default interval is 2s.interval: \u0026lt;string\u003e # Timeout for each probe in milliseconds # Only one of \u0026#34;timeout\u0026#34; and \u0026#34;timeout_msec\u0026#34; should be defined. # Default timeout is 1s.timeout_msec: \u0026lt;int32\u003e # Timeout for each probe in string format, e.g. 10s. # Only one of \u0026#34;timeout\u0026#34; and \u0026#34;timeout_msec\u0026#34; should be defined. # Default timeout is 1s.timeout: \u0026lt;string\u003e # Targets for the probe. Targets are required for all probes except # for external, user_defined, and extension probe types.targets: \u0026lt;cloudprober.targets.TargetsDef\u003e # Latency distribution. If specified, latency is stored as a distribution.latency_distribution: \u0026lt;cloudprober.metrics.Dist\u003e # Latency unit. Any string that\u0026#39;s parseable by time.ParseDuration. # Valid values: \u0026#34;ns\u0026#34;, \u0026#34;us\u0026#34; (or \u0026#34;Âµs\u0026#34;), \u0026#34;ms\u0026#34;, \u0026#34;s\u0026#34;, \u0026#34;m\u0026#34;, \u0026#34;h\u0026#34;.latency_unit: \u0026lt;string\u003e | default: us # Latency metric name. You may want to change the latency metric name, if: # you\u0026#39;re using latency_distribution for some probes, and regular metric for # other probes, and you want to differentiate between the two. # For example: # probe { # name: \u0026#34;web1_latency\u0026#34; # latency_distribution: {...} # latency_metric_name: \u0026#34;latency_dist\u0026#34; # ... # } # probe { # name: \u0026#34;app1\u0026#34; # ... # }latency_metric_name: \u0026lt;string\u003e | default: latency # Validators for this probe. Validators are run on the data returned by the # probe. See https://cloudprober.org/docs/how-to/validators/ for more info.validator: \u0026lt;cloudprober.validators.Validator\u003e # Set the source IP to send packets from, either by providing an IP address # directly, or a network interface.[source_ip \u0026lt;string\u0026gt; | source_interface \u0026lt;string\u0026gt;]: \u0026lt;oneof\u003e ip_version: (IP_VERSION_UNSPECIFIED|IPV4|IPV6): \u0026lt;enum\u003e # How often to export stats. Probes usually run at a higher frequency (e.g. # every second); stats from individual probes are aggregated within # cloudprober until exported. In most cases, users don\u0026#39;t need to change the # default. # # By default this field is set in the following way: # For all probes except UDP: # stats_export_interval=max(interval, 10s) # For UDP: # stats_export_interval=max(2*max(interval, timeout), 10s)stats_export_interval_msec: \u0026lt;int32\u003e # Additional labels to add to the probe results. Label\u0026#39;s value can either be # static or can be derived from target\u0026#39;s labels. # # Example: # additional_label { # key: \u0026#34;app\u0026#34; # value: \u0026#34;@target.label.app@\u0026#34; # } # (More detailed example at: examples/additional_label/cloudprober.cfg)additional_label: \u0026lt;cloudprober.probes.AdditionalLabel\u003e # (Experimental) If set, test is inversed, i.e. we count it as success if # target doesn\u0026#39;t respond. This is useful, for example, that your firewall is # working as expected. # # This is currently implemented only by PING and TCP probes. # Note: This field is currently experimental, and may change in future.negative_test: \u0026lt;bool\u003e # Alerts configuration. If specified, cloudprober will generate alerts on # probe failures. You can specify multiple alerts. # Example: # alert { # name: \u0026#34;alert1\u0026#34; # condition {...} # notify { # pagerduty { ...} # } # } # alert { # name: \u0026#34;alert2\u0026#34; # notify { ... } # }alert: \u0026lt;cloudprober.alerting.AlertConf\u003e [ping_probe \u0026lt;cloudprober.probes.ping.ProbeConf\u0026gt; | http_probe \u0026lt;cloudprober.probes.http.ProbeConf\u0026gt; | \u0026nbsp;dns_probe \u0026lt;cloudprober.probes.dns.ProbeConf\u0026gt; | external_probe \u0026lt;cloudprober.probes.external.ProbeConf\u0026gt; | \u0026nbsp;udp_probe \u0026lt;cloudprober.probes.udp.ProbeConf\u0026gt; | udp_listener_probe \u0026lt;cloudprober.probes.udplistener.ProbeConf\u0026gt; | \u0026nbsp;grpc_probe \u0026lt;cloudprober.probes.grpc.ProbeConf\u0026gt; | tcp_probe \u0026lt;cloudprober.probes.tcp.ProbeConf\u0026gt; | \u0026nbsp;user_defined_probe \u0026lt;string\u0026gt;]: \u0026lt;oneof\u003e # Which machines this probe should run on. If defined, cloudprober will run # this probe only if machine\u0026#39;s hostname matches this value. This is useful # for large deployments, where you may want to use the same prober config # everywhere but run this probe only on a subset of machines.run_on: \u0026lt;string\u003e # Schedule for the probe. You can use a schedule to specify when a probe # should or should not run. This is useful for running probes only during # business hours. # # You can specify multiple schedules. Probe will not run if any of the # \u0026#34;DISABLE\u0026#34; schedules are active. If both \u0026#34;ENABLE\u0026#34; and \u0026#34;DISABLE\u0026#34; schedules # overlap, \u0026#34;DISABLE\u0026#34; takes precedence. # # For example, to disable a probe during weekends and on Tuesday between 7pm # and 8pm, e.g. for rollouts: # schdule { # type: DISABLE # start_weekday: FRIDAY # start_time: \u0026#34;20:00\u0026#34; # end_weekday: SUNDAY # end_time: \u0026#34;17:00\u0026#34; # timezone: \u0026#34;America/New_York\u0026#34; # } # schdule { # type: DISABLE # start_weekday: TUESDAY # start_time: \u0026#34;19:00\u0026#34; # end_weekday: TUESDAY # end_time: \u0026#34;20:00\u0026#34; # timezone: \u0026#34;America/New_York\u0026#34; # }schedule: \u0026lt;cloudprober.probes.Schedule\u003e # Debug options. Currently only used to enable logging metrics.debug_options: \u0026lt;cloudprober.probes.DebugOptions\u003e cloudprober.probes.Schedule # type: (ScheduleType_UNSPECIFIED|ENABLE|DISABLE): \u0026lt;enum\u003e # Period start weekday. If not specified, it defaults to EVERYDAY.start_weekday: (EVERYDAY|SUNDAY|MONDAY|TUESDAY|WEDNESDAY|THURSDAY|FRIDAY|SATURDAY): \u0026lt;enum\u003e # Start time in 24 hour HH:MM format.start_time: \u0026lt;string\u003e | default: 00:00 # Period end weekday. If not specified, it defaults to EVERYDAY.end_weekday: (EVERYDAY|SUNDAY|MONDAY|TUESDAY|WEDNESDAY|THURSDAY|FRIDAY|SATURDAY): \u0026lt;enum\u003e # End time in 24 hour HH:MM format.end_time: \u0026lt;string\u003e | default: 23:59 # Timezone in which the probe should run. If not specified, it defaults to # UTC. Example: \u0026#34;America/New_York\u0026#34;timezone: \u0026lt;string\u003e | default: UTC cloudprober.probes.dns.ProbeConf # # Domain to use when making DNS queriesresolved_domain: \u0026lt;string\u003e | default: www.google.com. # DNS Query Typequery_type: (NONE|A|NS|CNAME|SOA|PTR|MX|TXT|RP|AFSDB|SIG|KEY|AAAA|LOC|SRV|NAPTR|KX|CERT|DNAME|APL|DS|SSHFP|IPSECKEY|RRSIG|NSEC|DNSKEY|DHCID|NSEC3|NSEC3PARAM|TLSA|HIP|CDS|CDNSKEY|OPENPGPKEY|TKEY|TSIG|URI|CAA|TA|DLV): \u0026lt;enum\u003e # Minimum number of answers expected. Default behavior is to return success # if DNS response status is NOERROR.min_answers: \u0026lt;uint32\u003e | default: 0 # Whether to resolve the target (target is DNS server here) before making # the request. If set to false, we hand over the target directly to the DNS # client. Otherwise, we resolve the target first to an IP address. By # default we resolve first if it\u0026#39;s a discovered resource, e.g., a k8s # endpoint.resolve_first: \u0026lt;bool\u003e # Requests per probe. # Number of DNS requests per probe. Requests are executed concurrently and # each DNS request contributes to probe results. For example, if you run two # requests per probe, \u0026#34;total\u0026#34; counter will be incremented by 2.requests_per_probe: \u0026lt;int32\u003e | default: 1 # How long to wait between two requests to the same target. Only relevant # if requests_per_probe is also configured. # # This value should be less than (interval - timeout) / requests_per_probe. # This is to ensure that all requests are executed within one probe interval # and all of them get sufficient time. For example, if probe interval is 2s, # timeout is 1s, and requests_per_probe is 10, requests_interval_msec # should be less than 10ms.requests_interval_msec: \u0026lt;int32\u003e | default: 0 cloudprober.probes.external.ProbeConf # mode: (ONCE|SERVER): \u0026lt;enum\u003e # Command. For ONCE probes, arguments are processed for the following field # substitutions: # @probe@ Name of the probe # @target@ Hostname of the target # @address@ IP address of the target # # For example, for target ig-us-central1-a, /tools/recreate_vm -vm @target@ # will get converted to: /tools/recreate_vm -vm ig-us-central1-acommand: \u0026lt;string\u003e # Command environment variables. These are passed on to the external probe # process as environment variables.env_var: \u0026lt;cloudprober.probes.external.ProbeConf.EnvVarEntry\u003e options: \u0026lt;cloudprober.probes.external.ProbeConf.Option\u003e # Export output as metrics, where output is the output returned by the # external probe process, over stdout for ONCE probes, and through ProbeReply # for SERVER probes. Cloudprober expects variables to be in the following # format in the output: # var1 value1 (for example: total_errors 589)output_as_metrics: \u0026lt;bool\u003e | default: true output_metrics_options: \u0026lt;cloudprober.metrics.payload.OutputMetricsOptions\u003e cloudprober.probes.external.ProbeConf.EnvVarEntry # key: \u0026lt;string\u003e value: \u0026lt;string\u003e cloudprober.probes.external.ProbeConf.Option # name: \u0026lt;string\u003e value: \u0026lt;string\u003e cloudprober.probes.grpc.GenericRequest # # Protoset contains descriptor source protos generated from the *.proto # files. You can use protoc to generate protoset files: # protoc --proto_path=. --descriptor_set_out=myservice.protoset \\ # --include_imports my/custom/server/service.protoprotoset_file: \u0026lt;string\u003e # Note first 3 methods are valid only if descriptor source is not set.[list_services \u0026lt;bool\u0026gt; | list_service_methods \u0026lt;string\u0026gt; | \u0026nbsp;describe_service_method \u0026lt;string\u0026gt; | call_service_method \u0026lt;string\u0026gt;]: \u0026lt;oneof\u003e # Request data (in JSON format) for the call_service_method request.body: \u0026lt;string\u003e cloudprober.probes.grpc.ProbeConf # # Optional oauth config. For GOOGLE_DEFAULT_CREDENTIALS, use: # oauth_config: { bearer_token { gce_service_account: \u0026#34;default\u0026#34; } }oauth_config: \u0026lt;cloudprober.oauth.Config\u003e # If alts_config is provided, gRPC client uses ALTS for authentication and # encryption. For default alts configs, use: # alts_config: {}alts_config: \u0026lt;cloudprober.probes.grpc.ProbeConf.ALTSConfig\u003e # If TLSConfig is specified, it\u0026#39;s used for authentication. # Note that only one of ALTSConfig and TLSConfig can be enabled at a time.tls_config: \u0026lt;cloudprober.tlsconfig.TLSConfig\u003e # if insecure_transport is set to true, TLS will not be used.insecure_transport: \u0026lt;bool\u003e method: (ECHO|READ|WRITE|HEALTH_CHECK|GENERIC): \u0026lt;enum\u003e # Blob size for ECHO, READ, and WRITE methods.blob_size: \u0026lt;int32\u003e | default: 1024 # For HEALTH_CHECK, name of the service to health check.health_check_service: \u0026lt;string\u003e # For HEALTH_CHECK, ignore status. By default, HEALTH_CHECK test passes # only if response-status is SERVING. Setting the following option makes # HEALTH_CHECK pass regardless of the response-status.health_check_ignore_status: \u0026lt;bool\u003e # Request definition for the GENERIC method.request: \u0026lt;cloudprober.probes.grpc.GenericRequest\u003e num_conns: \u0026lt;int32\u003e | default: 2 keep_alive: \u0026lt;bool\u003e | default: true # If connect_timeout is not specified, reuse probe timeout.connect_timeout_msec: \u0026lt;int32\u003e # URI scheme allows gRPC to use different resolvers # Example URI scheme: \u0026#34;google-c2p:///\u0026#34; # See https://github.com/grpc/grpc/blob/master/doc/naming.md for more detailsuri_scheme: \u0026lt;string\u003e headers: \u0026lt;cloudprober.probes.grpc.ProbeConf.Header\u003e cloudprober.probes.grpc.ProbeConf.ALTSConfig # # If provided, ALTS verifies that peer is using one of the given service # accounts.target_service_account: \u0026lt;string\u003e # Handshaker service address. Default is to use the local metadata server. # For most of the ALTS use cases, default address should be okay.handshaker_service_address: \u0026lt;string\u003e cloudprober.probes.grpc.ProbeConf.Header # name: \u0026lt;string\u003e value: \u0026lt;string\u003e cloudprober.probes.http.ProbeConf # # HTTP request scheme (Corresponding target label: \u0026#34;scheme\u0026#34;). If not set, we # use taget\u0026#39;s \u0026#39;scheme\u0026#39; label if present. # Note: protocol is deprecated, use scheme instead.[protocol: (HTTP|HTTPS) | scheme: (HTTP|HTTPS)]: \u0026lt;oneof\u003e # Relative URL (Corresponding target label: \u0026#34;path\u0026#34;). We construct the final # URL like this: # \u0026lt;scheme\u0026gt;://\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;/\u0026lt;relative_url\u0026gt;. # # Note that the relative_url should start with a \u0026#39;/\u0026#39;.relative_url: \u0026lt;string\u003e # Port for HTTP requests (Corresponding target field: port) # Default is to use the scheme specific port, but if this field is not # set and discovered target has a port (e.g., k8s services, ingresses), # we use target\u0026#39;s port.port: \u0026lt;int32\u003e # Whether to resolve the target before making the request. If set to true, # we resolve the target first to an IP address and make a request using # that while passing target name (or \u0026#39;host\u0026#39; label if present) as Host # header. # # This behavior is automatic for discovered targets if they have an IP # address associated with them. Usually you don\u0026#39;t need to worry about this # field and you can left it unspecified. We\u0026#39;ll ty to do the right thing.resolve_first: \u0026lt;bool\u003e # Export response (body) count as a metricexport_response_as_metrics: \u0026lt;bool\u003e | default: false # HTTP request methodmethod: (GET|POST|PUT|HEAD|DELETE|PATCH|OPTIONS): \u0026lt;enum\u003e # HTTP request headers # It is recommended to use \u0026#34;header\u0026#34; instead of \u0026#34;headers\u0026#34; for new configs. # header { # key: \u0026#34;Authorization\u0026#34; # value: \u0026#34;Bearer {{env \u0026#34;AUTH_TOKEN\u0026#34;}}\u0026#34; # } headers: \u0026lt;cloudprober.probes.http.ProbeConf.Header\u003e header: \u0026lt;cloudprober.probes.http.ProbeConf.HeaderEntry\u003e # Request body. This field works similar to the curl\u0026#39;s data flag. If there # are multiple \u0026#34;body\u0026#34; fields, we combine their values with a \u0026#39;\u0026amp;\u0026#39; in between. # # Also, we try to guess the content-type header based on the data: # 1) If data appears to be a valid json, we automatically set the # content-type header to \u0026#34;application/json\u0026#34;. # 2) If the final data string appears to be a valid query string, we # set content-type to \u0026#34;application/x-www-form-urlencoded\u0026#34;. Content type # header can still be overridden using the header field above. # Example: # body: \u0026#34;grant_type=client_credentials\u0026#34; # body: \u0026#34;scope=transferMoney\u0026#34; # body: \u0026#34;clientId=aweseomeClient\u0026#34; # body: \u0026#34;clientSecret=noSecret\u0026#34;body: \u0026lt;string\u003e # Enable HTTP keep-alive. If set to true, underlying connection is reused # for further probes. Default is to close the connection after every request.keep_alive: \u0026lt;bool\u003e # OAuth Configoauth_config: \u0026lt;cloudprober.oauth.Config\u003e # Disable HTTP2 # Golang HTTP client automatically enables HTTP/2 if server supports it. This # option disables that behavior to enforce HTTP/1.1 for testing purpose.disable_http2: \u0026lt;bool\u003e # Disable TLS certificate validation. If set to true, any certificate # presented by the server for any host name will be accepted # Deprecation: This option is now subsumed by the tls_config below. To # disable cert validation use: # tls_config { # disable_cert_validation: true # }disable_cert_validation: \u0026lt;bool\u003e # TLS configtls_config: \u0026lt;cloudprober.tlsconfig.TLSConfig\u003e # Proxy URL, e.g. http://myproxy:3128proxy_url: \u0026lt;string\u003e # User agent. Default user agent is Go\u0026#39;s default user agent.user_agent: \u0026lt;string\u003e # Maximum idle connections to keep alivemax_idle_conns: \u0026lt;int32\u003e | default: 256 # The maximum amount of redirects the HTTP client will follow. # To disable redirects, use max_redirects: 0.max_redirects: \u0026lt;int32\u003e # Interval between targets.interval_between_targets_msec: \u0026lt;int32\u003e | default: 10 # Requests per probe. # Number of HTTP requests per probe. Requests are executed concurrently and # each HTTP re contributes to probe results. For example, if you run two # requests per probe, \u0026#34;total\u0026#34; counter will be incremented by 2.requests_per_probe: \u0026lt;int32\u003e | default: 1 # How long to wait between two requests to the same target. Only relevant # if requests_per_probe is also configured. # # This value should be less than (interval - timeout) / requests_per_probe. # This is to ensure that all requests are executed within one probe interval # and all of them get sufficient time. For example, if probe interval is 2s, # timeout is 1s, and requests_per_probe is 10, requests_interval_msec # should be less than 10ms.requests_interval_msec: \u0026lt;int32\u003e | default: 0 cloudprober.probes.http.ProbeConf.Header # name: \u0026lt;string\u003e value: \u0026lt;string\u003e cloudprober.probes.http.ProbeConf.HeaderEntry # key: \u0026lt;string\u003e value: \u0026lt;string\u003e cloudprober.probes.ping.ProbeConf # # Packets per probepackets_per_probe: \u0026lt;int32\u003e | default: 2 # How long to wait between two packets to the same targetpackets_interval_msec: \u0026lt;int32\u003e | default: 25 # Resolve targets after these many probesresolve_targets_interval: \u0026lt;int32\u003e | default: 5 # Ping payload size in bytes. It cannot be smaller than 8, number of bytes # required for the nanoseconds timestamp.payload_size: \u0026lt;int32\u003e | default: 56 # Use datagram socket for ICMP. # This option enables unprivileged pings (that is, you don\u0026#39;t require root # privilege to send ICMP packets). Note that most of the Linux distributions # don\u0026#39;t allow unprivileged pings by default. To enable unprivileged pings on # some Linux distributions, you may need to run the following command: # # sudo sysctl -w net.ipv4.ping_group_range=\u0026#34;0 \u0026lt;large valid group id\u0026gt;\u0026#34; # # net.ipv4.ping_group_range system setting takes two integers that specify # the group id range that is allowed to execute the unprivileged pings. Note # that the same setting (with ipv4 in the path) applies to IPv6 as well. # # Note: This option is not supported on Windows and is automatically # disabled there.use_datagram_socket: \u0026lt;bool\u003e | default: true # Disable integrity checks. To detect data courruption in the network, we # craft the outgoing ICMP packet payload in a certain format and verify that # the reply payload matches the same format.disable_integrity_check: \u0026lt;bool\u003e | default: false # Do not allow OS-level fragmentation, only works on Linux systems.disable_fragmentation: \u0026lt;bool\u003e | default: false cloudprober.probes.tcp.ProbeConf # # Port for TCP requests. If not specfied, and port is provided by the # targets (e.g. kubernetes endpoint or service), that port is used.port: \u0026lt;int32\u003e # Whether to resolve the target before making the request. If set to false, # we hand over the target golang\u0026#39;s net.Dial module, Otherwise, we resolve # the target first to an IP address and make a request using that. By # default we resolve first if it\u0026#39;s a discovered resource, e.g., a k8s # endpoint.resolve_first: \u0026lt;bool\u003e # Interval between targets.interval_between_targets_msec: \u0026lt;int32\u003e | default: 10 cloudprober.probes.udp.ProbeConf # # Port to send UDP Ping to (UDP Echo). If running with the UDP server that # comes with cloudprober, it should be same as # ProberConfig.udp_echo_server_port.port: \u0026lt;int32\u003e | default: 31122 # Number of sending side ports to use.num_tx_ports: \u0026lt;int32\u003e | default: 16 # message max to account for MTU.max_length: \u0026lt;int32\u003e | default: 1300 # Payload sizepayload_size: \u0026lt;int32\u003e # Changes the exported monitoring streams to be per port: # 1. Changes the streams names to total-per-port, success-per-port etc. # 2. Adds src_port and dst_port as stream labels. # Note that the field name is experimental and may change in the future.export_metrics_by_port: \u0026lt;bool\u003e | default: false # Whether to use all transmit ports per probe, per target. # Default is to probe each target once per probe and round-robin through the # source ports. # Setting this field to true changes the behavior to send traffic from all # ports to all targets in each probe. # For example, if num_tx_ports is set to 16, in every probe cycle, we\u0026#39;ll send # 16 packets to every target (1 per tx port). # Note that setting this field to true will increase the probe traffic.use_all_tx_ports_per_probe: \u0026lt;bool\u003e | default: false # maxTargets is the maximum number of targets supported by this probe type. # If there are more targets, they are pruned from the list to bring targets # list under maxTargets. A large number of targets has impact on resource # consumption.max_targets: \u0026lt;int32\u003e | default: 500 cloudprober.probes.udplistener.ProbeConf # # Port to listen.port: \u0026lt;int32\u003e | default: 32212 type: (INVALID|ECHO|DISCARD): \u0026lt;enum\u003e # Number of packets sent in a single probe.packets_per_probe: \u0026lt;int32\u003e | default: 1 "}),e.add({id:24,href:"/docs/config/rds/",title:"RDS Config",description:"Configs:\u0026nbsp;\u0026nbsp;main | alerting | metrics | oauth | probes | rds | servers | surfacer | targets | tlsconfig | utils | validators YAML|TextPB cloudprober.rds.ClientConf.ServerOptions # server_address: \u0026lt;string\u003e # Optional oauth config for authentication.oauth_config: \u0026lt;cloudprober.oauth.Config\u003e # TLS config, it can be used to: # - Specify a CA cert for server cert verification: # tls_config { # ca_cert_file: \u0026#34;....\u0026#34; # } # # - Specify client\u0026#39;s TLS cert and key: # tls_config { # tls_cert_file: \u0026#34;.",content:" Configs:\u0026nbsp;\u0026nbsp;main | alerting | metrics | oauth | probes | rds | servers | surfacer | targets | tlsconfig | utils | validators YAML|TextPB cloudprober.rds.ClientConf.ServerOptions # server_address: \u0026lt;string\u003e # Optional oauth config for authentication.oauth_config: \u0026lt;cloudprober.oauth.Config\u003e # TLS config, it can be used to: # - Specify a CA cert for server cert verification: # tls_config { # ca_cert_file: \u0026#34;....\u0026#34; # } # # - Specify client\u0026#39;s TLS cert and key: # tls_config { # tls_cert_file: \u0026#34;...\u0026#34; # tls_key_file: \u0026#34;...\u0026#34; # }tls_config: \u0026lt;cloudprober.tlsconfig.TLSConfig\u003e cloudprober.rds.Filter # key: \u0026lt;string\u003e value: \u0026lt;string\u003e cloudprober.rds.IPConfig # # NIC indexnic_index: \u0026lt;int32\u003e | default: 0 ip_type: (DEFAULT|PUBLIC|ALIAS): \u0026lt;enum\u003e ip_version: (IP_VERSION_UNSPECIFIED|IPV4|IPV6): \u0026lt;enum\u003e cloudprober.rds.Provider # # Provider identifier, e.g. \u0026#34;gcp\u0026#34;. Server routes incoming requests to various # providers based on this id.id: \u0026lt;string\u003e [file_config \u0026lt;cloudprober.rds.file.ProviderConfig\u0026gt; | gcp_config \u0026lt;cloudprober.rds.gcp.ProviderConfig\u0026gt; | \u0026nbsp;kubernetes_config \u0026lt;cloudprober.rds.kubernetes.ProviderConfig\u0026gt;]: \u0026lt;oneof\u003e cloudprober.rds.ServerConf # # List of providers that server supports.provider: \u0026lt;cloudprober.rds.Provider\u003e cloudprober.rds.file.ProviderConfig # # File that contains resources in either textproto or json format. # Example in textproto format: # # resource { # name: \u0026#34;switch-xx-01\u0026#34; # ip: \u0026#34;10.11.112.3\u0026#34; # port: 8080 # labels { # key: \u0026#34;device_type\u0026#34; # value: \u0026#34;switch\u0026#34; # } # } # resource { # name: \u0026#34;switch-yy-01\u0026#34; # ip: \u0026#34;10.16.110.12\u0026#34; # port: 8080 # }file_path: \u0026lt;string\u003e format: (UNSPECIFIED|TEXTPB|JSON): \u0026lt;enum\u003e # If specified, file will be re-read at the given interval.re_eval_sec: \u0026lt;int32\u003e # Whenever possible, we reload a file only if it has been modified since the # last load. If following option is set, mod time check is disabled. # Note that mod-time check doesn\u0026#39;t work for GCS.disable_modified_time_check: \u0026lt;bool\u003e cloudprober.rds.gcp.ForwardingRules # # Optionl region filter regex to limit discovery to specific regions, e.g. # \u0026#34;region_filter:europe-*\u0026#34;region_filter: \u0026lt;string\u003e # How often resources should be refreshed.re_eval_sec: \u0026lt;int32\u003e | default: 300 cloudprober.rds.gcp.GCEInstances # # Optional zone filter regex to limit discovery to the specific zones # For example, zone_filter: \u0026#34;us-east1-*\u0026#34; will limit instances discovery to # only to the zones in the \u0026#34;us-east1\u0026#34; region.zone_filter: \u0026lt;string\u003e # How often resources should be refreshed.re_eval_sec: \u0026lt;int32\u003e | default: 300 cloudprober.rds.gcp.ProviderConfig # # GCP projects. If running on GCE, it defaults to the local project.project: \u0026lt;string\u003e # GCE instances discovery options. This field should be declared for the GCE # instances discovery to be enabled.gce_instances: \u0026lt;cloudprober.rds.gcp.GCEInstances\u003e # Forwarding rules discovery options. This field should be declared for the # forwarding rules discovery to be enabled. # Note that RDS supports only regional forwarding rules.forwarding_rules: \u0026lt;cloudprober.rds.gcp.ForwardingRules\u003e # RTC variables discovery options.rtc_variables: \u0026lt;cloudprober.rds.gcp.RTCVariables\u003e # PubSub messages discovery options.pubsub_messages: \u0026lt;cloudprober.rds.gcp.PubSubMessages\u003e # Compute API version.api_version: \u0026lt;string\u003e | default: v1 # Compute API endpoint. Currently supported only for GCE instances and # forwarding rules.api_endpoint: \u0026lt;string\u003e | default: https://www.googleapis.com/compute/ cloudprober.rds.gcp.PubSubMessages # subscription: \u0026lt;cloudprober.rds.gcp.PubSubMessages.Subscription\u003e # Only for testing.api_endpoint: \u0026lt;string\u003e cloudprober.rds.gcp.PubSubMessages.Subscription # # Subscription name. If it doesn\u0026#39;t exist already, we try to create one.name: \u0026lt;string\u003e # Topic name. This is used to create the subscription if it doesn\u0026#39;t exist # already.topic_name: \u0026lt;string\u003e # If subscription already exists, how far back to seek back on restart. # Note that duplicate data is fine as we filter by publish time.seek_back_duration_sec: \u0026lt;int32\u003e | default: 3600 cloudprober.rds.gcp.RTCVariables # rtc_config: \u0026lt;cloudprober.rds.gcp.RTCVariables.RTCConfig\u003e cloudprober.rds.gcp.RTCVariables.RTCConfig # name: \u0026lt;string\u003e # How often RTC variables should be evaluated/expanded.re_eval_sec: \u0026lt;int32\u003e | default: 10 cloudprober.rds.kubernetes.Endpoints # cloudprober.rds.kubernetes.Ingresses # cloudprober.rds.kubernetes.Pods # cloudprober.rds.kubernetes.ProviderConfig # # Namespace to list resources for. If not specified, we default to all # namespaces.namespace: \u0026lt;string\u003e # Pods discovery options. This field should be declared for the pods # discovery to be enabled.pods: \u0026lt;cloudprober.rds.kubernetes.Pods\u003e # Endpoints discovery options. This field should be declared for the # endpoints discovery to be enabled.endpoints: \u0026lt;cloudprober.rds.kubernetes.Endpoints\u003e # Services discovery options. This field should be declared for the # services discovery to be enabled.services: \u0026lt;cloudprober.rds.kubernetes.Services\u003e # Ingresses discovery options. This field should be declared for the # ingresses discovery to be enabled. # Note: Ingress support is experimental and may change in future.ingresses: \u0026lt;cloudprober.rds.kubernetes.Ingresses\u003e # Label selectors to filter resources. This is useful for large clusters. # label_selector: [\u0026#34;app=cloudprober\u0026#34;, \u0026#34;env!=dev\u0026#34;]label_selector: \u0026lt;string\u003e # Kubernetes API server address. If not specified, we assume in-cluster mode # and get it from the local environment variables.api_server_address: \u0026lt;string\u003e # TLS config to authenticate communication with the API server.tls_config: \u0026lt;cloudprober.tlsconfig.TLSConfig\u003e # How often resources should be evaluated/expanded.re_eval_sec: \u0026lt;int32\u003e | default: 60 cloudprober.rds.kubernetes.Services # cloudprober.rds.ClientConf.ServerOptions # server_address: \u0026lt;string\u003e # Optional oauth config for authentication.oauth_config: \u0026lt;cloudprober.oauth.Config\u003e # TLS config, it can be used to: # - Specify a CA cert for server cert verification: # tls_config { # ca_cert_file: \u0026#34;....\u0026#34; # } # # - Specify client\u0026#39;s TLS cert and key: # tls_config { # tls_cert_file: \u0026#34;...\u0026#34; # tls_key_file: \u0026#34;...\u0026#34; # }tls_config: \u0026lt;cloudprober.tlsconfig.TLSConfig\u003e cloudprober.rds.Filter # key: \u0026lt;string\u003e value: \u0026lt;string\u003e cloudprober.rds.IPConfig # # NIC indexnic_index: \u0026lt;int32\u003e | default: 0 ip_type: (DEFAULT|PUBLIC|ALIAS): \u0026lt;enum\u003e ip_version: (IP_VERSION_UNSPECIFIED|IPV4|IPV6): \u0026lt;enum\u003e cloudprober.rds.Provider # # Provider identifier, e.g. \u0026#34;gcp\u0026#34;. Server routes incoming requests to various # providers based on this id.id: \u0026lt;string\u003e [file_config \u0026lt;cloudprober.rds.file.ProviderConfig\u0026gt; | gcp_config \u0026lt;cloudprober.rds.gcp.ProviderConfig\u0026gt; | \u0026nbsp;kubernetes_config \u0026lt;cloudprober.rds.kubernetes.ProviderConfig\u0026gt;]: \u0026lt;oneof\u003e cloudprober.rds.ServerConf # # List of providers that server supports.provider: \u0026lt;cloudprober.rds.Provider\u003e cloudprober.rds.file.ProviderConfig # # File that contains resources in either textproto or json format. # Example in textproto format: # # resource { # name: \u0026#34;switch-xx-01\u0026#34; # ip: \u0026#34;10.11.112.3\u0026#34; # port: 8080 # labels { # key: \u0026#34;device_type\u0026#34; # value: \u0026#34;switch\u0026#34; # } # } # resource { # name: \u0026#34;switch-yy-01\u0026#34; # ip: \u0026#34;10.16.110.12\u0026#34; # port: 8080 # }file_path: \u0026lt;string\u003e format: (UNSPECIFIED|TEXTPB|JSON): \u0026lt;enum\u003e # If specified, file will be re-read at the given interval.re_eval_sec: \u0026lt;int32\u003e # Whenever possible, we reload a file only if it has been modified since the # last load. If following option is set, mod time check is disabled. # Note that mod-time check doesn\u0026#39;t work for GCS.disable_modified_time_check: \u0026lt;bool\u003e cloudprober.rds.gcp.ForwardingRules # # Optionl region filter regex to limit discovery to specific regions, e.g. # \u0026#34;region_filter:europe-*\u0026#34;region_filter: \u0026lt;string\u003e # How often resources should be refreshed.re_eval_sec: \u0026lt;int32\u003e | default: 300 cloudprober.rds.gcp.GCEInstances # # Optional zone filter regex to limit discovery to the specific zones # For example, zone_filter: \u0026#34;us-east1-*\u0026#34; will limit instances discovery to # only to the zones in the \u0026#34;us-east1\u0026#34; region.zone_filter: \u0026lt;string\u003e # How often resources should be refreshed.re_eval_sec: \u0026lt;int32\u003e | default: 300 cloudprober.rds.gcp.ProviderConfig # # GCP projects. If running on GCE, it defaults to the local project.project: \u0026lt;string\u003e # GCE instances discovery options. This field should be declared for the GCE # instances discovery to be enabled.gce_instances: \u0026lt;cloudprober.rds.gcp.GCEInstances\u003e # Forwarding rules discovery options. This field should be declared for the # forwarding rules discovery to be enabled. # Note that RDS supports only regional forwarding rules.forwarding_rules: \u0026lt;cloudprober.rds.gcp.ForwardingRules\u003e # RTC variables discovery options.rtc_variables: \u0026lt;cloudprober.rds.gcp.RTCVariables\u003e # PubSub messages discovery options.pubsub_messages: \u0026lt;cloudprober.rds.gcp.PubSubMessages\u003e # Compute API version.api_version: \u0026lt;string\u003e | default: v1 # Compute API endpoint. Currently supported only for GCE instances and # forwarding rules.api_endpoint: \u0026lt;string\u003e | default: https://www.googleapis.com/compute/ cloudprober.rds.gcp.PubSubMessages # subscription: \u0026lt;cloudprober.rds.gcp.PubSubMessages.Subscription\u003e # Only for testing.api_endpoint: \u0026lt;string\u003e cloudprober.rds.gcp.PubSubMessages.Subscription # # Subscription name. If it doesn\u0026#39;t exist already, we try to create one.name: \u0026lt;string\u003e # Topic name. This is used to create the subscription if it doesn\u0026#39;t exist # already.topic_name: \u0026lt;string\u003e # If subscription already exists, how far back to seek back on restart. # Note that duplicate data is fine as we filter by publish time.seek_back_duration_sec: \u0026lt;int32\u003e | default: 3600 cloudprober.rds.gcp.RTCVariables # rtc_config: \u0026lt;cloudprober.rds.gcp.RTCVariables.RTCConfig\u003e cloudprober.rds.gcp.RTCVariables.RTCConfig # name: \u0026lt;string\u003e # How often RTC variables should be evaluated/expanded.re_eval_sec: \u0026lt;int32\u003e | default: 10 cloudprober.rds.kubernetes.Endpoints # cloudprober.rds.kubernetes.Ingresses # cloudprober.rds.kubernetes.Pods # cloudprober.rds.kubernetes.ProviderConfig # # Namespace to list resources for. If not specified, we default to all # namespaces.namespace: \u0026lt;string\u003e # Pods discovery options. This field should be declared for the pods # discovery to be enabled.pods: \u0026lt;cloudprober.rds.kubernetes.Pods\u003e # Endpoints discovery options. This field should be declared for the # endpoints discovery to be enabled.endpoints: \u0026lt;cloudprober.rds.kubernetes.Endpoints\u003e # Services discovery options. This field should be declared for the # services discovery to be enabled.services: \u0026lt;cloudprober.rds.kubernetes.Services\u003e # Ingresses discovery options. This field should be declared for the # ingresses discovery to be enabled. # Note: Ingress support is experimental and may change in future.ingresses: \u0026lt;cloudprober.rds.kubernetes.Ingresses\u003e # Label selectors to filter resources. This is useful for large clusters. # label_selector: [\u0026#34;app=cloudprober\u0026#34;, \u0026#34;env!=dev\u0026#34;]label_selector: \u0026lt;string\u003e # Kubernetes API server address. If not specified, we assume in-cluster mode # and get it from the local environment variables.api_server_address: \u0026lt;string\u003e # TLS config to authenticate communication with the API server.tls_config: \u0026lt;cloudprober.tlsconfig.TLSConfig\u003e # How often resources should be evaluated/expanded.re_eval_sec: \u0026lt;int32\u003e | default: 60 cloudprober.rds.kubernetes.Services # "}),e.add({id:25,href:"/docs/how-to/rds/",title:"Resource Discovery Service",description:`Note: This is an advanced topic. From a user\u0026rsquo;s perspective, it\u0026rsquo;s only useful if you want to scale targets discovery by centralizing it.Cloudprober internally defines and uses a protocol called resource discovery service (RDS1) for targets discovery2. It helps provide a consistent interface between the targets subsystem, actual resource discovery mechanisms, and probes subsystem. It also provides a way to move targets discovery into an independent process, which can be used to reduce the upstream API traffic.`,content:`Note: This is an advanced topic. From a user\u0026rsquo;s perspective, it\u0026rsquo;s only useful if you want to scale targets discovery by centralizing it.Cloudprober internally defines and uses a protocol called resource discovery service (RDS1) for targets discovery2. It helps provide a consistent interface between the targets subsystem, actual resource discovery mechanisms, and probes subsystem. It also provides a way to move targets discovery into an independent process, which can be used to reduce the upstream API traffic.
Protocol (rds_targets) #To understand the RDS protocol, let\u0026rsquo;s look at the rds_targets targets type. You can think of rds_targets as a configuration interface to the RDS service. When you configure rds_targets, you\u0026rsquo;re creating an RDS client that talks to an RDS backend that is either part of the same process (default) or available over gRPC (usefule for centralizing the upstream API calls).
Here are the RDS targets configuration options:
message RDSTargets { // RDS server options, for example: // rds_server_options { // server_address: \u0026quot;rds-server.xyz:9314\u0026quot; // oauth_config: { // ... // } // } // Default is to use the local server if any. optional rds.ClientConf.ServerOptions rds_server_options = 1; // Resource path specifies the resources to return. Resources paths have the // following format: // \u0026lt;resource_provider\u0026gt;://\u0026lt;resource_type\u0026gt;/\u0026lt;additional_params\u0026gt; // // Examples: // For GCE instances in projectA: \u0026quot;gcp://gce_instances/\u0026lt;projectA\u0026gt;\u0026quot; // Kubernetes Pods : \u0026quot;k8s://pods\u0026quot; optional string resource_path = 2; // Filters to filter resources by. Example: // filter { // key: \u0026quot;namespace\u0026quot; // value: \u0026quot;mynamesspace\u0026quot; // } // filter { // key: \u0026quot;labels.app\u0026quot; // value: \u0026quot;web-service\u0026quot; // } repeated rds.Filter filter = 3; // IP config to specify the IP address to pick for a resource. IPConfig // is defined here: // https://github.com/cloudprober/cloudprober/blob/master/rds/proto/rds.proto optional rds.IPConfig ip_config = 4; } Most options are explained in the comments for a quick reference. Here is the further explanation of some of these options:
rds_server_options #This field specifies how to connect to the RDS server: server address and security options (OAuth and TLS). If left unspecified, it connects to the local server if any (started through rds_server option). Next up it looks for the rds_server_options in global_targets_options.
resource_path #Resource path specifies the resources we are interested in. It consists of a resource provider, resource type and an optional relative path: \u0026lt;resource_provider\u0026gt;://\u0026lt;resource_type\u0026gt;/\u0026lt;optional_relative_path\u0026gt;
resource_provider: Resource provider is a generic concept within the RDS protocol but usually maps to the cloud provider. Cloudprober RDS server currently implements the Kubernetes (k8s) and GCP (gcp) resource providers. We plan to add more resource providers in future. resource_type: Available resource types depend on the providers, for example, for k8s provider supports the following resource types: pods, endpoints, and services. optional_relative_path: For most resource types you can specify resource name in the resource path itself, e.g. k8s://services/cloudprober. Alternatively, you can use filters to filter by name, resource, etc. filter #Filters are key-value strings that can be used to filter resources by various fields. Filters depend on the resource types, but most resources support filtering by name and labels.
# Return resources that start with \u0026quot;web\u0026quot; and have label \u0026quot;service:service-a\u0026quot; ... filter { key: \u0026quot;name\u0026quot; value: \u0026quot;^web.*\u0026quot; } filter { key: \u0026quot;labels.service\u0026quot; value: \u0026quot;service-a\u0026quot; } Filters supported by kubernetes resources: k8s filters. Filters supported by GCP: GCE Instances Forwarding Rules Pub/Sub Messages Running RDS Server #RDS server can either be run as an independent process, or it can be a part of the main prober process. Former mode is useful for large deployments where you may want to reduce the API upcall traffic (for example, to GCP). For example, if you run 1000+ prober processes, it will be much more economical from the API quota usage point of view to have a centralized RDS service with much fewer (2-3) instances instead of having each prober process make its own API calls.
RDS server can be added to a cloudprober process using the rds_server stanza. If you\u0026rsquo;re running RDS server in a remote process, you\u0026rsquo;ll have to enable gRPC server in that process (using grpc_port) so that other instances can access it remotely.
Here is an example RDS server configuration:
rds_server { # GCP provider to discover GCP resources. provider { gcp_config { # Projects to discover resources in. project: \u0026quot;test-project-1\u0026quot; project: \u0026quot;test-project-2\u0026quot; # Discover GCE instances in us-central1. gce_instances { zone_filter: \u0026quot;name = us-central1-*\u0026quot; re_eval_sec: 60 # How often to refresh, default is 300s. } # GCE forwarding rules. forwarding_rules {} } } # Kubernetes targets are further discussed at: # https://cloudprober.org/how-to/run-on-kubernetes/#kubernetes-targets provider { kubernetes_config { endpoints {} } } } For the remote RDS server setup, if accessing over external network, you can secure the underlying gRPC communication using TLS certificates.
Remote RDS Server Example #Cloudprober config:
probe { rds_targets { rds_server_options { server_address: \u0026quot;rds-service:9314\u0026quot; # mTLS configuration tls_config { ca_cert_file: \u0026quot;/vol/certs/server_ca.crt\u0026quot; # To verify the server tls_cert_file: \u0026quot;/vol/certs/client.crt\u0026quot; # Own cert to present to server tls_key_file: \u0026quot;/vol/certs/client.key\u0026quot; # Own cert's private key } } resource_path: \u0026quot;gcp://gce_instances\u0026quot; filter { key: \u0026quot;name\u0026quot; value: \u0026quot;ins-cf-.*\u0026quot; } } } On a different cloudprober instance:
... rds_server { provider { gcp_config { instances { zone_filter: \u0026quot;us-east1-a\u0026quot; } } } } grpc_tls_config { ca_cert_file: \u0026quot;/vol/certs/client_ca.crt\u0026quot; # To verify the server tls_cert_file: \u0026quot;/vol/certs/server.crt\u0026quot; # Own cert to present to client tls_key_file: \u0026quot;/vol/certs/server.key\u0026quot; # Own cert's private key } # Required for remote access grpc_port: 9314 Note that this has nothing to do with the AWS RDS product. Naming is unfortunate.\u0026#160;\u0026#x21a9;\u0026#xfe0e;
In fact, all dynamically discovered targets use the RDS protocol behind the scene.\u0026#160;\u0026#x21a9;\u0026#xfe0e;
`}),e.add({id:26,href:"/docs/config/servers/",title:"Servers Config",description:"Configs:\u0026nbsp;\u0026nbsp;main | alerting | metrics | oauth | probes | rds | servers | surfacer | targets | tlsconfig | utils | validators YAML|TextPB cloudprober.servers.ServerDef # type: (HTTP|UDP|GRPC|EXTERNAL): \u0026lt;enum\u003e [http_server \u0026lt;cloudprober.servers.http.ServerConf\u0026gt; | udp_server \u0026lt;cloudprober.servers.udp.ServerConf\u0026gt; | \u0026nbsp;grpc_server \u0026lt;cloudprober.servers.grpc.ServerConf\u0026gt; | external_server \u0026lt;cloudprober.servers.external.ServerConf\u0026gt;]: \u0026lt;oneof\u003e cloudprober.servers.external.ServerConf # command: \u0026lt;string\u003e cloudprober.servers.grpc.ServerConf # port: \u0026lt;int32\u003e | default: 3142 # Enables gRPC reflection for publicly visible services, allowing grpc_cli to # work. See https://grpc.io/grpc/core/md_doc_server_reflection_tutorial.html.enable_reflection: \u0026lt;bool\u003e | default: false # If use_dedicated_server is set to true, then create a new gRPC server # to handle probes.",content:" Configs:\u0026nbsp;\u0026nbsp;main | alerting | metrics | oauth | probes | rds | servers | surfacer | targets | tlsconfig | utils | validators YAML|TextPB cloudprober.servers.ServerDef # type: (HTTP|UDP|GRPC|EXTERNAL): \u0026lt;enum\u003e [http_server \u0026lt;cloudprober.servers.http.ServerConf\u0026gt; | udp_server \u0026lt;cloudprober.servers.udp.ServerConf\u0026gt; | \u0026nbsp;grpc_server \u0026lt;cloudprober.servers.grpc.ServerConf\u0026gt; | external_server \u0026lt;cloudprober.servers.external.ServerConf\u0026gt;]: \u0026lt;oneof\u003e cloudprober.servers.external.ServerConf # command: \u0026lt;string\u003e cloudprober.servers.grpc.ServerConf # port: \u0026lt;int32\u003e | default: 3142 # Enables gRPC reflection for publicly visible services, allowing grpc_cli to # work. See https://grpc.io/grpc/core/md_doc_server_reflection_tutorial.html.enable_reflection: \u0026lt;bool\u003e | default: false # If use_dedicated_server is set to true, then create a new gRPC server # to handle probes. Otherwise, attempt to reuse gRPC server from runconfig # if that was set.use_dedicated_server: \u0026lt;bool\u003e | default: true cloudprober.servers.http.ServerConf # port: \u0026lt;int32\u003e | default: 3141 protocol: (HTTP|HTTPS): \u0026lt;enum\u003e # Maximum duration for reading the entire request, including the body.read_timeout_ms: \u0026lt;int32\u003e | default: 10000 # Maximum duration before timing out writes of the response.write_timeout_ms: \u0026lt;int32\u003e | default: 10000 # Maximum amount of time to wait for the next request when keep-alives are # enabled.idle_timeout_ms: \u0026lt;int32\u003e | default: 60000 # Certificate file to use for HTTPS servers.tls_cert_file: \u0026lt;string\u003e # Private key file corresponding to the certificate above.tls_key_file: \u0026lt;string\u003e # Disable HTTP/2 for HTTPS servers.disable_http2: \u0026lt;bool\u003e # Pattern data handler returns pattern data at the url /data_\u0026lt;size_in_bytes\u0026gt;, # e.g. \u0026#34;/data_2048\u0026#34;.pattern_data_handler: \u0026lt;cloudprober.servers.http.ServerConf.PatternDataHandler\u003e cloudprober.servers.http.ServerConf.PatternDataHandler # # Response sizes to server, e.g. 1024.response_size: \u0026lt;int32\u003e # Pattern is repeated to build the response, with \u0026#34;response_size mod # pattern_size\u0026#34; filled by \u0026#39;0\u0026#39; bytes.pattern: \u0026lt;string\u003e | default: cloudprober cloudprober.servers.udp.ServerConf # port: \u0026lt;int32\u003e type: (ECHO|DISCARD): \u0026lt;enum\u003e cloudprober.servers.ServerDef # type: (HTTP|UDP|GRPC|EXTERNAL): \u0026lt;enum\u003e [http_server \u0026lt;cloudprober.servers.http.ServerConf\u0026gt; | udp_server \u0026lt;cloudprober.servers.udp.ServerConf\u0026gt; | \u0026nbsp;grpc_server \u0026lt;cloudprober.servers.grpc.ServerConf\u0026gt; | external_server \u0026lt;cloudprober.servers.external.ServerConf\u0026gt;]: \u0026lt;oneof\u003e cloudprober.servers.external.ServerConf # command: \u0026lt;string\u003e cloudprober.servers.grpc.ServerConf # port: \u0026lt;int32\u003e | default: 3142 # Enables gRPC reflection for publicly visible services, allowing grpc_cli to # work. See https://grpc.io/grpc/core/md_doc_server_reflection_tutorial.html.enable_reflection: \u0026lt;bool\u003e | default: false # If use_dedicated_server is set to true, then create a new gRPC server # to handle probes. Otherwise, attempt to reuse gRPC server from runconfig # if that was set.use_dedicated_server: \u0026lt;bool\u003e | default: true cloudprober.servers.http.ServerConf # port: \u0026lt;int32\u003e | default: 3141 protocol: (HTTP|HTTPS): \u0026lt;enum\u003e # Maximum duration for reading the entire request, including the body.read_timeout_ms: \u0026lt;int32\u003e | default: 10000 # Maximum duration before timing out writes of the response.write_timeout_ms: \u0026lt;int32\u003e | default: 10000 # Maximum amount of time to wait for the next request when keep-alives are # enabled.idle_timeout_ms: \u0026lt;int32\u003e | default: 60000 # Certificate file to use for HTTPS servers.tls_cert_file: \u0026lt;string\u003e # Private key file corresponding to the certificate above.tls_key_file: \u0026lt;string\u003e # Disable HTTP/2 for HTTPS servers.disable_http2: \u0026lt;bool\u003e # Pattern data handler returns pattern data at the url /data_\u0026lt;size_in_bytes\u0026gt;, # e.g. \u0026#34;/data_2048\u0026#34;.pattern_data_handler: \u0026lt;cloudprober.servers.http.ServerConf.PatternDataHandler\u003e cloudprober.servers.http.ServerConf.PatternDataHandler # # Response sizes to server, e.g. 1024.response_size: \u0026lt;int32\u003e # Pattern is repeated to build the response, with \u0026#34;response_size mod # pattern_size\u0026#34; filled by \u0026#39;0\u0026#39; bytes.pattern: \u0026lt;string\u003e | default: cloudprober cloudprober.servers.udp.ServerConf # port: \u0026lt;int32\u003e type: (ECHO|DISCARD): \u0026lt;enum\u003e "}),e.add({id:27,href:"/docs/surfacers/stackdriver/",title:"Stackdriver (Google Cloud)",description:`Cloudprober can natively export metrics to Google Cloud Monitoring (formerly, Stackdriver) using stackdriver surfacer. Adding stackdriver surfacer to cloudprober is as simple as adding the following stanza to the config:
surfacer { type: STACKDRIVER } This config will work if you\u0026rsquo;re running on GCP and your VM (or GKE pod) has access to Cloud Monitoring (Stackdriver). If running on any other platform, you\u0026rsquo;ll have to specify the GCP project where you want to send the metrics, and you\u0026rsquo;ll have to configure your environment for Google Application Default Credentials.`,content:`Cloudprober can natively export metrics to Google Cloud Monitoring (formerly, Stackdriver) using stackdriver surfacer. Adding stackdriver surfacer to cloudprober is as simple as adding the following stanza to the config:
surfacer { type: STACKDRIVER } This config will work if you\u0026rsquo;re running on GCP and your VM (or GKE pod) has access to Cloud Monitoring (Stackdriver). If running on any other platform, you\u0026rsquo;ll have to specify the GCP project where you want to send the metrics, and you\u0026rsquo;ll have to configure your environment for Google Application Default Credentials.
By default, stackdriver surfacer exports metrics with the following prefix: custom.googleapis.com/cloudprober/\u0026lt;probe-type\u0026gt;/\u0026lt;probe\u0026gt;. For example, for HTTP probe named google_com, standard metrics will be exported as:
custom.googleapis.com/cloudprober/http/google_com/total custom.googleapis.com/cloudprober/http/google_com/success custom.googleapis.com/cloudprober/http/google_com/failure custom.googleapis.com/cloudprober/http/google_com/latency All the config options for the stackdriver surfacer: config
For example, you can configure stackdriver surfacer to export only metrics that match a specific regex:
surfacer { stackdriver_surfacer { # Export only \u0026quot;http\u0026quot; probe metrics. allowed_metrics_regex: \u0026quot;.*\\\\/http\\\\/.*\u0026quot; } } Accessing the data #Cloudprober exports metrics to stackdriver as custom metrics. Since all cloudprober metrics are counters (total number of probes, success, latency), you\u0026rsquo;ll see rates of these metrics in stackdriver metrics explorer by default. This data may not be very useful as it is (unless you\u0026rsquo;re using distributions in cludprober, more on that later).
However, stackdriver now provides a powerful monitoring query language,MQL, using which we can get more useful metrics.
MQL to get failure ratio:
fetch global | { metric 'custom.googleapis.com/cloudprober/http/google_com/failure' ; metric 'custom.googleapis.com/cloudprober/http/google_com/total' } | align delta(1m) | join | div MQL to get average latency for a probe:
fetch global | { metric 'custom.googleapis.com/cloudprober/http/google_com/latency' ; metric 'custom.googleapis.com/cloudprober/http/google_com/success' } | align delta(1m) | join | div You can use MQL to create graphs and generate alerts. Note that in the examples here we are fetching from the \u0026ldquo;global\u0026rdquo; source (fetch global); if you\u0026rsquo;re running on GCP, you can improve performance of your queries by specifying the \u0026ldquo;gceinstance\u0026rdquo; resource type: _fetch gce_instance.
`}),e.add({id:28,href:"/docs/config/surfacer/",title:"Surfacers Config",description:"Configs:\u0026nbsp;\u0026nbsp;main | alerting | metrics | oauth | probes | rds | servers | surfacer | targets | tlsconfig | utils | validators YAML|TextPB cloudprober.surfacer.LabelFilter # key: \u0026lt;string\u003e value: \u0026lt;string\u003e cloudprober.surfacer.SurfacerDef # # This name is used for logging. If not defined, it\u0026#39;s derived from the type. # Note that this field is required for the USER_DEFINED surfacer type and # should match with the name that you used while registering the user defined # surfacer.",content:" Configs:\u0026nbsp;\u0026nbsp;main | alerting | metrics | oauth | probes | rds | servers | surfacer | targets | tlsconfig | utils | validators YAML|TextPB cloudprober.surfacer.LabelFilter # key: \u0026lt;string\u003e value: \u0026lt;string\u003e cloudprober.surfacer.SurfacerDef # # This name is used for logging. If not defined, it\u0026#39;s derived from the type. # Note that this field is required for the USER_DEFINED surfacer type and # should match with the name that you used while registering the user defined # surfacer.name: \u0026lt;string\u003e type: (NONE|PROMETHEUS|STACKDRIVER|FILE|POSTGRES|PUBSUB|CLOUDWATCH|DATADOG|PROBESTATUS|BIGQUERY|OTEL|USER_DEFINED): \u0026lt;enum\u003e # How many metrics entries (EventMetrics) to buffer. This is the buffer # between incoming metrics and the metrics that are being processed. Default # value should work in most cases. You may need to increase it on a busy # system, but that\u0026#39;s usually a sign that you metrics processing pipeline is # slow for some reason, e.g. slow writes to a remote file. # Note: Only file and pubsub surfacer supports this option right now.metrics_buffer_size: \u0026lt;int64\u003e | default: 10000 # If specified, only allow metrics that match any of these label filters. # Example: # allow_metrics_with_label { # key: \u0026#34;probe\u0026#34;, # value: \u0026#34;check_homepage\u0026#34;, # }allow_metrics_with_label: \u0026lt;cloudprober.surfacer.LabelFilter\u003e # Ignore metrics that match any of these label filters. Ignore has precedence # over allow filters. # Example: # ignore_metrics_with_label { # key: \u0026#34;probe\u0026#34;, # value: \u0026#34;sysvars\u0026#34;, # }ignore_metrics_with_label: \u0026lt;cloudprober.surfacer.LabelFilter\u003e # Allow and ignore metrics based on their names. You can specify regexes # here. Ignore has precendence over allow. # Examples: # ignore_metrics_with_name: \u0026#34;validation_failure\u0026#34; # allow_metrics_with_name: \u0026#34;(total|success|latency)\u0026#34; # # For efficiency reasons, filtering by metric name has to be implemented by # individual surfacers (while going through metrics within an EventMetrics). # As FILE and PUBSUB surfacers export eventmetrics as is, they don\u0026#39;t support # this option.allow_metrics_with_name: \u0026lt;string\u003e ignore_metrics_with_name: \u0026lt;string\u003e # Whether to add failure metric or not. This option is enabled by default # for all surfacers except FILE and PUBSUB.add_failure_metric: \u0026lt;bool\u003e # If set to true, cloudprober will export all metrics as gauge metrics. Note # that cloudprober inherently generates only cumulative metrics. To create # gauge metrics from cumulative metrics, we keep a copy of the old metrics # and subtract new metrics from the previous metrics. This transformation in # metrics has an increased memory-overhead because extra copies required. # However, it should not be noticeable unless you\u0026#39;re producing large number # of metrics (say \u0026gt; 10000 metrics per second).export_as_gauge: \u0026lt;bool\u003e # Matching surfacer specific configuration (one for each type in the above # enum)[prometheus_surfacer \u0026lt;cloudprober.surfacer.prometheus.SurfacerConf\u0026gt; | stackdriver_surfacer \u0026lt;cloudprober.surfacer.stackdriver.SurfacerConf\u0026gt; | \u0026nbsp;file_surfacer \u0026lt;cloudprober.surfacer.file.SurfacerConf\u0026gt; | postgres_surfacer \u0026lt;cloudprober.surfacer.postgres.SurfacerConf\u0026gt; | \u0026nbsp;pubsub_surfacer \u0026lt;cloudprober.surfacer.pubsub.SurfacerConf\u0026gt; | cloudwatch_surfacer \u0026lt;cloudprober.surfacer.cloudwatch.SurfacerConf\u0026gt; | \u0026nbsp;datadog_surfacer \u0026lt;cloudprober.surfacer.datadog.SurfacerConf\u0026gt; | probestatus_surfacer \u0026lt;cloudprober.surfacer.probestatus.SurfacerConf\u0026gt; | \u0026nbsp;bigquery_surfacer \u0026lt;cloudprober.surfacer.bigquery.SurfacerConf\u0026gt; | otel_surfacer \u0026lt;cloudprober.surfacer.otel.SurfacerConf\u0026gt;]: \u0026lt;oneof\u003e cloudprober.surfacer.bigquery.BQColumn # label: \u0026lt;string\u003e column_name: \u0026lt;string\u003e column_type: \u0026lt;string\u003e cloudprober.surfacer.bigquery.SurfacerConf # project_name: \u0026lt;string\u003e bigquery_dataset: \u0026lt;string\u003e bigquery_table: \u0026lt;string\u003e # It represents the bigquery table columns. # bigquery_columns { # label: \u0026#34;id\u0026#34;, # column_name: \u0026#34;id\u0026#34;, # column_type: \u0026#34;string\u0026#34;, # }bigquery_columns: \u0026lt;cloudprober.surfacer.bigquery.BQColumn\u003e # It represents bigquery client timeout in seconds. So, if bigquery insertion # is not completed within this time period then the request will fail and the # failed rows will be retried later.bigquery_timeout_sec: \u0026lt;int64\u003e | default: 30 metrics_buffer_size: \u0026lt;int64\u003e | default: 100000 # This denotes the time interval after which data will be inserted in # bigquery. Default is 10 seconds. So after every 10 seconds all the em in # current will be inserted in bigquery in a default batch size of 1000batch_timer_sec: \u0026lt;int64\u003e | default: 10 metrics_batch_size: \u0026lt;int64\u003e | default: 1000 # Column name for metrics name, value and timestampmetric_time_col_name: \u0026lt;string\u003e | default: metric_time metric_name_col_name: \u0026lt;string\u003e | default: metric_name metric_value_col_name: \u0026lt;string\u003e | default: metric_value cloudprober.surfacer.cloudwatch.SurfacerConf # # The cloudwatch metric namespacenamespace: \u0026lt;string\u003e | default: cloudprober # The cloudwatch resolution value, lowering this below 60 will incur # additional charges as the metrics will be charged at a high resolution # rate.resolution: \u0026lt;int32\u003e | default: 60 # The AWS Region, used to create a CloudWatch session. # The order of fallback for evaluating the AWS Region: # 1. This config value. # 2. EC2 metadata endpoint, via cloudprober sysvars. # 3. AWS_REGION environment value. # 4. AWS_DEFAULT_REGION environment value, if AWS_SDK_LOAD_CONFIG is set. # https://aws.github.io/aws-sdk-go-v2/docs/configuring-sdk/region: \u0026lt;string\u003e # The maximum number of metrics that will be published at one # time. Metrics will be stored locally in a cache until this # limit is reached. 1000 is the maximum number of metrics # supported by the Cloudwatch PutMetricData API. # Metrics will be published when the timer expires, or the buffer is # full, whichever happens first.metrics_batch_size: \u0026lt;int32\u003e | default: 1000 # The maximum amount of time to hold metrics in the buffer (above). # Metrics will be published when the timer expires, or the buffer is # full, whichever happens first. batch_timer_sec: \u0026lt;int32\u003e | default: 30 cloudprober.surfacer.datadog.SurfacerConf # # Prefix to add to all metrics.prefix: \u0026lt;string\u003e | default: cloudprober # Datadog API key. If not set, DD_API_KEY env variable is used.api_key: \u0026lt;string\u003e # Datadog APP key. If not set, DD_APP_KEY env variable is used.app_key: \u0026lt;string\u003e # Datadog server, default: \u0026#34;api.datadoghq.com\u0026#34;server: \u0026lt;string\u003e # The maximum number of metrics that will be published at one # time. Metrics will be stored locally in a cache until this # limit is reached. Datadog\u0026#39;s SubmitMetric API has a maximum payload # size of 500 kilobytes (512000 bytes). Compressed payloads must have a # decompressed size of less than 5 megabytes (5242880 bytes). # Metrics will be published when the timer expires, or the buffer is # full, whichever happens first.metrics_batch_size: \u0026lt;int32\u003e | default: 1000 # The maximum amount of time to hold metrics in the buffer (above). # Metrics will be published when the timer expires, or the buffer is # full, whichever happens first. batch_timer_sec: \u0026lt;int32\u003e | default: 30 # Disable gzip compression of metric payload, when sending metrics to Datadog. # Compression is enabled by default.disable_compression: \u0026lt;bool\u003e cloudprober.surfacer.file.SurfacerConf # # Where to write the results. If left unset, file surfacer writes to the # standard output.file_path: \u0026lt;string\u003e prefix: \u0026lt;string\u003e | default: cloudprober # Compress data before writing to the file.compression_enabled: \u0026lt;bool\u003e | default: false cloudprober.surfacer.otel.GRPCExporter # # If no URL is provided, OpenTelemetry SDK will use the environment variable # OTEL_EXPORTER_OTLP_METRICS_ENDPOINT or OTEL_EXPORTER_OTLP_ENDPOINT in that # preference order.endpoint: \u0026lt;string\u003e tls_config: \u0026lt;cloudprober.tlsconfig.TLSConfig\u003e # HTTP request headers. These can also be set using environment variables.http_header: \u0026lt;cloudprober.surfacer.otel.GRPCExporter.HttpHeaderEntry\u003e # Compression algorithm to use for gRPC requests.compression: (NONE|GZIP): \u0026lt;enum\u003e # Whether to use insecure gRPC connection.insecure: \u0026lt;bool\u003e cloudprober.surfacer.otel.GRPCExporter.HttpHeaderEntry # key: \u0026lt;string\u003e value: \u0026lt;string\u003e cloudprober.surfacer.otel.HTTPExporter # # If no URL is provided, OpenTelemetry SDK will use the environment variable # OTEL_EXPORTER_OTLP_METRICS_ENDPOINT or OTEL_EXPORTER_OTLP_ENDPOINT in that # preference order.endpoint_url: \u0026lt;string\u003e tls_config: \u0026lt;cloudprober.tlsconfig.TLSConfig\u003e # HTTP request headers. These can also be set using environment variables.http_header: \u0026lt;cloudprober.surfacer.otel.HTTPExporter.HttpHeaderEntry\u003e # Compression algorithm to use for HTTP requests.compression: (NONE|GZIP): \u0026lt;enum\u003e cloudprober.surfacer.otel.HTTPExporter.HttpHeaderEntry # key: \u0026lt;string\u003e value: \u0026lt;string\u003e cloudprober.surfacer.otel.SurfacerConf # [otlp_http_exporter \u0026lt;cloudprober.surfacer.otel.HTTPExporter\u0026gt; | otlp_grpc_exporter \u0026lt;cloudprober.surfacer.otel.GRPCExporter\u0026gt;]: \u0026lt;oneof\u003e # How often metrics will be exported. Note that metrics are accumulated # internally and exported at this interval. Increasing this value will # increase the memory usage.export_interval_sec: \u0026lt;int32\u003e | default: 10 # Prefix to use for metrics. Defaults to \u0026#34;cloudprober_\u0026#34;.metrics_prefix: \u0026lt;string\u003e | default: cloudprober_ resource_attribute: \u0026lt;cloudprober.surfacer.otel.SurfacerConf.Attribute\u003e cloudprober.surfacer.otel.SurfacerConf.Attribute # key: \u0026lt;string\u003e value: \u0026lt;string\u003e cloudprober.surfacer.postgres.LabelToColumn # # Label namelabel: \u0026lt;string\u003e # Column to map this label to:column: \u0026lt;string\u003e cloudprober.surfacer.postgres.SurfacerConf # # Postgres connection string. # Example: # \u0026#34;postgresql://root:${PASSWORD}@localhost/cloudprober?sslmode=disable\u0026#34;connection_string: \u0026lt;string\u003e # Metrics table name. # To create table (when storing all labels in single column in JSON format): # CREATE TABLE metrics ( # time timestamp, metric_name varchar(80), value float8, labels jsonb # )metrics_table_name: \u0026lt;string\u003e # Adding label_to_column fields changes how labels are stored in a Postgres # table. If this field is not specified at all, all the labels are stored as # jsonb values as the \u0026#39;labels\u0026#39; column (this mode impacts performance # negatively). If label_to_colum entries are specified for some labels, # those labels are stored in their dedicated columns; all the labels that # don\u0026#39;t have a mapping will be dropped.label_to_column: \u0026lt;cloudprober.surfacer.postgres.LabelToColumn\u003e metrics_buffer_size: \u0026lt;int64\u003e | default: 10000 cloudprober.surfacer.probestatus.SurfacerConf # # default 60sresolution_sec: \u0026lt;int32\u003e | default: 60 # Number of points in each timeseries. This field dictates how far back # can you go up to (resolution_sec * timeseries_size). Note that higher # this number, more memory you\u0026#39;ll use.timeseries_size: \u0026lt;int32\u003e | default: 4320 # Max targets per probe.max_targets_per_probe: \u0026lt;int32\u003e | default: 20 # ProbeStatus URL # Note that older default URL /probestatus forwards to this URL to avoid # breaking older default setups.url: \u0026lt;string\u003e | default: /status # Page cache timecache_time_sec: \u0026lt;int32\u003e | default: 2 # Probestatus surfacer is enabled by default. To disable it, set this # option.disable: \u0026lt;bool\u003e cloudprober.surfacer.prometheus.SurfacerConf # # How many metrics entries (EventMetrics) to buffer. Incoming metrics # processing is paused while serving data to prometheus. This buffer is to # make writes to prometheus surfacer non-blocking. # NOTE: This field is confusing for users and will be removed from the config # after v0.10.3.metrics_buffer_size: \u0026lt;int64\u003e | default: 10000 # Whether to include timestamps in metrics. If enabled (default) each metric # string includes the metric timestamp as recorded in the EventMetric. # Prometheus associates the scraped values with this timestamp. If disabled, # i.e. timestamps are not exported, prometheus associates scraped values with # scrape timestamp.include_timestamp: \u0026lt;bool\u003e | default: true # URL that prometheus scrapes metrics from.metrics_url: \u0026lt;string\u003e | default: /metrics # Prefix to add to all metric names. For example setting this field to # \u0026#34;cloudprober_\u0026#34; will result in metrics with names: # cloudprober_total, cloudprober_success, cloudprober_latency, ..metrics_prefix: \u0026lt;string\u003e cloudprober.surfacer.pubsub.SurfacerConf # # GCP project name for pubsub. It\u0026#39;s required if not running on GCP, # otherwise it\u0026#39;s retrieved from the metadata.project: \u0026lt;string\u003e # Pubsub topic name. # Default is cloudprober-{hostname}topic_name: \u0026lt;string\u003e # Compress data before writing to pubsub.compression_enabled: \u0026lt;bool\u003e | default: false cloudprober.surfacer.stackdriver.SurfacerConf # # GCP project name for stackdriver. If not specified and running on GCP, # project is used.project: \u0026lt;string\u003e # How often to export metrics to stackdriver.batch_timer_sec: \u0026lt;uint64\u003e | default: 10 # If allowed_metrics_regex is specified, only metrics matching the given # regular expression will be exported to stackdriver. Since probe type and # probe name are part of the metric name, you can use this field to restrict # stackdriver metrics to a particular probe. # Example: # allowed_metrics_regex: \u0026#34;.*(http|ping).*(success|validation_failure).*\u0026#34; # # Deprecated: Please use the common surfacer options to filter metrics: # https://cloudprober.org/docs/surfacers/overview/#filtering-metricsallowed_metrics_regex: \u0026lt;string\u003e # Monitoring URL base. Full metric URL looks like the following: # \u0026lt;monitoring_url\u0026gt;/\u0026lt;ptype\u0026gt;/\u0026lt;probe\u0026gt;/\u0026lt;metric\u0026gt; # Example: # custom.googleapis.com/cloudprober/http/google-homepage/latencymonitoring_url: \u0026lt;string\u003e | default: custom.googleapis.com/cloudprober/ # How many metrics entries to buffer. Incoming metrics # processing is paused while serving data to Stackdriver. This buffer is to # make writes to Stackdriver surfacer non-blocking.metrics_buffer_size: \u0026lt;int64\u003e | default: 10000 # Metric prefix to use for stackdriver metrics. If not specified, default # is PTYPE_PROBE.metrics_prefix: (NONE|PROBE|PTYPE_PROBE): \u0026lt;enum\u003e cloudprober.surfacer.LabelFilter # key: \u0026lt;string\u003e value: \u0026lt;string\u003e cloudprober.surfacer.SurfacerDef # # This name is used for logging. If not defined, it\u0026#39;s derived from the type. # Note that this field is required for the USER_DEFINED surfacer type and # should match with the name that you used while registering the user defined # surfacer.name: \u0026lt;string\u003e type: (NONE|PROMETHEUS|STACKDRIVER|FILE|POSTGRES|PUBSUB|CLOUDWATCH|DATADOG|PROBESTATUS|BIGQUERY|OTEL|USER_DEFINED): \u0026lt;enum\u003e # How many metrics entries (EventMetrics) to buffer. This is the buffer # between incoming metrics and the metrics that are being processed. Default # value should work in most cases. You may need to increase it on a busy # system, but that\u0026#39;s usually a sign that you metrics processing pipeline is # slow for some reason, e.g. slow writes to a remote file. # Note: Only file and pubsub surfacer supports this option right now.metrics_buffer_size: \u0026lt;int64\u003e | default: 10000 # If specified, only allow metrics that match any of these label filters. # Example: # allow_metrics_with_label { # key: \u0026#34;probe\u0026#34;, # value: \u0026#34;check_homepage\u0026#34;, # }allow_metrics_with_label: \u0026lt;cloudprober.surfacer.LabelFilter\u003e # Ignore metrics that match any of these label filters. Ignore has precedence # over allow filters. # Example: # ignore_metrics_with_label { # key: \u0026#34;probe\u0026#34;, # value: \u0026#34;sysvars\u0026#34;, # }ignore_metrics_with_label: \u0026lt;cloudprober.surfacer.LabelFilter\u003e # Allow and ignore metrics based on their names. You can specify regexes # here. Ignore has precendence over allow. # Examples: # ignore_metrics_with_name: \u0026#34;validation_failure\u0026#34; # allow_metrics_with_name: \u0026#34;(total|success|latency)\u0026#34; # # For efficiency reasons, filtering by metric name has to be implemented by # individual surfacers (while going through metrics within an EventMetrics). # As FILE and PUBSUB surfacers export eventmetrics as is, they don\u0026#39;t support # this option.allow_metrics_with_name: \u0026lt;string\u003e ignore_metrics_with_name: \u0026lt;string\u003e # Whether to add failure metric or not. This option is enabled by default # for all surfacers except FILE and PUBSUB.add_failure_metric: \u0026lt;bool\u003e # If set to true, cloudprober will export all metrics as gauge metrics. Note # that cloudprober inherently generates only cumulative metrics. To create # gauge metrics from cumulative metrics, we keep a copy of the old metrics # and subtract new metrics from the previous metrics. This transformation in # metrics has an increased memory-overhead because extra copies required. # However, it should not be noticeable unless you\u0026#39;re producing large number # of metrics (say \u0026gt; 10000 metrics per second).export_as_gauge: \u0026lt;bool\u003e # Matching surfacer specific configuration (one for each type in the above # enum)[prometheus_surfacer \u0026lt;cloudprober.surfacer.prometheus.SurfacerConf\u0026gt; | stackdriver_surfacer \u0026lt;cloudprober.surfacer.stackdriver.SurfacerConf\u0026gt; | \u0026nbsp;file_surfacer \u0026lt;cloudprober.surfacer.file.SurfacerConf\u0026gt; | postgres_surfacer \u0026lt;cloudprober.surfacer.postgres.SurfacerConf\u0026gt; | \u0026nbsp;pubsub_surfacer \u0026lt;cloudprober.surfacer.pubsub.SurfacerConf\u0026gt; | cloudwatch_surfacer \u0026lt;cloudprober.surfacer.cloudwatch.SurfacerConf\u0026gt; | \u0026nbsp;datadog_surfacer \u0026lt;cloudprober.surfacer.datadog.SurfacerConf\u0026gt; | probestatus_surfacer \u0026lt;cloudprober.surfacer.probestatus.SurfacerConf\u0026gt; | \u0026nbsp;bigquery_surfacer \u0026lt;cloudprober.surfacer.bigquery.SurfacerConf\u0026gt; | otel_surfacer \u0026lt;cloudprober.surfacer.otel.SurfacerConf\u0026gt;]: \u0026lt;oneof\u003e cloudprober.surfacer.bigquery.BQColumn # label: \u0026lt;string\u003e column_name: \u0026lt;string\u003e column_type: \u0026lt;string\u003e cloudprober.surfacer.bigquery.SurfacerConf # project_name: \u0026lt;string\u003e bigquery_dataset: \u0026lt;string\u003e bigquery_table: \u0026lt;string\u003e # It represents the bigquery table columns. # bigquery_columns { # label: \u0026#34;id\u0026#34;, # column_name: \u0026#34;id\u0026#34;, # column_type: \u0026#34;string\u0026#34;, # }bigquery_columns: \u0026lt;cloudprober.surfacer.bigquery.BQColumn\u003e # It represents bigquery client timeout in seconds. So, if bigquery insertion # is not completed within this time period then the request will fail and the # failed rows will be retried later.bigquery_timeout_sec: \u0026lt;int64\u003e | default: 30 metrics_buffer_size: \u0026lt;int64\u003e | default: 100000 # This denotes the time interval after which data will be inserted in # bigquery. Default is 10 seconds. So after every 10 seconds all the em in # current will be inserted in bigquery in a default batch size of 1000batch_timer_sec: \u0026lt;int64\u003e | default: 10 metrics_batch_size: \u0026lt;int64\u003e | default: 1000 # Column name for metrics name, value and timestampmetric_time_col_name: \u0026lt;string\u003e | default: metric_time metric_name_col_name: \u0026lt;string\u003e | default: metric_name metric_value_col_name: \u0026lt;string\u003e | default: metric_value cloudprober.surfacer.cloudwatch.SurfacerConf # # The cloudwatch metric namespacenamespace: \u0026lt;string\u003e | default: cloudprober # The cloudwatch resolution value, lowering this below 60 will incur # additional charges as the metrics will be charged at a high resolution # rate.resolution: \u0026lt;int32\u003e | default: 60 # The AWS Region, used to create a CloudWatch session. # The order of fallback for evaluating the AWS Region: # 1. This config value. # 2. EC2 metadata endpoint, via cloudprober sysvars. # 3. AWS_REGION environment value. # 4. AWS_DEFAULT_REGION environment value, if AWS_SDK_LOAD_CONFIG is set. # https://aws.github.io/aws-sdk-go-v2/docs/configuring-sdk/region: \u0026lt;string\u003e # The maximum number of metrics that will be published at one # time. Metrics will be stored locally in a cache until this # limit is reached. 1000 is the maximum number of metrics # supported by the Cloudwatch PutMetricData API. # Metrics will be published when the timer expires, or the buffer is # full, whichever happens first.metrics_batch_size: \u0026lt;int32\u003e | default: 1000 # The maximum amount of time to hold metrics in the buffer (above). # Metrics will be published when the timer expires, or the buffer is # full, whichever happens first. batch_timer_sec: \u0026lt;int32\u003e | default: 30 cloudprober.surfacer.datadog.SurfacerConf # # Prefix to add to all metrics.prefix: \u0026lt;string\u003e | default: cloudprober # Datadog API key. If not set, DD_API_KEY env variable is used.api_key: \u0026lt;string\u003e # Datadog APP key. If not set, DD_APP_KEY env variable is used.app_key: \u0026lt;string\u003e # Datadog server, default: \u0026#34;api.datadoghq.com\u0026#34;server: \u0026lt;string\u003e # The maximum number of metrics that will be published at one # time. Metrics will be stored locally in a cache until this # limit is reached. Datadog\u0026#39;s SubmitMetric API has a maximum payload # size of 500 kilobytes (512000 bytes). Compressed payloads must have a # decompressed size of less than 5 megabytes (5242880 bytes). # Metrics will be published when the timer expires, or the buffer is # full, whichever happens first.metrics_batch_size: \u0026lt;int32\u003e | default: 1000 # The maximum amount of time to hold metrics in the buffer (above). # Metrics will be published when the timer expires, or the buffer is # full, whichever happens first. batch_timer_sec: \u0026lt;int32\u003e | default: 30 # Disable gzip compression of metric payload, when sending metrics to Datadog. # Compression is enabled by default.disable_compression: \u0026lt;bool\u003e cloudprober.surfacer.file.SurfacerConf # # Where to write the results. If left unset, file surfacer writes to the # standard output.file_path: \u0026lt;string\u003e prefix: \u0026lt;string\u003e | default: cloudprober # Compress data before writing to the file.compression_enabled: \u0026lt;bool\u003e | default: false cloudprober.surfacer.otel.GRPCExporter # # If no URL is provided, OpenTelemetry SDK will use the environment variable # OTEL_EXPORTER_OTLP_METRICS_ENDPOINT or OTEL_EXPORTER_OTLP_ENDPOINT in that # preference order.endpoint: \u0026lt;string\u003e tls_config: \u0026lt;cloudprober.tlsconfig.TLSConfig\u003e # HTTP request headers. These can also be set using environment variables.http_header: \u0026lt;cloudprober.surfacer.otel.GRPCExporter.HttpHeaderEntry\u003e # Compression algorithm to use for gRPC requests.compression: (NONE|GZIP): \u0026lt;enum\u003e # Whether to use insecure gRPC connection.insecure: \u0026lt;bool\u003e cloudprober.surfacer.otel.GRPCExporter.HttpHeaderEntry # key: \u0026lt;string\u003e value: \u0026lt;string\u003e cloudprober.surfacer.otel.HTTPExporter # # If no URL is provided, OpenTelemetry SDK will use the environment variable # OTEL_EXPORTER_OTLP_METRICS_ENDPOINT or OTEL_EXPORTER_OTLP_ENDPOINT in that # preference order.endpoint_url: \u0026lt;string\u003e tls_config: \u0026lt;cloudprober.tlsconfig.TLSConfig\u003e # HTTP request headers. These can also be set using environment variables.http_header: \u0026lt;cloudprober.surfacer.otel.HTTPExporter.HttpHeaderEntry\u003e # Compression algorithm to use for HTTP requests.compression: (NONE|GZIP): \u0026lt;enum\u003e cloudprober.surfacer.otel.HTTPExporter.HttpHeaderEntry # key: \u0026lt;string\u003e value: \u0026lt;string\u003e cloudprober.surfacer.otel.SurfacerConf # [otlp_http_exporter \u0026lt;cloudprober.surfacer.otel.HTTPExporter\u0026gt; | otlp_grpc_exporter \u0026lt;cloudprober.surfacer.otel.GRPCExporter\u0026gt;]: \u0026lt;oneof\u003e # How often metrics will be exported. Note that metrics are accumulated # internally and exported at this interval. Increasing this value will # increase the memory usage.export_interval_sec: \u0026lt;int32\u003e | default: 10 # Prefix to use for metrics. Defaults to \u0026#34;cloudprober_\u0026#34;.metrics_prefix: \u0026lt;string\u003e | default: cloudprober_ resource_attribute: \u0026lt;cloudprober.surfacer.otel.SurfacerConf.Attribute\u003e cloudprober.surfacer.otel.SurfacerConf.Attribute # key: \u0026lt;string\u003e value: \u0026lt;string\u003e cloudprober.surfacer.postgres.LabelToColumn # # Label namelabel: \u0026lt;string\u003e # Column to map this label to:column: \u0026lt;string\u003e cloudprober.surfacer.postgres.SurfacerConf # # Postgres connection string. # Example: # \u0026#34;postgresql://root:${PASSWORD}@localhost/cloudprober?sslmode=disable\u0026#34;connection_string: \u0026lt;string\u003e # Metrics table name. # To create table (when storing all labels in single column in JSON format): # CREATE TABLE metrics ( # time timestamp, metric_name varchar(80), value float8, labels jsonb # )metrics_table_name: \u0026lt;string\u003e # Adding label_to_column fields changes how labels are stored in a Postgres # table. If this field is not specified at all, all the labels are stored as # jsonb values as the \u0026#39;labels\u0026#39; column (this mode impacts performance # negatively). If label_to_colum entries are specified for some labels, # those labels are stored in their dedicated columns; all the labels that # don\u0026#39;t have a mapping will be dropped.label_to_column: \u0026lt;cloudprober.surfacer.postgres.LabelToColumn\u003e metrics_buffer_size: \u0026lt;int64\u003e | default: 10000 cloudprober.surfacer.probestatus.SurfacerConf # # default 60sresolution_sec: \u0026lt;int32\u003e | default: 60 # Number of points in each timeseries. This field dictates how far back # can you go up to (resolution_sec * timeseries_size). Note that higher # this number, more memory you\u0026#39;ll use.timeseries_size: \u0026lt;int32\u003e | default: 4320 # Max targets per probe.max_targets_per_probe: \u0026lt;int32\u003e | default: 20 # ProbeStatus URL # Note that older default URL /probestatus forwards to this URL to avoid # breaking older default setups.url: \u0026lt;string\u003e | default: /status # Page cache timecache_time_sec: \u0026lt;int32\u003e | default: 2 # Probestatus surfacer is enabled by default. To disable it, set this # option.disable: \u0026lt;bool\u003e cloudprober.surfacer.prometheus.SurfacerConf # # How many metrics entries (EventMetrics) to buffer. Incoming metrics # processing is paused while serving data to prometheus. This buffer is to # make writes to prometheus surfacer non-blocking. # NOTE: This field is confusing for users and will be removed from the config # after v0.10.3.metrics_buffer_size: \u0026lt;int64\u003e | default: 10000 # Whether to include timestamps in metrics. If enabled (default) each metric # string includes the metric timestamp as recorded in the EventMetric. # Prometheus associates the scraped values with this timestamp. If disabled, # i.e. timestamps are not exported, prometheus associates scraped values with # scrape timestamp.include_timestamp: \u0026lt;bool\u003e | default: true # URL that prometheus scrapes metrics from.metrics_url: \u0026lt;string\u003e | default: /metrics # Prefix to add to all metric names. For example setting this field to # \u0026#34;cloudprober_\u0026#34; will result in metrics with names: # cloudprober_total, cloudprober_success, cloudprober_latency, ..metrics_prefix: \u0026lt;string\u003e cloudprober.surfacer.pubsub.SurfacerConf # # GCP project name for pubsub. It\u0026#39;s required if not running on GCP, # otherwise it\u0026#39;s retrieved from the metadata.project: \u0026lt;string\u003e # Pubsub topic name. # Default is cloudprober-{hostname}topic_name: \u0026lt;string\u003e # Compress data before writing to pubsub.compression_enabled: \u0026lt;bool\u003e | default: false cloudprober.surfacer.stackdriver.SurfacerConf # # GCP project name for stackdriver. If not specified and running on GCP, # project is used.project: \u0026lt;string\u003e # How often to export metrics to stackdriver.batch_timer_sec: \u0026lt;uint64\u003e | default: 10 # If allowed_metrics_regex is specified, only metrics matching the given # regular expression will be exported to stackdriver. Since probe type and # probe name are part of the metric name, you can use this field to restrict # stackdriver metrics to a particular probe. # Example: # allowed_metrics_regex: \u0026#34;.*(http|ping).*(success|validation_failure).*\u0026#34; # # Deprecated: Please use the common surfacer options to filter metrics: # https://cloudprober.org/docs/surfacers/overview/#filtering-metricsallowed_metrics_regex: \u0026lt;string\u003e # Monitoring URL base. Full metric URL looks like the following: # \u0026lt;monitoring_url\u0026gt;/\u0026lt;ptype\u0026gt;/\u0026lt;probe\u0026gt;/\u0026lt;metric\u0026gt; # Example: # custom.googleapis.com/cloudprober/http/google-homepage/latencymonitoring_url: \u0026lt;string\u003e | default: custom.googleapis.com/cloudprober/ # How many metrics entries to buffer. Incoming metrics # processing is paused while serving data to Stackdriver. This buffer is to # make writes to Stackdriver surfacer non-blocking.metrics_buffer_size: \u0026lt;int64\u003e | default: 10000 # Metric prefix to use for stackdriver metrics. If not specified, default # is PTYPE_PROBE.metrics_prefix: (NONE|PROBE|PTYPE_PROBE): \u0026lt;enum\u003e "}),e.add({id:29,href:"/docs/config/targets/",title:"Targets Config",description:"Configs:\u0026nbsp;\u0026nbsp;main | alerting | metrics | oauth | probes | rds | servers | surfacer | targets | tlsconfig | utils | validators YAML|TextPB cloudprober.targets.DummyTargets # cloudprober.targets.Endpoint # # Endpoint name. Metrics for a target are identified by a combination of # endpoint name and port name, if specified.name: \u0026lt;string\u003e # Optional IP address. If not specified, endpoint name is DNS resolved.ip: \u0026lt;string\u003e # Endpoint port. If specified, this port will be used by the port-based # probes (e.",content:" Configs:\u0026nbsp;\u0026nbsp;main | alerting | metrics | oauth | probes | rds | servers | surfacer | targets | tlsconfig | utils | validators YAML|TextPB cloudprober.targets.DummyTargets # cloudprober.targets.Endpoint # # Endpoint name. Metrics for a target are identified by a combination of # endpoint name and port name, if specified.name: \u0026lt;string\u003e # Optional IP address. If not specified, endpoint name is DNS resolved.ip: \u0026lt;string\u003e # Endpoint port. If specified, this port will be used by the port-based # probes (e.g. TCP, HTTP), if probe\u0026#39;s configuration doesn\u0026#39;t specify a port.port: \u0026lt;int32\u003e # HTTP probe URL. If provided, this field is used by the HTTP probe, if # probe configuration itself doesn\u0026#39;t specify URL fields.url: \u0026lt;string\u003e # Endpoint labels. These labels can be exported as metrics labels using the # `additional_label` field in the probe configuration.labels: \u0026lt;cloudprober.targets.Endpoint.LabelsEntry\u003e cloudprober.targets.Endpoint.LabelsEntry # key: \u0026lt;string\u003e value: \u0026lt;string\u003e cloudprober.targets.GlobalTargetsOptions # # RDS server address # Deprecated: This option is now deprecated, please use rds_server_options # instead.rds_server_address: \u0026lt;string\u003e # RDS server options, for example: # rds_server_options { # server_address: \u0026#34;rds-server.xyz:9314\u0026#34; # oauth_config: { # ... # } # }rds_server_options: \u0026lt;cloudprober.rds.ClientConf.ServerOptions\u003e # GCE targets options.global_gce_targets_options: \u0026lt;cloudprober.targets.gce.GlobalOptions\u003e # Lame duck options. If provided, targets module checks for the lame duck # targets and removes them from the targets list.lame_duck_options: \u0026lt;cloudprober.targets.lameduck.Options\u003e cloudprober.targets.K8sTargets # # Targets namespace. If this field is unset, we select resources from all # namespaces.namespace: \u0026lt;string\u003e # labelSelector uses the same format as kubernetes API calls. # Example: # labelSelector: \u0026#34;k8s-app\u0026#34; # label k8s-app exists # labelSelector: \u0026#34;role=frontend\u0026#34; # label role=frontend # labelSelector: \u0026#34;!canary\u0026#34; # canary label doesn\u0026#39;t existlabelSelector: \u0026lt;string\u003e # Which resources to target. If value is not empty (\u0026#34;\u0026#34;), we use it as a # regex for resource names. # Example: # services: \u0026#34;\u0026#34; // All services. # endpoints: \u0026#34;.*-service\u0026#34; // Endpoints ending with \u0026#34;service\u0026#34;.[services \u0026lt;string\u0026gt; | endpoints \u0026lt;string\u0026gt; | \u0026nbsp;ingresses \u0026lt;string\u0026gt; | pods \u0026lt;string\u0026gt;]: \u0026lt;oneof\u003e # portFilter can be used to filter resources by port name. This is useful # for resources like endpoints and services, where each resource may have # multiple ports, and we may hit just a subset of those ports. portFilter # takes a regex -- we apply it on port names if port name is available, # otherwise we apply it port numbers. # Example: \u0026#34;.*-dns\u0026#34;, \u0026#34;metrics\u0026#34;, \u0026#34;.*-service\u0026#34;, etc.portFilter: \u0026lt;string\u003e # How often to re-check k8s API servers. Note this field will be irrelevant # when (and if) we move to the watch API. Default is 30s.re_eval_sec: \u0026lt;int32\u003e rds_server_options: \u0026lt;cloudprober.rds.ClientConf.ServerOptions\u003e cloudprober.targets.RDSTargets # # RDS server options, for example: # rds_server_options { # server_address: \u0026#34;rds-server.xyz:9314\u0026#34; # oauth_config: { # ... # } # }rds_server_options: \u0026lt;cloudprober.rds.ClientConf.ServerOptions\u003e # Resource path specifies the resources to return. Resources paths have the # following format: # \u0026lt;resource_provider\u0026gt;://\u0026lt;resource_type\u0026gt;/\u0026lt;additional_params\u0026gt; # # Examples: # For GCE instances in projectA: \u0026#34;gcp://gce_instances/\u0026lt;projectA\u0026gt;\u0026#34; # Kubernetes Pods : \u0026#34;k8s://pods\u0026#34;resource_path: \u0026lt;string\u003e # Filters to filter resources by.filter: \u0026lt;cloudprober.rds.Filter\u003e # IP config to specify the IP address to pick for a resource.ip_config: \u0026lt;cloudprober.rds.IPConfig\u003e cloudprober.targets.TargetsDef # [host_names \u0026lt;string\u0026gt; | shared_targets \u0026lt;string\u0026gt; | \u0026nbsp;gce_targets \u0026lt;cloudprober.targets.gce.TargetsConf\u0026gt; | rds_targets \u0026lt;cloudprober.targets.RDSTargets\u0026gt; | \u0026nbsp;file_targets \u0026lt;cloudprober.targets.file.TargetsConf\u0026gt; | k8s \u0026lt;cloudprober.targets.K8sTargets\u0026gt; | \u0026nbsp;dummy_targets \u0026lt;cloudprober.targets.DummyTargets\u0026gt;]: \u0026lt;oneof\u003e # Static endpoints. These endpoints are merged with the resources returned # by the targets type above. # Example: # endpoint { # name: \u0026#34;service-gtwy-1\u0026#34; # ip: \u0026#34;10.1.18.121\u0026#34; # port: 8080 # labels { # key: \u0026#34;service\u0026#34; # value: \u0026#34;products-service\u0026#34; # } # } # endpoint { # name: \u0026#34;frontend-url1\u0026#34; # url: \u0026#34;https://frontend.example.com/url1\u0026#34; # }endpoint: \u0026lt;cloudprober.targets.Endpoint\u003e # Regex to apply on the targets.regex: \u0026lt;string\u003e # Exclude lameducks. Lameduck targets can be set through RTC (realtime # configurator) service. This functionality works only if lame_duck_options # are specified.exclude_lameducks: \u0026lt;bool\u003e | default: true cloudprober.targets.file.TargetsConf # # File that contains resources in either textproto or json format. # Example in textproto format: # # resource { # name: \u0026#34;switch-xx-01\u0026#34; # ip: \u0026#34;10.11.112.3\u0026#34; # port: 8080 # labels { # key: \u0026#34;device_type\u0026#34; # value: \u0026#34;switch\u0026#34; # } # } # resource { # name: \u0026#34;switch-yy-01\u0026#34; # ip: \u0026#34;10.16.110.12\u0026#34; # port: 8080 # }file_path: \u0026lt;string\u003e filter: \u0026lt;cloudprober.rds.Filter\u003e format: (UNSPECIFIED|TEXTPB|JSON): \u0026lt;enum\u003e # If specified, file will be re-read at the given interval.re_eval_sec: \u0026lt;int32\u003e cloudprober.targets.gce.ForwardingRules # # Important: if multiple probes use forwarding_rules targets, only the # settings in the definition will take effect. # TODO(manugarg): Fix this behavior. # # For regional forwarding rules, regions to return forwarding rules for. # Default is to return forwarding rules from the region that the VM is # running in. To return forwarding rules from all regions, specify region as # \u0026#34;all\u0026#34;.region: \u0026lt;string\u003e # For global forwarding rules, if it is set to true, it will ignore # the value for the above region property.global_rule: \u0026lt;bool\u003e | default: false cloudprober.targets.gce.GlobalOptions # # How often targets should be evaluated/expandedre_eval_sec: \u0026lt;int32\u003e | default: 900 # Compute API version.api_version: \u0026lt;string\u003e | default: v1 cloudprober.targets.gce.Instances # # Use DNS to resolve target names (instances). If set to false (default), # IP addresses specified in the compute.Instance resource is used. If set # to true all the other resolving options are ignored.use_dns_to_resolve: \u0026lt;bool\u003e | default: false network_interface: \u0026lt;cloudprober.targets.gce.Instances.NetworkInterface\u003e # Labels to filter instances by (\u0026#34;key:value-regex\u0026#34; format).label: \u0026lt;string\u003e cloudprober.targets.gce.Instances.NetworkInterface # index: \u0026lt;int32\u003e | default: 0 ip_type: (PRIVATE|PUBLIC|ALIAS): \u0026lt;enum\u003e cloudprober.targets.gce.TargetsConf # # If running on GCE, this defaults to the local project. # Note: Multiple projects support in targets is experimental and may go away # with future iterations.project: \u0026lt;string\u003e [instances \u0026lt;cloudprober.targets.gce.Instances\u0026gt; | forwarding_rules \u0026lt;cloudprober.targets.gce.ForwardingRules\u0026gt;]: \u0026lt;oneof\u003e cloudprober.targets.lameduck.Options # # How often to check for lame-ducked targetsre_eval_sec: \u0026lt;int32\u003e | default: 10 # Runtime config project. If running on GCE, this defaults to the project # containing the VM.runtimeconfig_project: \u0026lt;string\u003e # Lame duck targets runtime config name. An operator will create a variable # here to mark a target as lame-ducked.runtimeconfig_name: \u0026lt;string\u003e | default: lame-duck-targets # Lame duck targets pubsub topic name. An operator will create a message # here to mark a target as lame-ducked.pubsub_topic: \u0026lt;string\u003e # Lame duck expiration time. We ignore variables (targets) that have been # updated more than these many seconds ago. This is a safety mechanism for # failing to cleanup. Also, the idea is that if a target has actually # disappeared, automatic targets expansion will take care of that some time # during this expiration period.expiration_sec: \u0026lt;int32\u003e | default: 300 # Use an RDS client to get lame-duck-targets. # This option is always true now and will be removed after v0.10.7.use_rds: \u0026lt;bool\u003e # RDS server options, for example: # rds_server_options { # server_address: \u0026#34;rds-server.xyz:9314\u0026#34; # oauth_config: { # ... # }rds_server_options: \u0026lt;cloudprober.rds.ClientConf.ServerOptions\u003e cloudprober.targets.DummyTargets # cloudprober.targets.Endpoint # # Endpoint name. Metrics for a target are identified by a combination of # endpoint name and port name, if specified.name: \u0026lt;string\u003e # Optional IP address. If not specified, endpoint name is DNS resolved.ip: \u0026lt;string\u003e # Endpoint port. If specified, this port will be used by the port-based # probes (e.g. TCP, HTTP), if probe\u0026#39;s configuration doesn\u0026#39;t specify a port.port: \u0026lt;int32\u003e # HTTP probe URL. If provided, this field is used by the HTTP probe, if # probe configuration itself doesn\u0026#39;t specify URL fields.url: \u0026lt;string\u003e # Endpoint labels. These labels can be exported as metrics labels using the # `additional_label` field in the probe configuration.labels: \u0026lt;cloudprober.targets.Endpoint.LabelsEntry\u003e cloudprober.targets.Endpoint.LabelsEntry # key: \u0026lt;string\u003e value: \u0026lt;string\u003e cloudprober.targets.GlobalTargetsOptions # # RDS server address # Deprecated: This option is now deprecated, please use rds_server_options # instead.rds_server_address: \u0026lt;string\u003e # RDS server options, for example: # rds_server_options { # server_address: \u0026#34;rds-server.xyz:9314\u0026#34; # oauth_config: { # ... # } # }rds_server_options: \u0026lt;cloudprober.rds.ClientConf.ServerOptions\u003e # GCE targets options.global_gce_targets_options: \u0026lt;cloudprober.targets.gce.GlobalOptions\u003e # Lame duck options. If provided, targets module checks for the lame duck # targets and removes them from the targets list.lame_duck_options: \u0026lt;cloudprober.targets.lameduck.Options\u003e cloudprober.targets.K8sTargets # # Targets namespace. If this field is unset, we select resources from all # namespaces.namespace: \u0026lt;string\u003e # labelSelector uses the same format as kubernetes API calls. # Example: # labelSelector: \u0026#34;k8s-app\u0026#34; # label k8s-app exists # labelSelector: \u0026#34;role=frontend\u0026#34; # label role=frontend # labelSelector: \u0026#34;!canary\u0026#34; # canary label doesn\u0026#39;t existlabelSelector: \u0026lt;string\u003e # Which resources to target. If value is not empty (\u0026#34;\u0026#34;), we use it as a # regex for resource names. # Example: # services: \u0026#34;\u0026#34; // All services. # endpoints: \u0026#34;.*-service\u0026#34; // Endpoints ending with \u0026#34;service\u0026#34;.[services \u0026lt;string\u0026gt; | endpoints \u0026lt;string\u0026gt; | \u0026nbsp;ingresses \u0026lt;string\u0026gt; | pods \u0026lt;string\u0026gt;]: \u0026lt;oneof\u003e # portFilter can be used to filter resources by port name. This is useful # for resources like endpoints and services, where each resource may have # multiple ports, and we may hit just a subset of those ports. portFilter # takes a regex -- we apply it on port names if port name is available, # otherwise we apply it port numbers. # Example: \u0026#34;.*-dns\u0026#34;, \u0026#34;metrics\u0026#34;, \u0026#34;.*-service\u0026#34;, etc.portFilter: \u0026lt;string\u003e # How often to re-check k8s API servers. Note this field will be irrelevant # when (and if) we move to the watch API. Default is 30s.re_eval_sec: \u0026lt;int32\u003e rds_server_options: \u0026lt;cloudprober.rds.ClientConf.ServerOptions\u003e cloudprober.targets.RDSTargets # # RDS server options, for example: # rds_server_options { # server_address: \u0026#34;rds-server.xyz:9314\u0026#34; # oauth_config: { # ... # } # }rds_server_options: \u0026lt;cloudprober.rds.ClientConf.ServerOptions\u003e # Resource path specifies the resources to return. Resources paths have the # following format: # \u0026lt;resource_provider\u0026gt;://\u0026lt;resource_type\u0026gt;/\u0026lt;additional_params\u0026gt; # # Examples: # For GCE instances in projectA: \u0026#34;gcp://gce_instances/\u0026lt;projectA\u0026gt;\u0026#34; # Kubernetes Pods : \u0026#34;k8s://pods\u0026#34;resource_path: \u0026lt;string\u003e # Filters to filter resources by.filter: \u0026lt;cloudprober.rds.Filter\u003e # IP config to specify the IP address to pick for a resource.ip_config: \u0026lt;cloudprober.rds.IPConfig\u003e cloudprober.targets.TargetsDef # [host_names \u0026lt;string\u0026gt; | shared_targets \u0026lt;string\u0026gt; | \u0026nbsp;gce_targets \u0026lt;cloudprober.targets.gce.TargetsConf\u0026gt; | rds_targets \u0026lt;cloudprober.targets.RDSTargets\u0026gt; | \u0026nbsp;file_targets \u0026lt;cloudprober.targets.file.TargetsConf\u0026gt; | k8s \u0026lt;cloudprober.targets.K8sTargets\u0026gt; | \u0026nbsp;dummy_targets \u0026lt;cloudprober.targets.DummyTargets\u0026gt;]: \u0026lt;oneof\u003e # Static endpoints. These endpoints are merged with the resources returned # by the targets type above. # Example: # endpoint { # name: \u0026#34;service-gtwy-1\u0026#34; # ip: \u0026#34;10.1.18.121\u0026#34; # port: 8080 # labels { # key: \u0026#34;service\u0026#34; # value: \u0026#34;products-service\u0026#34; # } # } # endpoint { # name: \u0026#34;frontend-url1\u0026#34; # url: \u0026#34;https://frontend.example.com/url1\u0026#34; # }endpoint: \u0026lt;cloudprober.targets.Endpoint\u003e # Regex to apply on the targets.regex: \u0026lt;string\u003e # Exclude lameducks. Lameduck targets can be set through RTC (realtime # configurator) service. This functionality works only if lame_duck_options # are specified.exclude_lameducks: \u0026lt;bool\u003e | default: true cloudprober.targets.file.TargetsConf # # File that contains resources in either textproto or json format. # Example in textproto format: # # resource { # name: \u0026#34;switch-xx-01\u0026#34; # ip: \u0026#34;10.11.112.3\u0026#34; # port: 8080 # labels { # key: \u0026#34;device_type\u0026#34; # value: \u0026#34;switch\u0026#34; # } # } # resource { # name: \u0026#34;switch-yy-01\u0026#34; # ip: \u0026#34;10.16.110.12\u0026#34; # port: 8080 # }file_path: \u0026lt;string\u003e filter: \u0026lt;cloudprober.rds.Filter\u003e format: (UNSPECIFIED|TEXTPB|JSON): \u0026lt;enum\u003e # If specified, file will be re-read at the given interval.re_eval_sec: \u0026lt;int32\u003e cloudprober.targets.gce.ForwardingRules # # Important: if multiple probes use forwarding_rules targets, only the # settings in the definition will take effect. # TODO(manugarg): Fix this behavior. # # For regional forwarding rules, regions to return forwarding rules for. # Default is to return forwarding rules from the region that the VM is # running in. To return forwarding rules from all regions, specify region as # \u0026#34;all\u0026#34;.region: \u0026lt;string\u003e # For global forwarding rules, if it is set to true, it will ignore # the value for the above region property.global_rule: \u0026lt;bool\u003e | default: false cloudprober.targets.gce.GlobalOptions # # How often targets should be evaluated/expandedre_eval_sec: \u0026lt;int32\u003e | default: 900 # Compute API version.api_version: \u0026lt;string\u003e | default: v1 cloudprober.targets.gce.Instances # # Use DNS to resolve target names (instances). If set to false (default), # IP addresses specified in the compute.Instance resource is used. If set # to true all the other resolving options are ignored.use_dns_to_resolve: \u0026lt;bool\u003e | default: false network_interface: \u0026lt;cloudprober.targets.gce.Instances.NetworkInterface\u003e # Labels to filter instances by (\u0026#34;key:value-regex\u0026#34; format).label: \u0026lt;string\u003e cloudprober.targets.gce.Instances.NetworkInterface # index: \u0026lt;int32\u003e | default: 0 ip_type: (PRIVATE|PUBLIC|ALIAS): \u0026lt;enum\u003e cloudprober.targets.gce.TargetsConf # # If running on GCE, this defaults to the local project. # Note: Multiple projects support in targets is experimental and may go away # with future iterations.project: \u0026lt;string\u003e [instances \u0026lt;cloudprober.targets.gce.Instances\u0026gt; | forwarding_rules \u0026lt;cloudprober.targets.gce.ForwardingRules\u0026gt;]: \u0026lt;oneof\u003e cloudprober.targets.lameduck.Options # # How often to check for lame-ducked targetsre_eval_sec: \u0026lt;int32\u003e | default: 10 # Runtime config project. If running on GCE, this defaults to the project # containing the VM.runtimeconfig_project: \u0026lt;string\u003e # Lame duck targets runtime config name. An operator will create a variable # here to mark a target as lame-ducked.runtimeconfig_name: \u0026lt;string\u003e | default: lame-duck-targets # Lame duck targets pubsub topic name. An operator will create a message # here to mark a target as lame-ducked.pubsub_topic: \u0026lt;string\u003e # Lame duck expiration time. We ignore variables (targets) that have been # updated more than these many seconds ago. This is a safety mechanism for # failing to cleanup. Also, the idea is that if a target has actually # disappeared, automatic targets expansion will take care of that some time # during this expiration period.expiration_sec: \u0026lt;int32\u003e | default: 300 # Use an RDS client to get lame-duck-targets. # This option is always true now and will be removed after v0.10.7.use_rds: \u0026lt;bool\u003e # RDS server options, for example: # rds_server_options { # server_address: \u0026#34;rds-server.xyz:9314\u0026#34; # oauth_config: { # ... # }rds_server_options: \u0026lt;cloudprober.rds.ClientConf.ServerOptions\u003e "}),e.add({id:30,href:"/docs/config/tlsconfig/",title:"TLSConfig Config",description:"Configs:\u0026nbsp;\u0026nbsp;main | alerting | metrics | oauth | probes | rds | servers | surfacer | targets | tlsconfig | utils | validators YAML|TextPB cloudprober.tlsconfig.TLSConfig # # CA certificate file to verify certificates provided by the other party.ca_cert_file: \u0026lt;string\u003e # Local certificate file.tls_cert_file: \u0026lt;string\u003e # Private key file corresponding to the certificate above.tls_key_file: \u0026lt;string\u003e # Whether to ignore the cert validation.disable_cert_validation: \u0026lt;bool\u003e # ServerName overrideserver_name: \u0026lt;string\u003e # Certificate reload interval in seconds.",content:" Configs:\u0026nbsp;\u0026nbsp;main | alerting | metrics | oauth | probes | rds | servers | surfacer | targets | tlsconfig | utils | validators YAML|TextPB cloudprober.tlsconfig.TLSConfig # # CA certificate file to verify certificates provided by the other party.ca_cert_file: \u0026lt;string\u003e # Local certificate file.tls_cert_file: \u0026lt;string\u003e # Private key file corresponding to the certificate above.tls_key_file: \u0026lt;string\u003e # Whether to ignore the cert validation.disable_cert_validation: \u0026lt;bool\u003e # ServerName overrideserver_name: \u0026lt;string\u003e # Certificate reload interval in seconds. If configured, the TLS cert will # be reloaded every reload_interval_sec seconds. This is useful when # certificates are generated and refreshed dynamically.reload_interval_sec: \u0026lt;int32\u003e cloudprober.tlsconfig.TLSConfig # # CA certificate file to verify certificates provided by the other party.ca_cert_file: \u0026lt;string\u003e # Local certificate file.tls_cert_file: \u0026lt;string\u003e # Private key file corresponding to the certificate above.tls_key_file: \u0026lt;string\u003e # Whether to ignore the cert validation.disable_cert_validation: \u0026lt;bool\u003e # ServerName overrideserver_name: \u0026lt;string\u003e # Certificate reload interval in seconds. If configured, the TLS cert will # be reloaded every reload_interval_sec seconds. This is useful when # certificates are generated and refreshed dynamically.reload_interval_sec: \u0026lt;int32\u003e "}),e.add({id:31,href:"/docs/config/utils/",title:"Utils Config",description:"Configs:\u0026nbsp;\u0026nbsp;main | alerting | metrics | oauth | probes | rds | servers | surfacer | targets | tlsconfig | utils | validators YAML|TextPB cloudprober.utils.httpreq.HTTPRequest # url: \u0026lt;string\u003e method: (GET|POST|PUT|DELETE|HEAD|OPTIONS|PATCH): \u0026lt;enum\u003e # Data to be sent as request body. If there are multiple \u0026#34;data\u0026#34; fields, we combine # their values with a \u0026#39;\u0026amp;\u0026#39; in between. Note: 1) If data appears to be a valid json, # we automatically set the content-type header to \u0026#34;application/json\u0026#34;, 2) If data # appears to be a query string we set content-type to # \u0026#34;application/x-www-form-urlencoded\u0026#34;.",content:" Configs:\u0026nbsp;\u0026nbsp;main | alerting | metrics | oauth | probes | rds | servers | surfacer | targets | tlsconfig | utils | validators YAML|TextPB cloudprober.utils.httpreq.HTTPRequest # url: \u0026lt;string\u003e method: (GET|POST|PUT|DELETE|HEAD|OPTIONS|PATCH): \u0026lt;enum\u003e # Data to be sent as request body. If there are multiple \u0026#34;data\u0026#34; fields, we combine # their values with a \u0026#39;\u0026amp;\u0026#39; in between. Note: 1) If data appears to be a valid json, # we automatically set the content-type header to \u0026#34;application/json\u0026#34;, 2) If data # appears to be a query string we set content-type to # \u0026#34;application/x-www-form-urlencoded\u0026#34;. Content type header can still be overridden # using the header field below.data: \u0026lt;string\u003e # HTTP request headersheader: \u0026lt;cloudprober.utils.httpreq.HTTPRequest.HeaderEntry\u003e cloudprober.utils.httpreq.HTTPRequest.HeaderEntry # key: \u0026lt;string\u003e value: \u0026lt;string\u003e cloudprober.utils.httpreq.HTTPRequest # url: \u0026lt;string\u003e method: (GET|POST|PUT|DELETE|HEAD|OPTIONS|PATCH): \u0026lt;enum\u003e # Data to be sent as request body. If there are multiple \u0026#34;data\u0026#34; fields, we combine # their values with a \u0026#39;\u0026amp;\u0026#39; in between. Note: 1) If data appears to be a valid json, # we automatically set the content-type header to \u0026#34;application/json\u0026#34;, 2) If data # appears to be a query string we set content-type to # \u0026#34;application/x-www-form-urlencoded\u0026#34;. Content type header can still be overridden # using the header field below.data: \u0026lt;string\u003e # HTTP request headersheader: \u0026lt;cloudprober.utils.httpreq.HTTPRequest.HeaderEntry\u003e cloudprober.utils.httpreq.HTTPRequest.HeaderEntry # key: \u0026lt;string\u003e value: \u0026lt;string\u003e "}),e.add({id:32,href:"/docs/config/validators/",title:"Validators Config",description:"Configs:\u0026nbsp;\u0026nbsp;main | alerting | metrics | oauth | probes | rds | servers | surfacer | targets | tlsconfig | utils | validators YAML|TextPB cloudprober.validators.Validator # name: \u0026lt;string\u003e [http_validator \u0026lt;cloudprober.validators.http.Validator\u0026gt; | integrity_validator \u0026lt;cloudprober.validators.integrity.Validator\u0026gt; | \u0026nbsp;json_validator \u0026lt;cloudprober.validators.json.Validator\u0026gt; | regex \u0026lt;string\u0026gt;]: \u0026lt;oneof\u003e cloudprober.validators.http.Validator # # Comma-separated list of success status codes and code ranges. # Example: success_stauts_codes: 200-299,301,302success_status_codes: \u0026lt;string\u003e # Comma-separated list of failure status codes and code ranges. If HTTP # status code matches failure_status_codes, validator fails.",content:" Configs:\u0026nbsp;\u0026nbsp;main | alerting | metrics | oauth | probes | rds | servers | surfacer | targets | tlsconfig | utils | validators YAML|TextPB cloudprober.validators.Validator # name: \u0026lt;string\u003e [http_validator \u0026lt;cloudprober.validators.http.Validator\u0026gt; | integrity_validator \u0026lt;cloudprober.validators.integrity.Validator\u0026gt; | \u0026nbsp;json_validator \u0026lt;cloudprober.validators.json.Validator\u0026gt; | regex \u0026lt;string\u0026gt;]: \u0026lt;oneof\u003e cloudprober.validators.http.Validator # # Comma-separated list of success status codes and code ranges. # Example: success_stauts_codes: 200-299,301,302success_status_codes: \u0026lt;string\u003e # Comma-separated list of failure status codes and code ranges. If HTTP # status code matches failure_status_codes, validator fails.failure_status_codes: \u0026lt;string\u003e # Header based validations. # TODO(manugarg): Add support for specifying multiple success and failure # headers. # # Success Header: # If specified, HTTP response headers should match the success_header for # validation to succeed. Example: # success_header: { # name: \u0026#34;Strict-Transport-Security\u0026#34; # value_regex: \u0026#34;max-age=31536000\u0026#34; # }success_header: \u0026lt;cloudprober.validators.http.Validator.Header\u003e # Failure Header: # If HTTP response headers match failure_header, validation fails.failure_header: \u0026lt;cloudprober.validators.http.Validator.Header\u003e # Last Modified Difference: # If specified, HTTP response\u0026#39;s Last-Modified header is checked to be # within the specified time difference from the current time. Example: # max_last_modified_diff_sec: 3600 # This will check that the Last-Modified header is within the last hour.max_last_modified_diff_sec: \u0026lt;uint64\u003e cloudprober.validators.http.Validator.Header # # Header name to look forname: \u0026lt;string\u003e # Header value to match. If omited - check for header existencevalue_regex: \u0026lt;string\u003e cloudprober.validators.integrity.Validator # # Validate the data integrity of the response using a pattern that is # repeated throughout the length of the response, with last len(response) % # len(pattern) bytes being zero bytes. # # For example if response length is 100 bytes and pattern length is 8 bytes, # first 96 bytes of the response should be pattern repeated 12 times, and # last 4 bytes should be set to zero byte (\u0026#39;\\0\u0026#39;)[pattern_string \u0026lt;string\u0026gt; | pattern_num_bytes \u0026lt;int32\u0026gt;]: \u0026lt;oneof\u003e cloudprober.validators.json.Validator # # If jq filter is specified, validator passes only if applying jq_filter to # the probe output, e.g. HTTP API response, results in \u0026#39;true\u0026#39; boolean. # See the following test file for some examples: # https://github.com/cloudprober/cloudprober/blob/master/validators/json/json_test.gojq_filter: \u0026lt;string\u003e cloudprober.validators.Validator # name: \u0026lt;string\u003e [http_validator \u0026lt;cloudprober.validators.http.Validator\u0026gt; | integrity_validator \u0026lt;cloudprober.validators.integrity.Validator\u0026gt; | \u0026nbsp;json_validator \u0026lt;cloudprober.validators.json.Validator\u0026gt; | regex \u0026lt;string\u0026gt;]: \u0026lt;oneof\u003e cloudprober.validators.http.Validator # # Comma-separated list of success status codes and code ranges. # Example: success_stauts_codes: 200-299,301,302success_status_codes: \u0026lt;string\u003e # Comma-separated list of failure status codes and code ranges. If HTTP # status code matches failure_status_codes, validator fails.failure_status_codes: \u0026lt;string\u003e # Header based validations. # TODO(manugarg): Add support for specifying multiple success and failure # headers. # # Success Header: # If specified, HTTP response headers should match the success_header for # validation to succeed. Example: # success_header: { # name: \u0026#34;Strict-Transport-Security\u0026#34; # value_regex: \u0026#34;max-age=31536000\u0026#34; # }success_header: \u0026lt;cloudprober.validators.http.Validator.Header\u003e # Failure Header: # If HTTP response headers match failure_header, validation fails.failure_header: \u0026lt;cloudprober.validators.http.Validator.Header\u003e # Last Modified Difference: # If specified, HTTP response\u0026#39;s Last-Modified header is checked to be # within the specified time difference from the current time. Example: # max_last_modified_diff_sec: 3600 # This will check that the Last-Modified header is within the last hour.max_last_modified_diff_sec: \u0026lt;uint64\u003e cloudprober.validators.http.Validator.Header # # Header name to look forname: \u0026lt;string\u003e # Header value to match. If omited - check for header existencevalue_regex: \u0026lt;string\u003e cloudprober.validators.integrity.Validator # # Validate the data integrity of the response using a pattern that is # repeated throughout the length of the response, with last len(response) % # len(pattern) bytes being zero bytes. # # For example if response length is 100 bytes and pattern length is 8 bytes, # first 96 bytes of the response should be pattern repeated 12 times, and # last 4 bytes should be set to zero byte (\u0026#39;\\0\u0026#39;)[pattern_string \u0026lt;string\u0026gt; | pattern_num_bytes \u0026lt;int32\u0026gt;]: \u0026lt;oneof\u003e cloudprober.validators.json.Validator # # If jq filter is specified, validator passes only if applying jq_filter to # the probe output, e.g. HTTP API response, results in \u0026#39;true\u0026#39; boolean. # See the following test file for some examples: # https://github.com/cloudprober/cloudprober/blob/master/validators/json/json_test.gojq_filter: \u0026lt;string\u003e "}),e.add({id:33,href:"/docs/overview/probe/",title:"What is a Probe",description:"Cloudprober runs probes, but what is a probe? A probe runs an operation, usually against a set of targets (e.g., your API servers), and looks for an expected outcome. Typically probes access your systems the same way as your customers, hence verifying systems\u0026rsquo; availability and performance from consumers\u0026rsquo; point of view. For example, an HTTP probe executes an HTTP request against a web server to verify that the web server is available.",content:`Cloudprober runs probes, but what is a probe? A probe runs an operation, usually against a set of targets (e.g., your API servers), and looks for an expected outcome. Typically probes access your systems the same way as your customers, hence verifying systems\u0026rsquo; availability and performance from consumers\u0026rsquo; point of view. For example, an HTTP probe executes an HTTP request against a web server to verify that the web server is available. Cloudprober probes run repeatedly at a configured interval and export probe results as a set of metrics.
Example of an HTTP Probe checking the frontend and API availability. _____________ _______________ | | HTTP Probe | | | Cloudprober | ------------\u0026gt; | Website/APIs | |_____________| |_______________| Here are some of the options used to configure a probe:
Field Description type Probe type, for example: HTTP, PING or UDP name Probe name. Each probe should have a unique name. interval_msec How often to run the probe (in milliseconds). timeout_msec Probe timeout (in milliseconds). targets Targets to run probe against. validator Probe validators, further explained here. \u0026lt;type\u0026gt;_probe Probe type specific configuration, e.g. http_probe Please take a look at the ProbeDef protobuf for further details on various fields and options. All probe types export at least the following metrics:
Metric Description total Total probes run so far. success Number of successful probes. Deficit between total and success indicates failures. latency Cumulative probe latency (by default in microseconds). You can get more insights into latency by using distributions. Note that by default all metrics are cumulative, i.e. we export sum of all the values so far. Cumulative metrics have this nice property that you don\u0026rsquo;t lose historical information if you miss a metrics read cycle, but they also make certain calculations slightly more complicated (see below). To provide a choice to the user, Cloudprober provides an option to export metrics as gauge values. See modifying metrics for more details.
Example: In prometheus, you\u0026rsquo;ll do something like the following to compute success ratio and average latency from cumulative metrics.
success_ratio_1m = increase(success[1m]) / increase(total[1m]) average_latency_1m = increase(latency[1m]) / increase(success[1m]) Probe Types #Cloudprober has built-in support for the following probe types:
HTTP External Ping DNS UDP TCP More probe types can be added through cloudprober extensions.
HTTP #Code | Config options
HTTP probe sends HTTP(s) requests to a target and verify that a response is received. Apart from the core probe metrics (total, success, and latency), HTTP probes also export a map of response code counts (resp_code). By default, requests are marked as successful as long as they succeed, regardless of the HTTP response code, but this behavior can be changed by using validators. For example, you can add a validator to require status code to in a certain range, or response body to match a regex, etc (validator example).
SSL Certificate Expiry: If the target serves an SSL Certificate, cloudprober will walk the certificate chain and export the earliest expiry time in seconds as a metric. The metric is named ssl_earliest_cert_expiry_sec, and will only be exported when the expiry time in seconds is a positive number. External #Code | Config options
External probe type allows running arbitrary programs for probing. This is useful for running complex checks through Cloudprober. External probes are documented in much more detail here: external probe.
Ping #Code | Config options
Ping probe type implements a fast native ICMP ping prober, that can probe hundreds of targets in parallel. Probe results are reported as number of packets sent (total), received (success) and round-trip time (latency). It supports both, privileged and unprivileged (uses ICMP datagram socket) pings.
Note that ICMP datagram sockets are not enabled by default on most Linux systems. You can enable them by running the following command: sudo sysctl -w net.ipv4.ping_group_range=\u0026quot;0 5000\u0026quot;
DNS #Code | Config options
As the name suggests, DNS probe sends a DNS request to the target. This is useful to verify that your DNS server, typically a critical component of the infrastructure e.g. kube-dns, is working as expected.
UDP #Code | Config options
UDP probe sends a UDP packet to the configured targets. UDP probe (and all other probes that use ports) provides more coverage for the network elements on the data path as most packet forwarding elements use 5-tuple hashing and using a new source port for each probe ensures that we hit different network element each time.
TCP #Code | Config options
TCP probe verifies that we can establish a TCP connection to the given target and port.
`}),search.addEventListener("input",t,!0);function t(){const s=5;var n=this.value,o=e.search(n,{limit:s,enrich:!0});const t=new Map;for(const e of o.flatMap(e=>e.result)){if(t.has(e.doc.href))continue;t.set(e.doc.href,e.doc)}if(suggestions.innerHTML="",suggestions.classList.remove("d-none"),t.size===0&&n){const e=document.createElement("div");e.innerHTML=`No results for "<strong>${n}</strong>"`,e.classList.add("suggestion__no-results"),suggestions.appendChild(e);return}for(const[r,a]of t){const n=document.createElement("div");suggestions.appendChild(n);const e=document.createElement("a");e.href=r,n.appendChild(e);const o=document.createElement("span");o.textContent=a.title,o.classList.add("suggestion__title"),e.appendChild(o);const i=document.createElement("span");if(i.textContent=a.description,i.classList.add("suggestion__description"),e.appendChild(i),suggestions.appendChild(n),suggestions.childElementCount==s)break}}})()
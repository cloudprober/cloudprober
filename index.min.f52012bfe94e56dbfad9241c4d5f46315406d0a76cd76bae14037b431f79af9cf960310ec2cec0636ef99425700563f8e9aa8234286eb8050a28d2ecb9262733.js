var suggestions=document.getElementById("suggestions"),search=document.getElementById("search");search!==null&&document.addEventListener("keydown",inputFocus);function inputFocus(e){e.ctrlKey&&e.key==="/"&&(e.preventDefault(),search.focus()),e.key==="Escape"&&(search.blur(),suggestions.classList.add("d-none"))}document.addEventListener("click",function(e){var t=suggestions.contains(e.target);t||suggestions.classList.add("d-none")}),document.addEventListener("keydown",suggestionFocus);function suggestionFocus(e){const s=suggestions.classList.contains("d-none");if(s)return;const t=[...suggestions.querySelectorAll("a")];if(t.length===0)return;const n=t.indexOf(document.activeElement);if(e.key==="ArrowUp"){e.preventDefault();const s=n>0?n-1:0;t[s].focus()}else if(e.key==="ArrowDown"){e.preventDefault();const s=n+1<t.length?n+1:n;t[s].focus()}}(function(){var e=new FlexSearch.Document({tokenize:"forward",cache:100,document:{id:"id",store:["href","title","description"],index:["title","description","content"]}});e.add({id:0,href:"/docs/overview/",title:"Get Started",description:"",content:""}),e.add({id:1,href:"/docs/how-to/",title:"How To",description:"",content:""}),e.add({id:2,href:"/docs/how-to/list/",title:"How Tos",description:" External Probe Alerting Targets Kubernetes Targets Running on Kubernetes Additional Labels Validators Percentiles and Histograms Extending Cloudprober Built-in Servers Resource Discovery Service ",content:" External Probe Alerting Targets Kubernetes Targets Running on Kubernetes Additional Labels Validators Percentiles and Histograms Extending Cloudprober Built-in Servers Resource Discovery Service "}),e.add({id:3,href:"/docs/about/",title:"About Cloudprober",description:`Posted\u0026nbsp;on\u0026nbsp;Oct 05, 2023 by Manu Garg\u0026nbsp;\u0026hyphen;\u0026nbsp;3 min\u0026nbsp;read Origin #I started building Cloudprober in 2016, while I was at Google, leading the Cloud Networking SRE team there. Google Cloud was just beginning to grow big, and we were still grappling with some early growth issue. Our biggest problem was that our customers were discovering problems before us, which resulted in bad experience for our customers and huge time sink for my team in debugging those issues.`,content:`Posted\u0026nbsp;on\u0026nbsp;Oct 05, 2023 by Manu Garg\u0026nbsp;\u0026hyphen;\u0026nbsp;3 min\u0026nbsp;read Origin #I started building Cloudprober in 2016, while I was at Google, leading the Cloud Networking SRE team there. Google Cloud was just beginning to grow big, and we were still grappling with some early growth issue. Our biggest problem was that our customers were discovering problems before us, which resulted in bad experience for our customers and huge time sink for my team in debugging those issues.1.
Google\u0026rsquo;s existing monitoring tools didn\u0026rsquo;t work well in Cloud, necessitating the need to build things from ground up. And since probers are the cornerstone of monitoring and reliability at Google2, that\u0026rsquo;s where we decided to start. Thus began the journey of Cloudprober.
Even though the primary goal of Cloudprober at that time was to discover and alert on Cloud Networking availability and performance problems, we decided to develop it as a generic prober that could be used to monitor a wide variety of systems and services. We also decided to make Cloudprober open source so that a wider community could trust it, contribute to it, and run it on their own systems.
Scale, Efficiency #For scales as big as Google Cloud, horizontal scalability and efficiency become critical requirements, and for a monitoring software to be useful reliability is super important as well. Keeping these requirements in mind, our goal for Cloudprober was for it to be able to reliably monitor 100s of 1000s of endpoints (IPs, Ports, HTTP/S URLs, etc) from each instance, while keeping the resource requirements and management overhead very low3.
Cloudprober maximizes resources utilization by relying heavily on Go concurrency (resource efficiency), supports probing large number of targets in parallel at a high frequency (each instance does more), minimizes the need of frequent updates by supporting dynamic targets discovery (ease of management), has native implementations for common probe types (efficiency), and so on.
Beyond Google and Open-Source #We open-sourced Cloudprober in 2017. That brought in a new phase in its evolution. We added many features over time to make it more useful to the wider community, such as first-class Kubernetes support, a built-in probe status UI, PostgreSQL and Cloudwatch surfacers, OAuth support, Validators, and most recently, built-in alerting capability.
We used the same codebase for the internal and open-source versions, which was more work but it created a huge advantage \u0026ndash; our own extensive internal deployment provided a continuous testing platform for Cloudprober, particularly for its scalability and performance aspects, while we added all these features.
Move away from Google Github #I left Google in Nov 2021. To keep working on Cloudprober independently, I moved Cloudprober\u0026rsquo;s Github repository from github.com/google/cloudprober to github.com/cloudprober/cloudprober. This was a disruptive move and we lost a lot of Github stars in the process (1.4k - ðŸ˜ƒ), but overall it was a good move as Cloudprober has grown much faster after becoming independent.
While I can\u0026rsquo;t say this authoritatively now as I don\u0026rsquo;t work there anymore, from what I know, Google still uses Cloudprober, in fact, even more widely now.
Growth and stability #Throughout its journey, Cloudprober has continuously adapted and expanded to meet the evolving needs of its users4. To ensure that Cloudprober thrives and evolves robustly, we\u0026rsquo;ve been very diligent that it grows in a structured way, a commitment we\u0026rsquo;ll uphold in future as well.
A customer-reported infrastructure issue is much harder to debug than an issue discovered by your own monitoring.\u0026#160;\u0026#x21a9;\u0026#xfe0e;
Almost all of Google\u0026rsquo;s systems rely on probers to detect customer facing problems.\u0026#160;\u0026#x21a9;\u0026#xfe0e;
Hostinger was able to probe 1.8M targets using a single instance: blog.\u0026#160;\u0026#x21a9;\u0026#xfe0e;
I think it\u0026rsquo;s an essential trait for any software. Software that don\u0026rsquo;t evolve with time wither away.\u0026#160;\u0026#x21a9;\u0026#xfe0e;
`}),e.add({id:4,href:"/docs/how-to/alerting/",title:"Alerting",description:`You can configure Cloudprober to send alerts on probe failures. Alerts are configured per probe and each probe can have multiple alerts with independent configuration. Alert configuration consists of mainly two parts:
Alert condition Notification config Alert Condition #Alert condition is defined in terms of number of failures (failures) out of a number of attempts (total). For example, if alert condition is specified as: condition {failures: 3, total: 5}, an alert will be triggered if 3 probes have failed out of the last 5 attempts.`,content:`You can configure Cloudprober to send alerts on probe failures. Alerts are configured per probe and each probe can have multiple alerts with independent configuration. Alert configuration consists of mainly two parts:
Alert condition Notification config Alert Condition #Alert condition is defined in terms of number of failures (failures) out of a number of attempts (total). For example, if alert condition is specified as: condition {failures: 3, total: 5}, an alert will be triggered if 3 probes have failed out of the last 5 attempts.
If no condition is specified, both failures and total are assumed to be 1, i.e., alert will trigger on the first probe failure itself.
Alert condition definition lets you take care of both the cases: continuous failures and sporadic failures. For example, if you run a probe every 30s and alert condition is {failures: 4, total: 10}, alert will trigger either after 2 minutes of consecutive failures, or if probe has failed 4 times in last 5 minutes.
More examples and explanation:
F=failure, S=success
condition { failure: 3 } or condition { failures: 3, total: 3} Trigger an alert on 3 consecutive failures. A pattern like F F F will immediately trigger an alert, but S S F F S S F F S S will not, even though failures are happening quite regularly. You could catch the second pattern by configuring the alert condition as {failures: 3, total: 5}.
condition { failure: 3, total: 6 }: Alert will trigger on 3 failures in the last 6 attempts. A pattern like F F F will immediately trigger an alert, so if probe interval is 10s, you\u0026rsquo;ll get an alert within 20-30s of incident starting. A pattern like F S F S S F or F F S S S F will also trigger an alert.
Alerts Dashboard #Cloudprober comes with an alerts dashboard that you can access at the /alerts URL. Alerts dashboard shows currently firing and 20 historical alerts.
Notifications #When you add alerts, you\u0026rsquo;d probably also want to be notified when they fire. You can do that using the notify config block:
# This example is in YAML format. You can use the original textpb format too. # See https://cloudprober.org/docs/config/alerting/cloudprober_alerting_AlertConf probe: ... alert: notify: pager_duty: routing_key: \u0026quot;...\u0026quot; slack: webhook_url: \u0026quot;...\u0026quot; Above example configures two notifications: PagerDuty \u0026amp; Slack. Cloudprober currently supports the following notification targets:
Email PagerDuty Opsgenie Slack Command HTTP Configuration documentation (linked above) has more details on each of them.
Notification Fields #You can customize the information included in the alert notification. The following table shows the top-level notification parameters, corresponding config fields, and their default values.
Info Configuration Field Default Dashboard URL dashboard_url_template http://localhost:9313/status?probe=@probe@ Playbook URL playbook_url_template \u0026quot;\u0026quot; Summary summary_template Cloudprober alert \u0026ldquo;@alert@\u0026rdquo; for \u0026ldquo;@target@\u0026rdquo; Details details_template Cloudprober alert \u0026ldquo;@alert@\u0026rdquo; for \u0026ldquo;@target@\u0026rdquo;:Failures: @failures@ out of @total@ probesFailing since: @since@
Probe: @probe@ Dashboard: @dashboard_url@ Playbook: @playbook_url@ As you see here, you can embed alert information in notifications using placeholders like @field@. For example if you host your alert playbooks at https://playbook/, you can configure playbook_url to be https://playbook/@alert@. Cloudprober supports the following alert fields placeholders:
Placeholder Alert field @alert@ Alert name. Same as probe name if alert name is not configured @probe@ Probe name @target@ Target name @target.label.\u0026lt;label\u0026gt;@ Target label value, e.g. if target has a label: env=prod, @target.label.env@ will be replaced with prod @since@ Alert start time @failure@ Failure count that caused the alert @total@ Total number of probes @target_ip@ Target IP if available. It only works for targets discovered by Cloudprober @dashboard_url@, @playbook_url@, @summary@, @details@ See the table above. `}),e.add({id:5,href:"/docs/",title:"Docs",description:"",content:""}),e.add({id:6,href:"/docs/how-to/external-probe/",title:"External Probe",description:`External probe type allows you to run arbitrary, complex probes through Cloudprober. An external probe runs an independent external program for actual probing. Cloudprober calculates probe metrics based on program\u0026rsquo;s exit status and time elapsed in execution.
Cloudprober also allows external programs to provide additional metrics. Every message sent to stdout will be parsed as a new metrics to be emitted. For general logging you can use another I/O stream like stderr.`,content:`External probe type allows you to run arbitrary, complex probes through Cloudprober. An external probe runs an independent external program for actual probing. Cloudprober calculates probe metrics based on program\u0026rsquo;s exit status and time elapsed in execution.
Cloudprober also allows external programs to provide additional metrics. Every message sent to stdout will be parsed as a new metrics to be emitted. For general logging you can use another I/O stream like stderr.
Sample Probe #To understand how it works, lets create a sample probe that sets and gets a key in a redis server. Here is the main function of such a probe:
func main() { var client redis.Client var key = \u0026quot;hello\u0026quot; startTime := time.Now() client.Set(key, []byte(\u0026quot;world\u0026quot;)) fmt.Printf(\u0026quot;op_latency_ms{op=set} %f\\n\u0026quot;, float64(time.Since(startTime).Nanoseconds())/1e6) startTime = time.Now() val, _ := client.Get(\u0026quot;hello\u0026quot;) log.Printf(\u0026quot;%s=%s\u0026quot;, key, string(val)) fmt.Printf(\u0026quot;op_latency_ms{op=get} %f\\n\u0026quot;, float64(time.Since(startTime).Nanoseconds())/1e6) } (Full listing: https://github.com/cloudprober/cloudprober/blob/master/examples/external/redis_probe.go) This program sets and gets a key in redis and prints the time taken for both operations. op_latency_ms{op=get|set} will be emitted as metrics. You could also define your own labels using this format:
Cloudprober can use this program as an external probe, to verify the availability and performance of the redis server. This program assumes that redis server is running locally, at its default port. For the sake of demonstration, lets run a local redis server (you can also easily modify this program to use a different server.)
#!bash OS=\$(uname) [[ \u0026quot;\$OS\u0026quot; == \u0026quot;Darwin\u0026quot; ]] \u0026amp;\u0026amp; brew install redis [[\u0026quot;\$OS\u0026quot; == \u0026quot;Linux\u0026quot;]] \u0026amp;\u0026amp; sudo apt install redis Let\u0026rsquo;s compile our probe program (redis_probe.go) and verify that it\u0026rsquo;s working as expected:
#!bash CGO_ENABLED=0 go build -ldflags â€œ-extldflags=-staticâ€ examples/external/redis_probe.go ./redis_probe 2022/02/24 12:39:45 hello=world op_latency_ms{op=set} 22.656588 op_latency_ms{op=get} 2.173560 Configuration #Here is the external probe configuration that makes use of this program:
Full example in examples/external/cloudprober.cfg.
# Run an external probe that executes a command from the current working # directory. probe { name: \u0026quot;redis_probe\u0026quot; type: EXTERNAL targets { dummy_targets {} } external_probe { mode: ONCE command: \u0026quot;./redis_probe\u0026quot; } } Note: To pass target information to your external program as arguments use the @label@ notation. Supported fields are: @target@, @address@, @port@, @probe@, and target labels like @target.label.fqdn@.
command: \u0026quot;./redis_probe\u0026quot; -host=@address@ -port=@port@ Running it through cloudprober, you\u0026rsquo;ll see the following output:
# Launch cloudprober cloudprober --config_file=cloudprober.cfg cloudprober 1519..0 1519583408 labels=ptype=external,probe=redis_probe,dst= success=1 total=1 latency=12143.765 cloudprober 1519..1 1519583408 labels=ptype=external,probe=redis_probe,dst=,op=get op_latency_ms=0.516 get_latency_ms=0.491 cloudprober 1519..2 1519583410 labels=ptype=external,probe=redis_probe,dst= success=2 total=2 latency=30585.915 cloudprober 1519..3 1519583410 labels=ptype=external,probe=redis_probe,dst=,op=set op_latency_ms=0.636 get_latency_ms=0.994 cloudprober 1519..4 1519583412 labels=ptype=external,probe=redis_probe,dst= success=3 total=3 latency=42621.871 You can import this data in prometheus following the process outlined at: Running Prometheus. Before doing that, let\u0026rsquo;s make it more interesting.
Distributions #How nice will it be if we could find distribution of the set and get latency. If tail latency was too high, it could explain the random timeouts in your application. Fortunately, it\u0026rsquo;s very easy to create distributions in Cloudprober. You just need to add the following section to your probe definition:
Full example in examples/external/cloudprober_aggregate.cfg.
# Run an external probe and aggregate metrics in cloudprober. ... output_metrics_options { aggregate_in_cloudprober: true # Create distributions for get_latency_ms and set_latency_ms. dist_metric { key: \u0026quot;op_latency_ms\u0026quot; value: { explicit_buckets: \u0026quot;0.1,0.2,0.4,0.6,0.8,1.0,2.0\u0026quot; } } } This configuration adds options to aggregate the metrics in the cloudprober and configures \u0026ldquo;op_latency_ms\u0026rdquo; as a distribution metric with explicit buckets. Cloudprober will now build cumulative distributions using for these metrics. We can import this data in Stackdriver or Prometheus and get the percentiles of the \u0026ldquo;get\u0026rdquo; and \u0026ldquo;set\u0026rdquo; latencies. Following screenshot shows the grafana dashboard built using these metrics.
Server Mode #The probe that we created above forks out a new redis_probe process for every probe cycle. This can get expensive if probe frequency is high and the process is big (e.g. a Java binary). Also, what if you want to keep some state across probes, for example, lets say you want to monitor performance over HTTP/2 where you keep using the same TCP connection for multiple HTTP requests. A new process every time makes keeping state impossible.
External probe\u0026rsquo;s server mode provides a way to run the external probe process in daemon mode. Cloudprober communicates with this process over stdout/stdin (connected with OS pipes), using serialized protobuf messages. Cloudprober comes with a serverutils package that makes it easy to build external probe servers in Go.
Please see the code at examples/external/redis_probe.go for server mode implementation of the above probe. Here is the corresponding cloudprober config to run this probe in server mode: examples/external/cloudprober_server.cfg.
In server mode, if external probe process dies for reason, it\u0026rsquo;s restarted by Cloudprober.
`}),e.add({id:7,href:"/docs/how-to/k8s_targets/",title:"Kubernetes Targets",description:`Cloudprober supports dynamic discovery of Kubernetes resources (e.g. pods, endpoints, ingresses, etc) through the targets type k8s.
For example, the following config adds an HTTP probe for the endpoints named cloudprober (equivalent to running kubectl get ep cloudprober).
probe { name: \u0026quot;pod-to-endpoints\u0026quot; type: HTTP targets { # Equivalent to kubectl get ep cloudprober k8s { endpoints: \u0026quot;cloudprober\u0026quot; } } # Note that the following http_probe automatically uses target's discovered # port.`,content:`Cloudprober supports dynamic discovery of Kubernetes resources (e.g. pods, endpoints, ingresses, etc) through the targets type k8s.
For example, the following config adds an HTTP probe for the endpoints named cloudprober (equivalent to running kubectl get ep cloudprober).
probe { name: \u0026quot;pod-to-endpoints\u0026quot; type: HTTP targets { # Equivalent to kubectl get ep cloudprober k8s { endpoints: \u0026quot;cloudprober\u0026quot; } } # Note that the following http_probe automatically uses target's discovered # port. http_probe { relative_url: \u0026quot;/status\u0026quot; } } Supported Resource and Filters #Cloudprober supports discovery for the following k8s resources:
Services Endpoints Pods Ingresses Filters #You can filter k8s resources using the following options:
name: (regex) Resource name filter. It can be a regex. Example: # Endpoints with names ending in \u0026quot;service\u0026quot; targets { k8s { endpoints: \u0026quot;.*-service\u0026quot; } } namespace: Namespace filter. Example: # Ingresses in \u0026quot;prod\u0026quot; namespace, ending in \u0026quot;lb\u0026quot; targets { k8s { namespace: \u0026quot;prod\u0026quot; ingresses: \u0026quot;.*-lb\u0026quot; } } # Kube-DNS service targets { k8s { namespace: \u0026quot;kube-system\u0026quot; services: \u0026quot;kube-dns\u0026quot; } } labelSelector: Label based selector. It can be repeated, and works similar to the kubectl\u0026rsquo;s \u0026ndash;selector/-l flag. Example: targets { k8s { pods: \u0026quot;.*\u0026quot; labelSelector: \u0026quot;k8s-app\u0026quot; # k8a-app label exists labelSelector: \u0026quot;role=frontend\u0026quot; # label \u0026quot;role\u0026quot; is set to \u0026quot;frontend\u0026quot; labelSelector: \u0026quot;!no-monitoring\u0026quot; # label \u0026quot;no-monitoring is not set\u0026quot; } } portFilter: (regex) Filter resources by port name or number (if port name is not set). This is useful for resources like endpoints and services, where each resource may have multiple ports. Example: targets { k8s { endpoints: \u0026quot;.*-service\u0026quot; portFilter: \u0026quot;http-.*\u0026quot; } } Cluster Resources Access #Note: If you\u0026rsquo;ve installed Cloudprober using Helm Chart, this step is automatically taken care of.
Cloudprober discovers k8s resources using kubernetes APIs. It assumes that we are interested in the cluster we are running it in, and uses in-cluster config to talk to the kubernetes API server. For this set up to work, we need to give our container read-only access to kubernetes resources:
# Define a ClusterRole (resource-reader) for read-only access to the cluster # resources and bind this ClusterRole to the default service account. cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: v1 kind: ServiceAccount metadata: name: cloudprober --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \u0026quot;true\u0026quot; name: resource-reader namespace: default rules: - apiGroups: [\u0026quot;\u0026quot;] resources: [\u0026quot;*\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;] - apiGroups: - extensions - \u0026quot;networking.k8s.io\u0026quot; # k8s 1.14+ resources: - ingresses - ingresses/status verbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;] --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: default-resource-reader namespace: default subjects: - kind: ServiceAccount name: cloudprober namespace: default roleRef: kind: ClusterRole name: resource-reader apiGroup: rbac.authorization.k8s.io EOF This will create a new service account cloudprober and will give it read-only access to the cluster resources.
`}),e.add({id:8,href:"/docs/how-to/run-on-kubernetes/",title:"Running On Kubernetes",description:`Kubernetes is a popular platform for running containers, and Cloudprober container runs on Kubernetes right out of the box. This document shows how you can run Cloudprober on kubernetes, use ConfigMap for config, and discover kubernetes targets automatically.
â“˜ If you use helm charts for k8s installations, Cloudprober helm chart provides the most convenient way to run Cloudprober on k8s.ConfigMap #In Kubernetes, a convenient way to provide config to containers is to use config maps.`,content:`Kubernetes is a popular platform for running containers, and Cloudprober container runs on Kubernetes right out of the box. This document shows how you can run Cloudprober on kubernetes, use ConfigMap for config, and discover kubernetes targets automatically.
â“˜ If you use helm charts for k8s installations, Cloudprober helm chart provides the most convenient way to run Cloudprober on k8s.ConfigMap #In Kubernetes, a convenient way to provide config to containers is to use config maps. Let\u0026rsquo;s create a config that specifies a probe to monitor \u0026ldquo;google.com\u0026rdquo;.
probe { name: \u0026quot;google-http\u0026quot; type: HTTP targets { host_names: \u0026quot;www.google.com\u0026quot; } http_probe {} interval_msec: 15000 timeout_msec: 1000 } Save this config in cloudprober.cfg, create a config map using the following command:
kubectl create configmap cloudprober-config \\ --from-file=cloudprober.cfg=cloudprober.cfg If you change the config, you can update the config map using the following command:
kubectl create configmap cloudprober-config \\ --from-file=cloudprober.cfg=cloudprober.cfg -o yaml --dry-run | \\ kubectl replace -f - Deployment Map #Now let\u0026rsquo;s add a deployment.yaml to add the config volume and cloudprober container:
apiVersion: apps/v1 kind: Deployment metadata: name: cloudprober spec: replicas: 1 selector: matchLabels: app: cloudprober template: metadata: annotations: checksum/config: \u0026quot;\${CONFIG_CHECKSUM}\u0026quot; labels: app: cloudprober spec: volumes: - name: cloudprober-config configMap: name: cloudprober-config containers: - name: cloudprober image: cloudprober/cloudprober command: [\u0026quot;/cloudprober\u0026quot;] args: [\u0026quot;--config_file\u0026quot;, \u0026quot;/cfg/cloudprober.cfg\u0026quot;] volumeMounts: - name: cloudprober-config mountPath: /cfg ports: - name: http containerPort: 9313 --- apiVersion: v1 kind: Service metadata: name: cloudprober labels: app: cloudprober spec: ports: - port: 9313 protocol: TCP targetPort: 9313 selector: app: cloudprober type: NodePort Note that we added an annotation to the deployment spec; this annotation allows us to update the deployment whenever cloudprober config changes. We can update this annotation based on the local cloudprober config content, and update the deployment using the following one-liner:
# Update the config checksum annotation in deployment.yaml before running # kubectl apply. export CONFIG_CHECKSUM=\$(kubectl get cm/cloudprober-config -o yaml | sha256sum) \u0026amp;\u0026amp; \\ cat deployment.yaml | envsubst | kubectl apply -f - (Note: If you use Helm for Kubernetes deployments, Helm provides a more native way to include config checksums in deployments.)
Applying the above yaml file, should create a deployment with a service at port 9313:
\$ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE cloudprober 1/1 1 1 94m \$ kubectl get service cloudprober NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE cloudprober NodePort 10.31.249.108 \u0026lt;none\u0026gt; 9313:31367/TCP 94m Now you should be able to access various cloudprober URLs (/status for status,/config for config, /metrics for prometheus-format metrics) from within the cluster. For quick verification you can also set up a port forwarder and access these URLs locally at localhost:9313:
kubectl port-forward svc/cloudprober 9313:9313 Once you\u0026rsquo;ve verified that everything is working as expected, you can go on setting up metrics collection through prometheus (or stackdriver) in usual ways.
Kubernetes Targets #If you\u0026rsquo;re running on Kubernetes, you\u0026rsquo;d probably want to monitor Kubernetes resources (e.g. pods, endpoints, etc) as well. Cloudprober supports dynamic discovery of Kubernetes resources through the targets type k8s.
For example, the following config adds an HTTP probe for the endpoints named cloudprober (equivalent to running kubectl get ep cloudprober).
probe { name: \u0026quot;pod-to-endpoints\u0026quot; type: HTTP targets { # Equivalent to kubectl get ep cloudprober k8s { endpoints: \u0026quot;cloudprober\u0026quot; } } # Note that the following http_probe automatically uses target's discovered # port. http_probe { relative_url: \u0026quot;/status\u0026quot; } } See Kubernetes Targets for more details on Kubernetes targets.
Push Config Update #To push new cloudprober config to the cluster:
# Update the config map kubectl create configmap cloudprober-config \\ --from-file=cloudprober.cfg=cloudprober.cfg -o yaml --dry-run | \\ kubectl replace -f - # Update deployment export CONFIG_CHECKSUM=\$(kubectl get cm/cloudprober-config -o yaml | sha256sum) \u0026amp;\u0026amp; \\ cat deployment.yaml | envsubst | kubectl apply -f - Cloudprober should now start monitoring cloudprober endpoints. To verify:
# Set up port fowarding such that you can access cloudprober:9313 through # localhost:9313. kubectl port-forward svc/cloudprober 9313:9313 \u0026amp; # Check status curl localhost:9313/status # Check metrics (prometheus data format) curl localhost:9313/metrics If you\u0026rsquo;re running on GKE and have not disabled cloud logging, you\u0026rsquo;ll also see logs in Stackdriver Logging.
`}),e.add({id:9,href:"/docs/how-to/additional-labels/",title:"Additional Labels",description:`You can add additional labels to Cloudprober metrics using two methods:
Probe level config field: additional_label Through an environment variable: CLOUDPROBER_ADDITIONAL_LABELS. Probe Level Additional Labels #You can add additional labels to a specific probe\u0026rsquo;s metrics using probe-level config field: additional_label. An additional label\u0026rsquo;s value can be static, or it can be determined at the run-time: from the environment that the probe is running in (e.g. GCE instance labels, or location), or target\u0026rsquo;s labels.`,content:`You can add additional labels to Cloudprober metrics using two methods:
Probe level config field: additional_label Through an environment variable: CLOUDPROBER_ADDITIONAL_LABELS. Probe Level Additional Labels #You can add additional labels to a specific probe\u0026rsquo;s metrics using probe-level config field: additional_label. An additional label\u0026rsquo;s value can be static, or it can be determined at the run-time: from the environment that the probe is running in (e.g. GCE instance labels, or location), or target\u0026rsquo;s labels.
Example config here demonstrates adding various types of additional labels to probe metrics. For this config (also listed below for quick rerefence):
if ingress target has label \u0026ldquo;fqdn:app.example.com\u0026rdquo;, and prober is running in the GCE zone us-east1-c, and prober\u0026rsquo;s GCE instance has label env:prod. Probe metrics will look like the following:
total{probe=\u0026quot;my_ingress\u0026quot;,ptype=\u0026quot;http\u0026quot;,metrictype=\u0026quot;prober\u0026quot;,env=\u0026quot;prod\u0026quot;,src_zone=\u0026quot;us-east1-c\u0026quot;,host=\u0026quot;app.example.com\u0026quot;}: 90 success{probe=\u0026quot;my_ingress\u0026quot;,ptype=\u0026quot;http\u0026quot;,metrictype=\u0026quot;prober\u0026quot;,env=\u0026quot;prod\u0026quot;,src_zone=\u0026quot;us-east1-c\u0026quot;,host=\u0026quot;app.example.com\u0026quot;}: 80 probe { name: \u0026quot;my_ingress\u0026quot; type: HTTP targets { k8s { ingresses: \u0026quot;\u0026quot; namespace: \u0026quot;default\u0026quot; } } # Static label additional_label { key: \u0026quot;metrictype\u0026quot; value: \u0026quot;prober\u0026quot; } # Label is configured at the run-time, based on the prober instance label (GCE). additional_label { key: \u0026quot;env\u0026quot; value: \u0026quot;{{.label_env}}\u0026quot; } # Label is configured at the run-time, based on the prober environment (GCE). additional_label { key: \u0026quot;src_zone\u0026quot; value: \u0026quot;{{.zone}}\u0026quot; } # Label is configured based on the target's labels. additional_label { key: \u0026quot;host\u0026quot; value: \u0026quot;@target.label.fqdn@\u0026quot; } http_probe {} } (Listing source: examples/additional_label/cloudprober.cfg)
Global Additional Labels #You can also add labels to all metrics exported by cloudprober using an environment variable: CLOUDPROBER_ADDITIONAL_LABELS. You can choose a different environemnt variable or disable this behavior completely by modifying the additional_labels_env_var config field in the surfacers config.
Value of the environment variable should be a comma separated list of key=value pairs. For example if CLOUDPROBER_ADDITIONAL_LABELS is set to app=ingester,env=prod, cloudprober will add the following two labels to all the metrics: {env=prod, app=ingester}.
This feature is particularly useful in a multi-single-tenant setup (where you run one instance per tenant) to filter out metrics by tenant. For example, you can add tenant=team1 label to all cloudprober metrics by setting CLOUDPROBER_ADDITIONAL_LABELS=tenant=team1 in team1\u0026rsquo;s pod\u0026rsquo;s environment.
Note that existing probe labels have precedence over environment based labels. If probe metrics already have a label (e.g. dst), and you try to add the same label through this method, it will be silently ignored.
Related #See Exporting Metrics to learn more about how metrics are exported from Cloudprober.
`}),e.add({id:10,href:"/docs/how-to/validators/",title:"Validators",description:`Validators allow you to run checks on the probe request output (if any). For example, you can specify if you expect the probe output to match a certain regex or return a certain status code (for HTTP). You can configure more than one validators and all validators should succeed for the probe to be marked as success.
probe { name: \u0026quot;google_homepage\u0026quot; type: HTTP targets { host_names: \u0026quot;www.google.com\u0026quot; } interval_msec: 10000 # Probe every 10s # This validator should succeed.`,content:`Validators allow you to run checks on the probe request output (if any). For example, you can specify if you expect the probe output to match a certain regex or return a certain status code (for HTTP). You can configure more than one validators and all validators should succeed for the probe to be marked as success.
probe { name: \u0026quot;google_homepage\u0026quot; type: HTTP targets { host_names: \u0026quot;www.google.com\u0026quot; } interval_msec: 10000 # Probe every 10s # This validator should succeed. validator { name: \u0026quot;status_code_2xx\u0026quot; http_validator { success_status_codes: \u0026quot;200-299\u0026quot; } } # This validator will fail, notice missing 'o' in our regex. validator { name: \u0026quot;gogle_re\u0026quot; regex: \u0026quot;gogle\u0026quot; } } (Full listing: https://github.com/cloudprober/cloudprober/blob/master/examples/validators/cloudprober_validator.cfg)
To make the debugging easier, validation failures are logged and exported as an independent map counter \u0026ndash; validation_failure, with validator key. For example, the above example will result in the following counters being exported after 5 runs:
total{probe=\u0026quot;google_homepage\u0026quot;,dst=\u0026quot;www.google.com\u0026quot;} 5 success{probe=\u0026quot;google_homepage\u0026quot;,dst=\u0026quot;www.google.com\u0026quot;} 0 validation_failure{validator=\u0026quot;status_code_2xx\u0026quot;,probe=\u0026quot;google_homepage\u0026quot;,dst=\u0026quot;www.google.com\u0026quot;} 0 validation_failure{validator=\u0026quot;gogle_re\u0026quot;,probe=\u0026quot;google_homepage\u0026quot;,dst=\u0026quot;www.google.com\u0026quot;} 5 Note that validator counter will not go up if probe fails for other reasons, for example web server timing out. That\u0026rsquo;s why you typically don\u0026rsquo;t want to alert only on validation failures. That said, in some cases, validation failures could be the only thing you\u0026rsquo;re interested in, for example, if you\u0026rsquo;re trying to make sure that a certain copyright is always present in your web pages or you want to catch data integrity issues in your network.
Let\u0026rsquo;s take a look at the types of validators you can configure.
Regex Validator #Regex validator simply checks for a regex in the probe request output. It works for all probe types except for UDP and UDP_LISTENER - these probe types don\u0026rsquo;t support any validators at the moment.
HTTP Validator #HTTP response validator works only for the HTTP probe type. You can currently use HTTP validator to define success and failure status codes (represented by success_status_codes and failure_stauts_codes in the config):
If failure_status_codes is defined and response status code falls within that range, validator is considered to have failed. If success_status_codes is defined and response status code does not fall within that range, validator is considered to have failed. If failure_header is defined and HTTP response include specified header and there are matching values, validator is considered to have failed. Leaving value_regex empty checks only for header name. If success_header is defined and HTTP response does not include specified header with matching values, validator is considered to have failed. Leaving value_regex empty checks only for header name. Data Integrity Validator #Data integrity validator is designed to catch the packet corruption issues in the network. We have a basic check that verifies that the probe output is made up purely of a pattern repeated many times over.
`}),e.add({id:11,href:"/docs/how-to/percentiles/",title:"Percentiles, Histograms, and Distributions",description:"Percentiles give you a deeper insight into how your system is behaving. For example, if your application\u0026rsquo;s response latency is very low 94 times out 100 but very high for the remaining 6 times, your average latency will still be low but it won\u0026rsquo;t be a great experience for your users. In other words, this is the case where your 95th percentile latency is high, even though your average and median (50th-%ile) latency is very low.",content:`Percentiles give you a deeper insight into how your system is behaving. For example, if your application\u0026rsquo;s response latency is very low 94 times out 100 but very high for the remaining 6 times, your average latency will still be low but it won\u0026rsquo;t be a great experience for your users. In other words, this is the case where your 95th percentile latency is high, even though your average and median (50th-%ile) latency is very low.
A typical way to measure percentiles from continuous monitoring data, which you may have to aggregate across various sources, is to use histograms (also called, distributions). In a histogram, you assign the incoming data points (samples) to pre-defined buckets. Each data point increases the count for the bucket that it falls into; data point itself is discarded after that. You can take a look at the bucket counts at any point of time and get an estimate of the percentiles. Histograms make it easy to aggregate data across multiple entities, for example, from probes running on multiple machines.
Following diagram shows distribution of latencies into 9 equal sized histogram buckets:
(Above diagram shows histogram for the following samples: 5.1, 6.2, 9.0, 12.1, 8.3, 9.7, 9.4, 10.3, 14.1, 11.2, 16.6, 9.9, 10.6, 14.1, 0.9, 7.1, 17.7)
Histograms in Cloudprober (Distributions) #Cloudprober uses a metric type called \u0026lsquo;distribution\u0026rsquo; to create and export histograms. Cloudprober supports creating distributions for probe latencies, and for metrics generated from external probe payloads. To create distributions, you have to specify how the data should be bucketed \u0026ndash; you can either explicitly specify all bucket bounds, or use exponential buckets type which generates bucket bounds from only a few variables.
Here is an example of using explicit buckets for latencies:
probe { name: \u0026quot;...\u0026quot; type: HTTP targets { host_names: \u0026quot;...\u0026quot; } latency_unit: \u0026quot;ms\u0026quot; latency_distribution { explicit_buckets: \u0026quot;0.01,0.1,0.15,0.2,0.25,0.35,0.5,0.75,1.0,1.5,2.0,3.0,4.0,5.0,10.0,15.0,20.0\u0026quot; } } Configuring distributions #As seen in the example above, for latencies you configure distribution at the probe level by adding a field called latency_distribution. Without this field, cloudprober exports only cumulative latencies. To create distributions from an external probe\u0026rsquo;s data, take a look at the external probe\u0026rsquo;s documentation.
Format for the distribution field is in turn defined in dist.proto.
// Dist defines a Distribution data type. message Dist { oneof buckets { // Comma-separated list of lower bounds, where each lower bound is a float // value. Example: 0.5,1,2,4,8. string explicit_buckets = 1; // Exponentially growing buckets ExponentialBuckets exponential_buckets = 2; } } // ExponentialBucket defines a set of num_buckets+2 buckets: // bucket[0] covers (âˆ’Inf, 0) // bucket[1] covers [0, scale_factor) // bucket[2] covers [scale_factor, scale_factor*base) // ... // bucket[i] covers [scale_factor*base^(iâˆ’2), scale_factor*base^(iâˆ’1)) // ... // bucket[num_buckets+1] covers [scale_factor*base^(num_bucketsâˆ’1), +Inf) // Note: Base must be at least 1.01. message ExponentialBuckets { optional float scale_factor = 1 [default = 1.0]; optional float base = 2 [default = 2]; optional uint32 num_buckets = 3 [default = 20]; } Percentiles and Heatmap #Now that we\u0026rsquo;ve configured cloudprober to generate distributions, how do we make use of this new information. This depends on the monitoring system (prometheus, stackdriver, postgres, etc) you\u0026rsquo;re exporting your data to.
Both prometheus and stackdriver support computing and plotting percentiles from the distributions data. Stackdriver can natively create heatmaps from distributions while for prometheus you need to use grafana to create heatmaps.
Stackdriver (Google Cloud Monitoring) #Stackdriver automatically shows percentile aggregator for distribution metrics in metrics explorer (example). You can also use Stackdriver MQL to create percentiles (see stackdriver documentation for other usages of MQL for cloudprober metrics):
fetch gce_instance | metric 'custom.googleapis.com/cloudprober/http/google_homepage/latency' | filter (resource.zone == 'us-central1-a') | align delta(1m) | every 1m | group_by [resource.zone], [value_latency_percentile: percentile(value.latency, 95)] Stackdriver has detailed documentation on charting distributions.
Prometheus #Cloudprober surfaces distributions to prometheus as prometheus metric type histogram. Here is an example of prometheus metrics page created by cloudprober:
# TYPE latency histogram latency_sum{ptype=\u0026quot;http\u0026quot;,probe=\u0026quot;my_probe\u0026quot;,dst=\u0026quot;hostA\u0026quot;} 77557.14022499947 1607766316442 latency_count{ptype=\u0026quot;http\u0026quot;,probe=\u0026quot;my_probe\u0026quot;,dst=\u0026quot;hostA\u0026quot;} 172150 1607766316442 latency_bucket{ptype=\u0026quot;http\u0026quot;,probe=\u0026quot;my_probe\u0026quot;,dst=\u0026quot;hostA\u0026quot;,le=\u0026quot;0.01\u0026quot;} 0 1607766316442 latency_bucket{ptype=\u0026quot;http\u0026quot;,probe=\u0026quot;my_probe\u0026quot;,dst=\u0026quot;hostA\u0026quot;,le=\u0026quot;0.1\u0026quot;} 0 1607766316442 ... ... latency_bucket{ptype=\u0026quot;http\u0026quot;,probe=\u0026quot;my_probe\u0026quot;,dst=\u0026quot;hostA\u0026quot;,le=\u0026quot;75\u0026quot;} 172150 1607766316442 latency_bucket{ptype=\u0026quot;http\u0026quot;,probe=\u0026quot;my_probe\u0026quot;,dst=\u0026quot;hostA\u0026quot;,le=\u0026quot;100\u0026quot;} 172150 1607766316442 latency_bucket{ptype=\u0026quot;http\u0026quot;,probe=\u0026quot;my_probe\u0026quot;,dst=\u0026quot;hostA\u0026quot;,le=\u0026quot;+Inf\u0026quot;} 172150 1607766316442 Fortunately there is already a plenty of good documentation on how to make use of histograms in prometheus and grafana:
Grafana blog on how to visualize prometheus histograms in grafana. Prometheus documentation on histrograms. More Resources #The Problem with Percentiles â€“ Aggregation brings Aggravation. Why percentiles don\u0026rsquo;t work the way you think. `}),e.add({id:12,href:"/docs/how-to/targets/",title:"Targets",description:`Cloudprober probes usually run against some targets1 to check those targets' status, such as an HTTP probe to your APIs servers, or PING/TCP probes to a third-party provider to verify network connectivity to them. Each probe can have multiple targets. If a probe has multiple targets, Cloudprober runs parallel probes for each target. This page further explains how targets work in Cloudprober.
Dynamically Discovered Targets #One of the core features of Cloudprober is the automatic and continuous discovery of targets.`,content:`Cloudprober probes usually run against some targets1 to check those targets' status, such as an HTTP probe to your APIs servers, or PING/TCP probes to a third-party provider to verify network connectivity to them. Each probe can have multiple targets. If a probe has multiple targets, Cloudprober runs parallel probes for each target. This page further explains how targets work in Cloudprober.
Dynamically Discovered Targets #One of the core features of Cloudprober is the automatic and continuous discovery of targets. This feature is especially important for the dynamic environments that today\u0026rsquo;s cloud based deployments make possible. For example in a kubernetes cluster the number of pods and their IPs can change on the fly, either in response to replica count changes or node failures. Automated targets discovery makes sure that we don\u0026rsquo;t have to reconfigure Cloudprober in response to such events.
Targets Configuration #Cloudprober provides multiple ways to configure targets for a probe.
Static targets #Static targets are the easiest and most straight-forward to configure:
probe { ... targets { host_names: \u0026quot;www.google.com,www.yahoo.com,cloudprober:9313\u0026quot; } .. } In the above config, probe will run against 3 hosts in parallel: www.google.com, www.yahoo.com, and cloudprober:9313 (yes, you can specify ports here for port-aware probes).
You can specify more detailed targets using the endpoint field. Using endpoints, you can even specify the URL directly in target definition; this method is particularly useful if you want to run an HTTP probe for multiple similar targets.
probe { type: HTTP ... targets { endpoint { # This will probe https://web.example.com/url1, target will show up as # \u0026quot;frontend_main\u0026quot; in metrics. name: \u0026quot;frontend_main\u0026quot; url: \u0026quot;https://web.example.com/url1\u0026quot; } endpoint { # This will probe http://cms.example.com, target will show up as # \u0026quot;cms.example.com\u0026quot; in metrics. name: \u0026quot;cms.example.com\u0026quot; } } .. } File based targets #You can define your targets in a file and refer to them in Cloudprober through that file. You can configure cloudprober to reload the targets file at a regular interval to incorporate any changes to the targets.
Example configuration:
targets { file_targets { file_path: \u0026quot;/var/run/cloudprober/vips.json\u0026quot; re_eval_sec: 30 # check file for changes every 30s. } } In the targets file, resources should be specified in a specific format. Here is an example of targets in JSON format:
{ \u0026quot;resource\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;switch-xx-1\u0026quot;, \u0026quot;ip\u0026quot;: \u0026quot;10.1.1.1\u0026quot;, \u0026quot;port\u0026quot;: 8080, \u0026quot;labels\u0026quot;: { \u0026quot;device_type\u0026quot;: \u0026quot;switch\u0026quot;, \u0026quot;cluster\u0026quot;: \u0026quot;xx\u0026quot; } }, { \u0026quot;name\u0026quot;: \u0026quot;switch-xx-2\u0026quot;, \u0026quot;ip\u0026quot;: \u0026quot;10.1.1.2\u0026quot;, \u0026quot;port\u0026quot;: 8081, \u0026quot;labels\u0026quot;: { \u0026quot;cluster\u0026quot;: \u0026quot;xx\u0026quot; } } ] } (You can also define targets in the textproto format: example. Full example with cloudprober.cfg: file_based_targets)
Even if you don\u0026rsquo;t intend to use the auto-reload feature of the file targets, they can still be quite useful over static targets as they allow you to specify additional details for targets. For example, specifying target\u0026rsquo;s IP address in the example above lets you tackle the case where you want to specify target\u0026rsquo;s name, let\u0026rsquo;s say for better identification or for HTTP requests to work, but don\u0026rsquo;t want to rely on DNS for resolving its IP address.
K8s targets #K8s targets are explained at Kubernetes Targets.
GCP targets #Since Cloudprober started at GCP, it\u0026rsquo;s no surprise that Cloudprober has great support for GCP targets. Cloudprober supports the following GCP resources:
GCE Instances Forwarding Rules (regional and global) Cloud pub/sub (list of hostnames over cloud pub/sub) TODO: Add more details on GCP targets.
Probe configuration through target fields #Field Or Label Probe Type Configuration port Port aware probes (HTTP, DNS, TCP, UDP, etc) If a target has an associated port, for example, a Kubernetes endpoint, it will automatically be used for probing unless a port has been explicitly configured in the probe. __cp_path__ or relative_url HTTP If an explicit relative URL is not set in the config, HTTP probe will use target\u0026rsquo;s __cp_path__ and realtive_url labels if set. __cp_host__ or fqdn HTTP HTTP probe will use target\u0026rsquo;s __cp_host__ and fqdn labels as URL-host and Host header if set and if Host header has not been configured explicitly. __cp_scheme__ HTTP HTTP probe will use target\u0026rsquo;s __cp_scheme__ label as HTTP URL scheme (http or https) header if available and if scheme has not been configured explicitly. Metrics #Target name: All metrics generated by Cloudprober have a dst label which is set to the target name. Target labels: See additional labels for how resource labels can be used to set additional labels on the metrics. Scaling targets discovery and other features #If you run a lot of Cloudprober instances with targets discovery, you may end up overwhelming the API servers, or running out of your API quota in case of Cloud resources. To avoid that, Cloudprober allows centralizing the targets discovery through the Resource Discovery Service (RDS) mechanism. See Resource Discovery Service for more details on that.
Other salient features of the cloudprober\u0026rsquo;s targets discovery:
Continuous discovery. We don\u0026rsquo;t just discover targets in the beginning, but keep refreshing them at a regular interval. Protection against the upstream provider failures. If refreshing of the targets fails during one of the refresh cycles, we continue using the existing set of targets. There are some cases where there is no explicit target, for example, you may run a probe to measure your CI system\u0026rsquo;s performance, or run a complex probe that touches many endpoints.\u0026#160;\u0026#x21a9;\u0026#xfe0e;
`}),e.add({id:13,href:"/docs/how-to/built-in-servers/",title:"Built-in Servers",description:`Cloudprober includes built-in servers that can be enabled through configuration. These servers can act as targets for other probes. For example, you can run two Cloudprober instances on different machines, with one instance\u0026rsquo;s servers acting as targets and the other instance probing those targets. These servers are useful for monitoring the underlying infrastructure, such as underlying network or load balancers. Cloudprober (probes) ===(Network)===\u003e Cloudprober (servers) HTTP Server #server { type: HTTP http_server { port: 8080 } } This creates an HTTP server that responds on the port 8080.`,content:`Cloudprober includes built-in servers that can be enabled through configuration. These servers can act as targets for other probes. For example, you can run two Cloudprober instances on different machines, with one instance\u0026rsquo;s servers acting as targets and the other instance probing those targets.
These servers are useful for monitoring the underlying infrastructure, such as underlying network or load balancers.
Cloudprober (probes) ===(Network)===\u003e Cloudprober (servers) HTTP Server #server { type: HTTP http_server { port: 8080 } } This creates an HTTP server that responds on the port 8080. This HTTP server supports the following two endpoints by default:
/healthcheck - returns \u0026lsquo;OK\u0026rsquo; if instance is not in the lameduck mode. /lameduck - returns the lameduck status (true/false). Lameduck mode is a mode in which a server is still running but is signaling that it is about to go down for maintenance so new requests should not be sent to it. This is typically used with load balancers to take out a backend for maintenance without returning any actual errors.
TODO(manugarg): Document how a Cloudprober can be put in the lameduck mode.
Data Handlers #You can also add custom data handlers to the above HTTP server:
server { type: HTTP http_server { port: 8080 pattern_data_handler { response_size: 1024 } pattern_data_handler { response_size: 4 pattern: \u0026quot;four\u0026quot; } } } Above configuration adds the following two URLs to the HTTP server:
/data_1024 which responds with 1024 bytes of cloudprobercloudprober...(repeated). /data_4 which responds with four. These endpoints are useful to monitor other aspects of the underlying network like MTU, and consistency (make sure data is not getting corrupted), etc.
See this for all HTTP server configuration options.
UDP #UDP server can either echo packets back or completely ignore them. In echo mode, you can use it along with the UDP probe type.
server { type: UDP udp_server { port: 85 type: ECHO } } server { type: UDP udp_server { port: 90 type: DISCARD } } See ServerConf for all UDP server configuration options.
GRPC #See ServerConf for all GRPC server configuration options.
`}),e.add({id:14,href:"/docs/surfacers/cloudwatch/",title:"Cloudwatch (AWS)",description:`Cloudprober can natively export metrics to AWS Cloudwatch using the cloudwatch surfacer. Adding the cloudwatch surfacer to cloudprover is as simple as adding the following stanza to the config:
surfacer { type: CLOUDWATCH } Authentication #The cloudwatch surfacer uses the AWS Go SDK, and supports the default credential chain:
Environment variables. Shared credentials file. If your application uses an ECS task definition or RunTask API operation, IAM role for tasks.`,content:`Cloudprober can natively export metrics to AWS Cloudwatch using the cloudwatch surfacer. Adding the cloudwatch surfacer to cloudprover is as simple as adding the following stanza to the config:
surfacer { type: CLOUDWATCH } Authentication #The cloudwatch surfacer uses the AWS Go SDK, and supports the default credential chain:
Environment variables. Shared credentials file. If your application uses an ECS task definition or RunTask API operation, IAM role for tasks. If your application is running on an Amazon EC2 instance, IAM role for Amazon EC2. Cloudwatch Region #The list below is the order of precedence that will be used to determine the AWS region that Cloudprober will publish metrics to.
Region configuration EC2 metadata. AWS_REGION environment variable. AWS_DEFAULT_REGION environment variable, if AWS_SDK_LOAD_CONFIG is set (See AWS package documentation for more details). Authorization #In order to permit Cloudprober to publish metric data to cloudwatch, ensure the profile being used for authentication has the following permissions, where the \u0026ldquo;cloudwatch:namespace\u0026rdquo; is the metric namespace used by Cloudprober.
If the default metric namespace is changed, also change the condition in the IAM policy below to match the same value.
{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Condition\u0026quot;: { \u0026quot;StringEqualsIgnoreCase\u0026quot;: { \u0026quot;cloudwatch:namespace\u0026quot;: \u0026quot;cloudprober\u0026quot; } }, \u0026quot;Action\u0026quot;: [ \u0026quot;cloudwatch:PutMetricData\u0026quot; ], \u0026quot;Resource\u0026quot;: [ \u0026quot;*\u0026quot; ], \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Sid\u0026quot;: \u0026quot;PutMetrics\u0026quot; } ] } Metric Namespace #The metric namespace used to publish metrics to by default is set to cloudprober. This can be changed by expanding the surfacer configuration:
surfacer { type: CLOUDWATCH cloudwatch_surfacer { namespace: \u0026quot;/cloudprober/website/probes\u0026quot; } } Note: If the namespace is modified, also modify the IAM policy condition for the namespace PutMetricData call.
Configuration Options #The full list of configuration options for the cloudwatch surfacer is:
// The cloudwatch metric namespace optional string namespace = 1 [default = \u0026quot;cloudprober\u0026quot;]; // The cloudwatch resolution value, lowering this below 60 will incur // additional charges as the metrics will be charged at a high resolution rate. optional int64 resolution = 2 [default=60]; // The AWS Region, used to create a CloudWatch session. // The order of fallback for evaluating the AWS Region: // 1. This config value. // 2. EC2 metadata endpoint, via cloudprober sysvars. // 3. AWS_REGION environment value. // 4. AWS_DEFAULT_REGION environment value, if AWS_SDK_LOAD_CONFIG is set. // https://docs.aws.amazon.com/sdk-for-go/api/aws/session/ optional string region = 3; // The maximum number of metrics that will be published at one // time. Metrics will be stored locally in a cache until this // limit is reached. 1000 is the maximum number of metrics // supported by the Cloudwatch PutMetricData API. // Metrics will be published when the timer expires, or the buffer is // full, whichever happens first. optional int32 metrics_batch_size = 4 [default = 1000]; // The maximum amount of time to hold metrics in the buffer (above). // Metrics will be published when the timer expires, or the buffer is // full, whichever happens first. optional int32 batch_timer_sec = 5 [default = 30]; (All config options: SurfacerConf
Calculating the metric delta with Cloudwatch Metric Maths #The metrics produced by Cloudprober are cumulative. Most services producing metrics into cloudwatch produce snapshot data whereby the metrics are recorded for a specific point in time.
In order to achieve a similar effect here, the Cloudwatch Metric Maths RATE and PERIOD functions can be used to determine the delta values.
RATE(m1) * PERIOD(m1) Whereby m1 is the metric id for the Cloudprober metrics, for example:
namespace: cloudprober metric name: latency dst: google.com ptype: http probe: probe name `}),e.add({id:15,href:"/docs/config/",title:"Configuration",description:" ",content:" "}),e.add({id:16,href:"/docs/config/main/",title:"Configuration",description:" ",content:" "}),e.add({id:17,href:"/docs/surfacers/overview/",title:"Exporting Metrics (Surfacers)",description:"One of the biggest strengths of cloudprober is that it can export data to multiple monitoring systems, even simultaneously, just based on simple configuration. Cloudprober does that using a built-in mechanism, called surfacers. Each surfacer type implements interface for a specific monitoring system, for example, cloudwatch surfacer publishes data to AWS Cloudwatch. You can configure multiple surfacers at the same time. If you don\u0026rsquo;t specify any surfacer, prometheus and file surfacers are enabled automatically.",content:`One of the biggest strengths of cloudprober is that it can export data to multiple monitoring systems, even simultaneously, just based on simple configuration. Cloudprober does that using a built-in mechanism, called surfacers. Each surfacer type implements interface for a specific monitoring system, for example, cloudwatch surfacer publishes data to AWS Cloudwatch. You can configure multiple surfacers at the same time. If you don\u0026rsquo;t specify any surfacer, prometheus and file surfacers are enabled automatically.
Cloudprober currently supports following surfacer types:
Prometheus (config) OpenTelemetry (OTEL) (config) [New in v0.13.2] Stackdriver (Google Cloud Monitoring) Google Pub/Sub (config) Postgres (config) File (config) Cloudwatch (AWS Cloud Monitoring) Overall surfacers config.
It\u0026rsquo;s easy to add more surfacers without having to understand the internals of cloudprober. You only need to implement the Surfacer interface.
Configuration #Adding surfacers to cloudprober is as easy as adding \u0026ldquo;surfacer\u0026rdquo; config stanzas to your config, like the following:
# Enable prometheus and stackdriver surfacers. # Make probe metrics available at the URL :\u0026lt;cloudprober_port\u0026gt;/metrics, for # scraping by prometheus. surfacer { type: PROMETHEUS prometheus_surfacer { # Following option adds a prefix to exported metrics, for example, # \u0026quot;total\u0026quot; metric is exported as \u0026quot;cloudprober_total\u0026quot;. metrics_prefix: \u0026quot;cloudprober_\u0026quot; } } # Stackdriver (Google Cloud Monitoring) surfacer_ No other configuration # is necessary if running on GCP. surfacer { type: STACKDRIVER } Filtering Metrics #You can control which metrics are published to a surfacer using the filtering mechanisms. For example, you may want to publish only specific metrics to AWS Cloudwatch to save on the costs.
Filtering by Label #To filter metrics by labels, use one of the following options in the surfacers config:
allow_metrics_with_label (allowMetricsWithLabel in yaml) ignore_metrics_with_label (ignoreMetricsWithLabel in yaml) Note: ignore_metrics_with_label takes precedence over allow_metrics_with_label.
For example, to ignore all sysvar metrics:
surfacer { type: PROMETHEUS ignore_metrics_with_label { key: \u0026quot;probe\u0026quot;, value: \u0026quot;sysvars\u0026quot;, } } Or to only allow metrics from http probes:
surfacer { type: PROMETHEUS allow_metrics_with_label { key: \u0026quot;ptype\u0026quot;, value: \u0026quot;http\u0026quot;, } } Filtering by Metric Name #To filter metrics by name, use one of the following options in the surfacers config:
allow_metrics_with_name (allowMetricsWithName in yaml) ignore_metrics_with_name (ignoreMetricsWithName in yaml) Note: ignore_metrics_with_name takes precedence over allow_metrics_with_name.
To filter out all validation_failure metrics by name:
surfacer { type: PROMETHEUS ignore_metrics_with_name: \u0026quot;validation_failure\u0026quot; } Modifying Metrics #You can configure surfacers to modify the metrics before they are sent to the backend monitoring system:
add_failure_metric: Export failure count along with the default total and success metrics:
surfacer { type: ... add_failure_metric = true .. } NOTE: This option is now enabled by default for all surfacers, except for FILE and PUBSUB surfacers.
export_as_gauge: Export gauge metrics instead of cumulative. Cloudprober exports cumulative metrics (sum of values so far) by default, but you can configure it to export gauge metrics instead. Gauge metrics make point-in-time calculations easier (e.g. you can just divide latency by success to get the average latency), but we lose the historical information if metrics are not received for a few intervals for some reason.
surfacer { type: ... export_as_gauge = true .. } Additional labels #See additional labels for how you can add additional labels to cloudprober metrics.
Adding your own metrics #For external probes, Cloudprober also allows external programs to provide additional metrics. See External Probe for more details.
`}),e.add({id:18,href:"/docs/how-to/extensions/",title:"Extending Cloudprober",description:"Cloudprober allows you to extend it across \u0026ldquo;probe\u0026rdquo; and \u0026ldquo;target\u0026rdquo; dimensions, that is, you can add new probe and target types to it without having to fork the entire codebase. Note that to extend cloudprober in this way, you will have to maintain your own cloudprober binary (which is mostly a wrapper around the \u0026ldquo;cloudprober package\u0026rdquo;), but you\u0026rsquo;ll be able to use rest of the cloudprober code from the common location.",content:`Cloudprober allows you to extend it across \u0026ldquo;probe\u0026rdquo; and \u0026ldquo;target\u0026rdquo; dimensions, that is, you can add new probe and target types to it without having to fork the entire codebase. Note that to extend cloudprober in this way, you will have to maintain your own cloudprober binary (which is mostly a wrapper around the \u0026ldquo;cloudprober package\u0026rdquo;), but you\u0026rsquo;ll be able to use rest of the cloudprober code from the common location.
Sample probe type #To demonstrate how it works, let\u0026rsquo;s add a new probe-type to Cloudprober. We\u0026rsquo;ll take the sample redis probe that we added in the external probe how-to, and convert it into a probe type that one can easily re-use. Let\u0026rsquo;s say that this probe-type provides a way to test redis server functionality and it takes the following options - operation (GET vs SET vs DELETE), key, value. This probe\u0026rsquo;s configuration looks like this:
probe { name: \u0026quot;redis_set\u0026quot; type: EXTENSION targets { host_names: \u0026quot;localhost:6379\u0026quot; } redis_probe { op: \u0026quot;set\u0026quot; key: \u0026quot;testkey\u0026quot; value: \u0026quot;testval\u0026quot; } } To make cloudprober understand this config, we\u0026rsquo;ll have to do a few things:
Define the probe config in a protobuf (.proto) file and mark it as an extension of the overall config.
Implement the probe type, possibly as a Go package, even though it can be embedded directly into the top-level binary.
Create a new cloudprober binary that includes the new probe type package.
Protobuf for the new probe type #Let\u0026rsquo;s create a new directory for our code: \$GOPATH/src/myprober.
// File: \$GOPATH/src/myprober/myprobe/myprobe.proto syntax = \u0026quot;proto2\u0026quot;; import \u0026quot;github.com/cloudprober/cloudprober/probes/proto/config.proto\u0026quot;; package myprober; message ProbeConf { // Redis operation required string op = 1; // Key and value for the redis operation required string key = 2; optional string value = 3; } extend cloudprober.probes.ProbeDef { optional ProbeConf redis_probe = 200; } Let\u0026rsquo;s generate Go code for this protobuf:
# From the myprober directory protoc --go_out=.,import_path=myprobe:. --proto_path=\$GOPATH/src:. myprobe/*.proto \$ ls myprobe/ myprobe.pb.go myprobe.proto Implement the probe type #Now let\u0026rsquo;s implement our probe type. Our probe type should implement the probes.Probe interface.
package myprobe // Probe holds aggregate information about all probe runs, per-target. type Probe struct { name string c *configpb.ProbeConf targets []string opts *options.Options ... } // Init initializes the probe with the given params. func (p *Probe) Init(name string, opts *options.Options) error { c, ok := opts.ProbeConf.(*ProbeConf) if !ok { return fmt.Errorf(\u0026quot;not a my probe config\u0026quot;) } // initialize p fields, p.name = name, etc. } // Start runs the probe indefinitely, at the configured interval. func (p *Probe) Start(ctx context.Context, dataChan chan *metrics.EventMetrics) { probeTicker := time.NewTicker(p.opts.Interval) for { select { case \u0026lt;-ctx.Done(): probeTicker.Stop() return case \u0026lt;-probeTicker.C: for _, em := range p.res { dataChan \u0026lt;- em } p.targets = p.opts.Targets.List() ... probeCtx, cancelFunc := context.WithDeadline(ctx, time.Now().Add(p.opts.Timeout)) p.runProbe(probeCtx) cancelFunc() } } } // runProbe runs probe for all targets and update EventMetrics. func (p *Probe) runProbe(ctx context.Context) { p.targets = p.opts.Targets.List() var wg sync.WaitGroup for _, target := range p.targets { wg.Add(1) go func(target string, em *metrics.EventMetrics) { defer wg.Done() em.Metric(\u0026quot;total\u0026quot;).(*metrics.Int).Inc() start := time.Now() err := p.runProbeForTarget(ctx, target) // run probe just for a single target if err != nil { p.l.Errorf(err.Error()) return } em.Metric(\u0026quot;success\u0026quot;).(*metrics.Int).Inc() em.Metric(\u0026quot;latency\u0026quot;).(metrics.LatencyValue).AddFloat64( time.Since(start). Seconds() / p.opts.LatencyUnit.Seconds()) }(target, p.res[target]) } wg.Wait() } Full example in examples/extensions/myprober/myprobe/myprobe.go.
This probe type sets or gets (depending on the configuration) a key-valye in redis and records success and time taken (latency) if operation is successful.
Implement a cloudprober binary that includes support for our probe #package main ... func main() { flag.Parse() // Register our probe type probes.RegisterProbeType(int(myprobe.E_RedisProbe.TypeDescriptor().Number()), func() probes.Probe { return \u0026amp;myprobe.Probe{} }) err := cloudprober.Init() // getConfig not shown here. if err != nil { glog.Exitf(\u0026quot;Error initializing cloudprober. Err: %v\u0026quot;, err) } // web.Init sets up web UI for cloudprober. web.Init() cloudprober.Start(context.Background()) // Wait forever select {} } Full example in examples/extensions/myprober/myprober.go.
Let\u0026rsquo;s write a test config that uses the newly defined probe type:
probe { name: \u0026quot;redis_set\u0026quot; type: EXTENSION interval_msec: 10000 timeout_msec: 5000 targets { host_names: \u0026quot;localhost:6379\u0026quot; } [myprober.redis_probe] { op: \u0026quot;set\u0026quot; key: \u0026quot;testkey\u0026quot; value: \u0026quot;testval\u0026quot; } } Full example in examples/extensions/myprober/myprober.cfg.
Let\u0026rsquo;s compile our prober and run it with the above config:
go run ./myprober.go --config_file=myprober.cfg you should see an output like the following:
cloudprober 1540848577649139842 1540848587 labels=ptype=redis,probe=redis_set,dst=localhost:6379 total=31 success=31 latency=70579.823 cloudprober 1540848577649139843 1540848887 labels=ptype=sysvars,probe=sysvars hostname=\u0026quot;manugarg-macbookpro5.roam.corp.google.com\u0026quot; start_timestamp=\u0026quot;1540848577\u0026quot; cloudprober 1540848577649139844 1540848887 labels=ptype=sysvars,probe=sysvars uptime_msec=310007.784 gc_time_msec=0.000 mallocs=14504 frees=826 cloudprober 1540848577649139845 1540848887 labels=ptype=sysvars,probe=sysvars goroutines=12 mem_stats_sys_bytes=7211256 cloudprober 1540848577649139846 1540848587 labels=ptype=redis,probe=redis_set,dst=localhost:6379 total=32 success=32 latency=72587.981 cloudprober 1540848577649139847 1540848897 labels=ptype=sysvars,probe=sysvars hostname=\u0026quot;manugarg-macbookpro5.roam.corp.google.com\u0026quot; start_timestamp=\u0026quot;1540848577\u0026quot; cloudprober 1540848577649139848 1540848897 labels=ptype=sysvars,probe=sysvars uptime_msec=320006.541 gc_time_msec=0.000 mallocs=14731 frees=844 cloudprober 1540848577649139849 1540848897 labels=ptype=sysvars,probe=sysvars goroutines=12 mem_stats_sys_bytes=7211256 You can import this data in prometheus following the process outlined at: Running Prometheus.
Conclusion #The article shows how to add a new probe type to cloudprober. Extending cloudprober allows you to implement new probe types that may make sense for your organization, but not for the open source community. You have to implement the logic for the probe type, but other cloudprober features work as it is \u0026ndash; targets, metrics (e.g. latency distribution if you configure it), surfacers - data can be multiple systems simultaneously, etc.
`}),e.add({id:19,href:"/docs/overview/getting-started/",title:"Getting Started",description:`Installation #If you\u0026rsquo;ve Go installed, you can install cloudprober from source using the following command:
go install github.com/cloudprober/cloudprober/cmd/cloudprober@latest Other Methods: #Method Instructions Platform Brew brew install cloudprober MacOS, Linux Docker Image docker run ghcr.io/cloudprober/cloudprober (other docker versions) Docker Helm chart See here for instructions Kubernetes Pre-built binaries Download from the releases page. MacOS, Linux, Windows See this page for how to access unreleased binaries.
Configuration #Without any config, cloudprober will run only the \u0026ldquo;sysvars\u0026rdquo; module (no probes) and write metrics to stdout in cloudprober\u0026rsquo;s line protocol format (to be documented).`,content:`Installation #If you\u0026rsquo;ve Go installed, you can install cloudprober from source using the following command:
go install github.com/cloudprober/cloudprober/cmd/cloudprober@latest Other Methods: #Method Instructions Platform Brew brew install cloudprober MacOS, Linux Docker Image docker run ghcr.io/cloudprober/cloudprober (other docker versions) Docker Helm chart See here for instructions Kubernetes Pre-built binaries Download from the releases page. MacOS, Linux, Windows See this page for how to access unreleased binaries.
Configuration #Without any config, cloudprober will run only the \u0026ldquo;sysvars\u0026rdquo; module (no probes) and write metrics to stdout in cloudprober\u0026rsquo;s line protocol format (to be documented). It will also start a Prometheus exporter at: http://localhost:9313 (you can change the default port through the environment variable CLOUDPROBER_PORT and the default listening address through the environment variable CLOUDPROBER_HOST).
Since sysvars variables are not very interesting themselves, lets add a simple config that probes Google\u0026rsquo;s homepage:
# Write config to a file in /tmp cat \u0026gt; /tmp/cloudprober.cfg \u0026lt;\u0026lt;EOF probe { name: \u0026quot;google_homepage\u0026quot; type: HTTP targets { host_names: \u0026quot;www.google.com\u0026quot; } interval_msec: 5000 # 5s timeout_msec: 1000 # 1s } EOF This config adds an HTTP probe that accesses the homepage of the target \u0026ldquo;www.google.com\u0026rdquo; every 5s with a timeout of 1s. Cloudprober configuration is specified in the text protobuf format, with config schema described by the proto file: config.proto.
Assuming that you saved this file at /tmp/cloudprober.cfg (following the command above), you can have cloudprober use this config file using the following command line:
./cloudprober --config_file /tmp/cloudprober.cfg You can have the standard docker image use this config using the following command:
docker run -v /tmp/cloudprober.cfg:/etc/cloudprober.cfg \\ ghcr.io/cloudprober/cloudprober Note: While running on GCE, cloudprober config can also be provided through a custom metadata attribute: cloudprober_config.
Verification #One quick way to verify that cloudprober got the correct config is to access the URL http://localhost:9313/config (through cURL or in browser). It returns the config that cloudprober is using. You can also look at its current status at the URL (replace localhost by the actual hostname if not running locally): http://localhost:9313/status.
You should be able to see the generated metrics at http://localhost:9313/metrics (prometheus format) and the stdout (cloudprober format):
cloudprober 15.. 1500590520 labels=ptype=http,probe=google-http,dst=.. total=17 success=17 latency=180835 cloudprober 15.. 1500590530 labels=ptype=sysvars,probe=sysvars hostname=\u0026quot;manugarg-ws\u0026quot; uptime=100 cloudprober 15.. 1500590530 labels=ptype=http,probe=google-http,dst=.. total=19 success=19 latency=211644 This information is good for debugging monitoring issues, but to really make sense of this data, you\u0026rsquo;ll need to feed this data to another monitoring system like Prometheus or StackDriver (see Surfacers for more details). Lets set up a Prometheus and Grafana stack to make pretty graphs for us.
Running Prometheus #Download prometheus binary from its release page. You can use a config like the following to scrape a cloudprober instance running on the same host.
# Write config to a file in /tmp cat \u0026gt; /tmp/prometheus.yml \u0026lt;\u0026lt;EOF scrape_configs: - job_name: 'cloudprober' scrape_interval: 10s static_configs: - targets: ['localhost:9313'] EOF # Start prometheus: ./prometheus --config.file=/tmp/prometheus.yml Prometheus provides a web interface at http://localhost:9090. You can explore probe metrics and build useful graphs through this interface. All probes in cloudprober export at least 3 counters:
total: Total number of probes. success: Number of successful probes. Difference between total and success indicates failures. latency: Total (cumulative) probe latency. Using these counters, probe failure ratio and average latency can be calculated as:
failure_ratio = (rate(total) - rate(success)) / rate(total) avg_latency = rate(latency) / rate(success) Assuming that prometheus is running at localhost:9090, graphs depicting failure ratio and latency over time can be accessed in prometheus at: this url . Even though prometheus provides a graphing interface, Grafana provides much richer interface and has excellent support for prometheus.
Grafana #Grafana is a popular tool for building monitoring dashboards. Grafana has native support for prometheus and thanks to the excellent support for prometheus in Cloudprober itself, it\u0026rsquo;s a breeze to build Grafana dashboards from Cloudprober\u0026rsquo;s probe results.
To get started with Grafana, follow the Grafana-Prometheus integration guide.
`}),e.add({id:20,href:"/docs/overview/cloudprober/",title:"Overview",description:`Cloudprober provides a reliable and easy-to-use solution to monitor the availability and performance of your systems. Employing an \u0026ldquo;active\u0026rdquo; monitoring approach, Cloudprober executes probes on or against these systems to verify their proper functioning.
For example, you could use Cloudprober to run a probe to verify that your users can access your website and your APIs, your microservices can talk to each other, your kubernetes clusters can schedule pods, your CI/CD pipelines are functioning as expected, or VPN connectivity with your partners is working as expected, and much more.`,content:` Cloudprober provides a reliable and easy-to-use solution to monitor the availability and performance of your systems. Employing an \u0026ldquo;active\u0026rdquo; monitoring approach, Cloudprober executes probes on or against these systems to verify their proper functioning.
For example, you could use Cloudprober to run a probe to verify that your users can access your website and your APIs, your microservices can talk to each other, your kubernetes clusters can schedule pods, your CI/CD pipelines are functioning as expected, or VPN connectivity with your partners is working as expected, and much more.
This kind of monitoring makes it possible to monitor your systems\u0026rsquo; interfaces regardless of the implementation, and helps you quickly pin down what\u0026rsquo;s broken in your systems.
Features #Out of the box, config based, integration with many popular monitoring systems:
Prometheus/Grafana DataDog PostgreSQL AWS CloudWatch StackDriver / Google Cloud Monitoring Multiple options for checks:
Efficient, highly scalable, built-in probes: HTTP, PING, TCP, DNS, gRPC, UDP. Run custom checks through the \u0026quot;external\u0026quot; probe type. Automated targets discovery to make Cloud deployments as painless as possible:
Kubernetes resources. GCP instances, forwarding rules, and pub/sub messages. File based targets. Deployment friendly:
Written entirely in Go, and compiles into a static binary. Deploy as a standalone binary, or through docker containers. Continuous, automated target discovery, to ensure that most infrastructure changes don\u0026rsquo;t require re-deployment. Low footprint. Cloudprober takes advantage of the Go\u0026rsquo;s concurrency paradigms, and makes most of the available processing power. Configurable metrics:
Configurable metrics labels, based on the resource labels. Latency histograms for percentile calculations. Extensible architecture. Cloudprober can be easily extended along most of the dimensions. Adding support for other Cloud targets, monitoring systems and even a new probe type, is straight-forward and fairly easy. Getting Started #Visit Getting Started page to get started with Cloudprober.
Feedback #We\u0026rsquo;d love to hear your feedback. If you\u0026rsquo;re using Cloudprober, would you please mind sharing how you use it by adding a comment here. It will be a great help in planning Cloudprober\u0026rsquo;s future progression.
Join Cloudprober Slack or Github discussions for questions and discussion about Cloudprober.
`}),e.add({id:21,href:"/docs/how-to/rds/",title:"Resource Discovery Service",description:`Note: This is an advanced topic. From a user\u0026rsquo;s perspective, it\u0026rsquo;s only useful if you want to scale targets discovery by centralizing it.Cloudprober internally defines and uses a protocol called resource discovery service (RDS1) for targets discovery2. It helps provide a consistent interface between the targets subsystem, actual resource discovery mechanisms, and probes subsystem. It also provides a way to move targets discovery into an independent process, which can be used to reduce the upstream API traffic.`,content:`Note: This is an advanced topic. From a user\u0026rsquo;s perspective, it\u0026rsquo;s only useful if you want to scale targets discovery by centralizing it.Cloudprober internally defines and uses a protocol called resource discovery service (RDS1) for targets discovery2. It helps provide a consistent interface between the targets subsystem, actual resource discovery mechanisms, and probes subsystem. It also provides a way to move targets discovery into an independent process, which can be used to reduce the upstream API traffic.
Protocol (rds_targets) #To understand the RDS protocol, let\u0026rsquo;s look at the rds_targets targets type. You can think of rds_targets as a configuration interface to the RDS service. When you configure rds_targets, you\u0026rsquo;re creating an RDS client that talks to an RDS backend that is either part of the same process (default) or available over gRPC (usefule for centralizing the upstream API calls).
Here are the RDS targets configuration options:
message RDSTargets { // RDS server options, for example: // rds_server_options { // server_address: \u0026quot;rds-server.xyz:9314\u0026quot; // oauth_config: { // ... // } // } // Default is to use the local server if any. optional rds.ClientConf.ServerOptions rds_server_options = 1; // Resource path specifies the resources to return. Resources paths have the // following format: // \u0026lt;resource_provider\u0026gt;://\u0026lt;resource_type\u0026gt;/\u0026lt;additional_params\u0026gt; // // Examples: // For GCE instances in projectA: \u0026quot;gcp://gce_instances/\u0026lt;projectA\u0026gt;\u0026quot; // Kubernetes Pods : \u0026quot;k8s://pods\u0026quot; optional string resource_path = 2; // Filters to filter resources by. Example: // filter { // key: \u0026quot;namespace\u0026quot; // value: \u0026quot;mynamesspace\u0026quot; // } // filter { // key: \u0026quot;labels.app\u0026quot; // value: \u0026quot;web-service\u0026quot; // } repeated rds.Filter filter = 3; // IP config to specify the IP address to pick for a resource. IPConfig // is defined here: // https://github.com/cloudprober/cloudprober/blob/master/rds/proto/rds.proto optional rds.IPConfig ip_config = 4; } Most options are explained in the comments for a quick reference. Here is the further explanation of some of these options:
rds_server_options #This field specifies how to connect to the RDS server: server address and security options (OAuth and TLS). If left unspecified, it connects to the local server if any (started through rds_server option). Next up it looks for the rds_server_options in global_targets_options.
resource_path #Resource path specifies the resources we are interested in. It consists of a resource provider, resource type and an optional relative path: \u0026lt;resource_provider\u0026gt;://\u0026lt;resource_type\u0026gt;/\u0026lt;optional_relative_path\u0026gt;
resource_provider: Resource provider is a generic concept within the RDS protocol but usually maps to the cloud provider. Cloudprober RDS server currently implements the Kubernetes (k8s) and GCP (gcp) resource providers. We plan to add more resource providers in future. resource_type: Available resource types depend on the providers, for example, for k8s provider supports the following resource types: pods, endpoints, and services. optional_relative_path: For most resource types you can specify resource name in the resource path itself, e.g. k8s://services/cloudprober. Alternatively, you can use filters to filter by name, resource, etc. filter #Filters are key-value strings that can be used to filter resources by various fields. Filters depend on the resource types, but most resources support filtering by name and labels.
# Return resources that start with \u0026quot;web\u0026quot; and have label \u0026quot;service:service-a\u0026quot; ... filter { key: \u0026quot;name\u0026quot; value: \u0026quot;^web.*\u0026quot; } filter { key: \u0026quot;labels.service\u0026quot; value: \u0026quot;service-a\u0026quot; } Filters supported by kubernetes resources: k8s filters. Filters supported by GCP: GCE Instances Forwarding Rules Pub/Sub Messages Running RDS Server #RDS server can either be run as an independent process, or it can be a part of the main prober process. Former mode is useful for large deployments where you may want to reduce the API upcall traffic (for example, to GCP). For example, if you run 1000+ prober processes, it will be much more economical from the API quota usage point of view to have a centralized RDS service with much fewer (2-3) instances instead of having each prober process make its own API calls.
RDS server can be added to a cloudprober process using the rds_server stanza. If you\u0026rsquo;re running RDS server in a remote process, you\u0026rsquo;ll have to enable gRPC server in that process (using grpc_port) so that other instances can access it remotely.
Here is an example RDS server configuration:
rds_server { # GCP provider to discover GCP resources. provider { gcp_config { # Projects to discover resources in. project: \u0026quot;test-project-1\u0026quot; project: \u0026quot;test-project-2\u0026quot; # Discover GCE instances in us-central1. gce_instances { zone_filter: \u0026quot;name = us-central1-*\u0026quot; re_eval_sec: 60 # How often to refresh, default is 300s. } # GCE forwarding rules. forwarding_rules {} } } # Kubernetes targets are further discussed at: # https://cloudprober.org/how-to/run-on-kubernetes/#kubernetes-targets provider { kubernetes_config { endpoints {} } } } For the remote RDS server setup, if accessing over external network, you can secure the underlying gRPC communication using TLS certificates.
Remote RDS Server Example #Cloudprober config:
probe { rds_targets { rds_server_options { server_address: \u0026quot;rds-service:9314\u0026quot; # mTLS configuration tls_config { ca_cert_file: \u0026quot;/vol/certs/server_ca.crt\u0026quot; # To verify the server tls_cert_file: \u0026quot;/vol/certs/client.crt\u0026quot; # Own cert to present to server tls_key_file: \u0026quot;/vol/certs/client.key\u0026quot; # Own cert's private key } } resource_path: \u0026quot;gcp://gce_instances\u0026quot; filter { key: \u0026quot;name\u0026quot; value: \u0026quot;ins-cf-.*\u0026quot; } } } On a different cloudprober instance:
... rds_server { provider { gcp_config { instances { zone_filter: \u0026quot;us-east1-a\u0026quot; } } } } grpc_tls_config { ca_cert_file: \u0026quot;/vol/certs/client_ca.crt\u0026quot; # To verify the server tls_cert_file: \u0026quot;/vol/certs/server.crt\u0026quot; # Own cert to present to client tls_key_file: \u0026quot;/vol/certs/server.key\u0026quot; # Own cert's private key } # Required for remote access grpc_port: 9314 Note that this has nothing to do with the AWS RDS product. Naming is unfortunate.\u0026#160;\u0026#x21a9;\u0026#xfe0e;
In fact, all dynamically discovered targets use the RDS protocol behind the scene.\u0026#160;\u0026#x21a9;\u0026#xfe0e;
`}),e.add({id:22,href:"/docs/surfacers/stackdriver/",title:"Stackdriver (Google Cloud)",description:`Cloudprober can natively export metrics to Google Cloud Monitoring (formerly, Stackdriver) using stackdriver surfacer. Adding stackdriver surfacer to cloudprober is as simple as adding the following stanza to the config:
surfacer { type: STACKDRIVER } This config will work if you\u0026rsquo;re running on GCP and your VM (or GKE pod) has access to Cloud Monitoring (Stackdriver). If running on any other platform, you\u0026rsquo;ll have to specify the GCP project where you want to send the metrics, and you\u0026rsquo;ll have to configure your environment for Google Application Default Credentials.`,content:`Cloudprober can natively export metrics to Google Cloud Monitoring (formerly, Stackdriver) using stackdriver surfacer. Adding stackdriver surfacer to cloudprober is as simple as adding the following stanza to the config:
surfacer { type: STACKDRIVER } This config will work if you\u0026rsquo;re running on GCP and your VM (or GKE pod) has access to Cloud Monitoring (Stackdriver). If running on any other platform, you\u0026rsquo;ll have to specify the GCP project where you want to send the metrics, and you\u0026rsquo;ll have to configure your environment for Google Application Default Credentials.
By default, stackdriver surfacer exports metrics with the following prefix: custom.googleapis.com/cloudprober/\u0026lt;probe-type\u0026gt;/\u0026lt;probe\u0026gt;. For example, for HTTP probe named google_com, standard metrics will be exported as:
custom.googleapis.com/cloudprober/http/google_com/total custom.googleapis.com/cloudprober/http/google_com/success custom.googleapis.com/cloudprober/http/google_com/failure custom.googleapis.com/cloudprober/http/google_com/latency All the config options for the stackdriver surfacer: config
For example, you can configure stackdriver surfacer to export only metrics that match a specific regex:
surfacer { stackdriver_surfacer { # Export only \u0026quot;http\u0026quot; probe metrics. allowed_metrics_regex: \u0026quot;.*\\\\/http\\\\/.*\u0026quot; } } Accessing the data #Cloudprober exports metrics to stackdriver as custom metrics. Since all cloudprober metrics are counters (total number of probes, success, latency), you\u0026rsquo;ll see rates of these metrics in stackdriver metrics explorer by default. This data may not be very useful as it is (unless you\u0026rsquo;re using distributions in cludprober, more on that later).
However, stackdriver now provides a powerful monitoring query language,MQL, using which we can get more useful metrics.
MQL to get failure ratio:
fetch global | { metric 'custom.googleapis.com/cloudprober/http/google_com/failure' ; metric 'custom.googleapis.com/cloudprober/http/google_com/total' } | align delta(1m) | join | div MQL to get average latency for a probe:
fetch global | { metric 'custom.googleapis.com/cloudprober/http/google_com/latency' ; metric 'custom.googleapis.com/cloudprober/http/google_com/success' } | align delta(1m) | join | div You can use MQL to create graphs and generate alerts. Note that in the examples here we are fetching from the \u0026ldquo;global\u0026rdquo; source (fetch global); if you\u0026rsquo;re running on GCP, you can improve performance of your queries by specifying the \u0026ldquo;gceinstance\u0026rdquo; resource type: _fetch gce_instance.
`}),e.add({id:23,href:"/docs/overview/probe/",title:"What is a Probe",description:"Cloudprober runs probes, but what is a probe? A probe runs an operation, usually against a set of targets (e.g., your API servers), and looks for an expected outcome. Typically probes access your systems the same way as your customers, hence verifying systems\u0026rsquo; availability and performance from consumers\u0026rsquo; point of view. For example, an HTTP probe executes an HTTP request against a web server to verify that the web server is available.",content:`Cloudprober runs probes, but what is a probe? A probe runs an operation, usually against a set of targets (e.g., your API servers), and looks for an expected outcome. Typically probes access your systems the same way as your customers, hence verifying systems\u0026rsquo; availability and performance from consumers\u0026rsquo; point of view. For example, an HTTP probe executes an HTTP request against a web server to verify that the web server is available. Cloudprober probes run repeatedly at a configured interval and export probe results as a set of metrics.
Example of an HTTP Probe checking the frontend and API availability. _____________ _______________ | | HTTP Probe | | | Cloudprober | ------------\u0026gt; | Website/APIs | |_____________| |_______________| Here are some of the options used to configure a probe:
Field Description type Probe type, for example: HTTP, PING or UDP name Probe name. Each probe should have a unique name. interval_msec How often to run the probe (in milliseconds). timeout_msec Probe timeout (in milliseconds). targets Targets to run probe against. validator Probe validators, further explained here. \u0026lt;type\u0026gt;_probe Probe type specific configuration, e.g. http_probe Please take a look at the ProbeDef protobuf for further details on various fields and options. All probe types export at least the following metrics:
Metric Description total Total probes run so far. success Number of successful probes. Deficit between total and success indicates failures. latency Cumulative probe latency (by default in microseconds). You can get more insights into latency by using distributions. Note that by default all metrics are cumulative, i.e. we export sum of all the values so far. Cumulative metrics have this nice property that you don\u0026rsquo;t lose historical information if you miss a metrics read cycle, but they also make certain calculations slightly more complicated (see below). To provide a choice to the user, Cloudprober provides an option to export metrics as gauge values. See modifying metrics for more details.
Example: In prometheus, you\u0026rsquo;ll do something like the following to compute success ratio and average latency from cumulative metrics.
success_ratio_1m = increase(success[1m]) / increase(total[1m]) average_latency_1m = increase(latency[1m]) / increase(success[1m]) Probe Types #Cloudprober has built-in support for the following probe types:
HTTP External Ping DNS UDP TCP More probe types can be added through cloudprober extensions.
HTTP #Code | Config options
HTTP probe sends HTTP(s) requests to a target and verify that a response is received. Apart from the core probe metrics (total, success, and latency), HTTP probes also export a map of response code counts (resp_code). By default, requests are marked as successful as long as they succeed, regardless of the HTTP response code, but this behavior can be changed by using validators. For example, you can add a validator to require status code to in a certain range, or response body to match a regex, etc (validator example).
SSL Certificate Expiry: If the target serves an SSL Certificate, cloudprober will walk the certificate chain and export the earliest expiry time in seconds as a metric. The metric is named ssl_earliest_cert_expiry_sec, and will only be exported when the expiry time in seconds is a positive number. External #Code | Config options
External probe type allows running arbitrary programs for probing. This is useful for running complex checks through Cloudprober. External probes are documented in much more detail here: external probe.
Ping #Code | Config options
Ping probe type implements a fast native ICMP ping prober, that can probe hundreds of targets in parallel. Probe results are reported as number of packets sent (total), received (success) and round-trip time (latency). It supports both, privileged and unprivileged (uses ICMP datagram socket) pings.
Note that ICMP datagram sockets are not enabled by default on most Linux systems. You can enable them by running the following command: sudo sysctl -w net.ipv4.ping_group_range=\u0026quot;0 5000\u0026quot;
DNS #Code | Config options
As the name suggests, DNS probe sends a DNS request to the target. This is useful to verify that your DNS server, typically a critical component of the infrastructure e.g. kube-dns, is working as expected.
UDP #Code | Config options
UDP probe sends a UDP packet to the configured targets. UDP probe (and all other probes that use ports) provides more coverage for the network elements on the data path as most packet forwarding elements use 5-tuple hashing and using a new source port for each probe ensures that we hit different network element each time.
TCP #Code | Config options
TCP probe verifies that we can establish a TCP connection to the given target and port.
`}),search.addEventListener("input",t,!0);function t(){const s=5;var n=this.value,o=e.search(n,{limit:s,enrich:!0});const t=new Map;for(const e of o.flatMap(e=>e.result)){if(t.has(e.doc.href))continue;t.set(e.doc.href,e.doc)}if(suggestions.innerHTML="",suggestions.classList.remove("d-none"),t.size===0&&n){const e=document.createElement("div");e.innerHTML=`No results for "<strong>${n}</strong>"`,e.classList.add("suggestion__no-results"),suggestions.appendChild(e);return}for(const[r,a]of t){const n=document.createElement("div");suggestions.appendChild(n);const e=document.createElement("a");e.href=r,n.appendChild(e);const o=document.createElement("span");o.textContent=a.title,o.classList.add("suggestion__title"),e.appendChild(o);const i=document.createElement("span");if(i.textContent=a.description,i.classList.add("suggestion__description"),e.appendChild(i),suggestions.appendChild(n),suggestions.childElementCount==s)break}}})()
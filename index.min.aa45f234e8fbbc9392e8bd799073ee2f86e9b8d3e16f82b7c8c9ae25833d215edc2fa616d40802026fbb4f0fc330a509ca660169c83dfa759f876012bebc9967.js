var suggestions=document.getElementById("suggestions"),search=document.getElementById("search");search!==null&&document.addEventListener("keydown",inputFocus);function inputFocus(e){e.ctrlKey&&e.key==="/"&&(e.preventDefault(),search.focus()),e.key==="Escape"&&(search.blur(),suggestions.classList.add("d-none"))}document.addEventListener("click",function(e){var t=suggestions.contains(e.target);t||suggestions.classList.add("d-none")}),document.addEventListener("keydown",suggestionFocus);function suggestionFocus(e){const s=suggestions.classList.contains("d-none");if(s)return;const t=[...suggestions.querySelectorAll("a")];if(t.length===0)return;const n=t.indexOf(document.activeElement);if(e.key==="ArrowUp"){e.preventDefault();const s=n>0?n-1:0;t[s].focus()}else if(e.key==="ArrowDown"){e.preventDefault();const s=n+1<t.length?n+1:n;t[s].focus()}}(function(){var e=new FlexSearch.Document({tokenize:"forward",cache:100,document:{id:"id",store:["href","title","description"],index:["title","description","content"]}});e.add({id:0,href:"/docs/overview/",title:"Get Started",description:"",content:""}),e.add({id:1,href:"/docs/how-to/",title:"How To",description:"",content:""}),e.add({id:2,href:"/docs/surfacers/",title:"Exporting Metrics",description:"",content:""}),e.add({id:3,href:"/docs/",title:"Docs",description:"",content:""}),e.add({id:4,href:"/docs/how-to/external-probe/",title:"External Probe",description:`External probe type allows you to run arbitrary, complex probes through Cloudprober. An external probe runs an independent external program for actual probing. Cloudprober calculates probe metrics based on program\u0026rsquo;s exit status and time elapsed in execution.
Cloudprober also allows external programs to provide additional metrics. Every message sent to stdout will be parsed as a new metrics to be emitted. For general logging you can use another I/O stream like stderr.`,content:`External probe type allows you to run arbitrary, complex probes through Cloudprober. An external probe runs an independent external program for actual probing. Cloudprober calculates probe metrics based on program\u0026rsquo;s exit status and time elapsed in execution.
Cloudprober also allows external programs to provide additional metrics. Every message sent to stdout will be parsed as a new metrics to be emitted. For general logging you can use another I/O stream like stderr.
Sample Probe #To understand how it works, lets create a sample probe that sets and gets a key in a redis server. Here is the main function of such a probe:
func main() { var client redis.Client var key = \u0026quot;hello\u0026quot; startTime := time.Now() client.Set(key, []byte(\u0026quot;world\u0026quot;)) fmt.Printf(\u0026quot;op_latency_ms{op=set} %f\\n\u0026quot;, float64(time.Since(startTime).Nanoseconds())/1e6) startTime = time.Now() val, _ := client.Get(\u0026quot;hello\u0026quot;) log.Printf(\u0026quot;%s=%s\u0026quot;, key, string(val)) fmt.Printf(\u0026quot;op_latency_ms{op=get} %f\\n\u0026quot;, float64(time.Since(startTime).Nanoseconds())/1e6) } (Full listing: https://github.com/cloudprober/cloudprober/blob/master/examples/external/redis_probe.go) This program sets and gets a key in redis and prints the time taken for both operations. op_latency_ms{op=get|set} will be emitted as metrics. You could also define your own labels using this format:
Cloudprober can use this program as an external probe, to verify the availability and performance of the redis server. This program assumes that redis server is running locally, at its default port. For the sake of demonstration, lets run a local redis server (you can also easily modify this program to use a different server.)
#!bash OS=\$(uname) [[ \u0026quot;\$OS\u0026quot; == \u0026quot;Darwin\u0026quot; ]] \u0026amp;\u0026amp; brew install redis [[\u0026quot;\$OS\u0026quot; == \u0026quot;Linux\u0026quot;]] \u0026amp;\u0026amp; sudo apt install redis Let\u0026rsquo;s compile our probe program (redis_probe.go) and verify that it\u0026rsquo;s working as expected:
#!bash CGO_ENABLED=0 go build -ldflags “-extldflags=-static” examples/external/redis_probe.go ./redis_probe 2022/02/24 12:39:45 hello=world op_latency_ms{op=set} 22.656588 op_latency_ms{op=get} 2.173560 Configuration #Here is the external probe configuration that makes use of this program:
Full example in examples/external/cloudprober.cfg.
# Run an external probe that executes a command from the current working # directory. probe { name: \u0026quot;redis_probe\u0026quot; type: EXTERNAL targets { dummy_targets {} } external_probe { mode: ONCE command: \u0026quot;./redis_probe\u0026quot; } } Note: To pass target information to your external program as arguments use the @label@ notation. Supported fields are: @target@, @address@, @port@, @probe@, and target labels like @target.label.fqdn@.
command: \u0026quot;./redis_probe\u0026quot; -host=@address@ -port=@port@ Running it through cloudprober, you\u0026rsquo;ll see the following output:
# Launch cloudprober cloudprober --config_file=cloudprober.cfg cloudprober 1519..0 1519583408 labels=ptype=external,probe=redis_probe,dst= success=1 total=1 latency=12143.765 cloudprober 1519..1 1519583408 labels=ptype=external,probe=redis_probe,dst=,op=get op_latency_ms=0.516 get_latency_ms=0.491 cloudprober 1519..2 1519583410 labels=ptype=external,probe=redis_probe,dst= success=2 total=2 latency=30585.915 cloudprober 1519..3 1519583410 labels=ptype=external,probe=redis_probe,dst=,op=set op_latency_ms=0.636 get_latency_ms=0.994 cloudprober 1519..4 1519583412 labels=ptype=external,probe=redis_probe,dst= success=3 total=3 latency=42621.871 You can import this data in prometheus following the process outlined at: Running Prometheus. Before doing that, let\u0026rsquo;s make it more interesting.
Distributions #How nice will it be if we could find distribution of the set and get latency. If tail latency was too high, it could explain the random timeouts in your application. Fortunately, it\u0026rsquo;s very easy to create distributions in Cloudprober. You just need to add the following section to your probe definition:
Full example in examples/external/cloudprober_aggregate.cfg.
# Run an external probe and aggregate metrics in cloudprober. ... output_metrics_options { aggregate_in_cloudprober: true # Create distributions for get_latency_ms and set_latency_ms. dist_metric { key: \u0026quot;op_latency_ms\u0026quot; value: { explicit_buckets: \u0026quot;0.1,0.2,0.4,0.6,0.8,1.0,2.0\u0026quot; } } } This configuration adds options to aggregate the metrics in the cloudprober and configures \u0026ldquo;op_latency_ms\u0026rdquo; as a distribution metric with explicit buckets. Cloudprober will now build cumulative distributions using for these metrics. We can import this data in Stackdriver or Prometheus and get the percentiles of the \u0026ldquo;get\u0026rdquo; and \u0026ldquo;set\u0026rdquo; latencies. Following screenshot shows the grafana dashboard built using these metrics.
Server Mode #The probe that we created above forks out a new redis_probe process for every probe cycle. This can get expensive if probe frequency is high and the process is big (e.g. a Java binary). Also, what if you want to keep some state across probes, for example, lets say you want to monitor performance over HTTP/2 where you keep using the same TCP connection for multiple HTTP requests. A new process every time makes keeping state impossible.
External probe\u0026rsquo;s server mode provides a way to run the external probe process in daemon mode. Cloudprober communicates with this process over stdout/stdin (connected with OS pipes), using serialized protobuf messages. Cloudprober comes with a serverutils package that makes it easy to build external probe servers in Go.
Please see the code at examples/external/redis_probe.go for server mode implementation of the above probe. Here is the corresponding cloudprober config to run this probe in server mode: examples/external/cloudprober_server.cfg.
In server mode, if external probe process dies for reason, it\u0026rsquo;s restarted by Cloudprober.
`}),e.add({id:5,href:"/docs/how-to/k8s_targets/",title:"Kubernetes Targets",description:`If you\u0026rsquo;re running on Kubernetes, you\u0026rsquo;d probably want to monitor Kubernetes resources (e.g. pods, endpoints, ingresses, etc) as well. Cloudprober supports dynamic discovery of Kubernetes resources through the targets type k8s.
For example, the following config adds an HTTP probe for the endpoints named cloudprober (equivalent to running kubectl get ep cloudprober).
probe { name: \u0026quot;pod-to-endpoints\u0026quot; type: HTTP targets { # Equivalent to kubectl get ep cloudprober k8s { endpoints: \u0026quot;cloudprober\u0026quot; } } # Note that the following http_probe automatically uses target's discovered # port.`,content:`If you\u0026rsquo;re running on Kubernetes, you\u0026rsquo;d probably want to monitor Kubernetes resources (e.g. pods, endpoints, ingresses, etc) as well. Cloudprober supports dynamic discovery of Kubernetes resources through the targets type k8s.
For example, the following config adds an HTTP probe for the endpoints named cloudprober (equivalent to running kubectl get ep cloudprober).
probe { name: \u0026quot;pod-to-endpoints\u0026quot; type: HTTP targets { # Equivalent to kubectl get ep cloudprober k8s { endpoints: \u0026quot;cloudprober\u0026quot; } } # Note that the following http_probe automatically uses target's discovered # port. http_probe { relative_url: \u0026quot;/status\u0026quot; } } Supported Resource and Filters #Cloudprober supports discovery for the following k8s resources:
Services Endpoints Pods Ingresses Filters #You can filter k8s resources using the following options:
name: (regex) Resource name filter. It can be a regex. Example: # Endpoints with names ending in \u0026quot;service\u0026quot; targets { k8s { endpoints: \u0026quot;.*-service\u0026quot; } } namespace: Namespace filter. Example: # Ingresses in \u0026quot;prod\u0026quot; namespace, ending in \u0026quot;lb\u0026quot; targets { k8s { namespace: \u0026quot;prod\u0026quot; ingresses: \u0026quot;.*-lb\u0026quot; } } # Kube-DNS service targets { k8s { namespace: \u0026quot;kube-system\u0026quot; services: \u0026quot;kube-dns\u0026quot; } } labelSelector: Label based selector. It can be repeated, and works similar to the kubectl\u0026rsquo;s \u0026ndash;selector/-l flag. Example: targets { k8s { pods: \u0026quot;.*\u0026quot; labelSelector: \u0026quot;k8s-app\u0026quot; # k8a-app label exists labelSelector: \u0026quot;role=frontend\u0026quot; # label \u0026quot;role\u0026quot; is set to \u0026quot;frontend\u0026quot; labelSelector: \u0026quot;!no-monitoring\u0026quot; # label \u0026quot;no-monitoring is not set\u0026quot; } } portFilter: (regex) Filter resources by port name or number (if port name is not set). This is useful for resources like endpoints and services, where each resource may have multiple ports. Example: targets { k8s { endpoints: \u0026quot;.*-service\u0026quot; portFilter: \u0026quot;http-.*\u0026quot; } } Cluster Resources Access #Note: If you\u0026rsquo;ve installed Cloudprober using Helm Chart, this step is automatically taken care of.
Cloudprober discovers k8s resources using kubernetes APIs. It assumes that we are interested in the cluster we are running it in, and uses in-cluster config to talk to the kubernetes API server. For this set up to work, we need to give our container read-only access to kubernetes resources:
# Define a ClusterRole (resource-reader) for read-only access to the cluster # resources and bind this ClusterRole to the default service account. cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: v1 kind: ServiceAccount metadata: name: cloudprober --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \u0026quot;true\u0026quot; name: resource-reader namespace: default rules: - apiGroups: [\u0026quot;\u0026quot;] resources: [\u0026quot;*\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;] - apiGroups: - extensions - \u0026quot;networking.k8s.io\u0026quot; # k8s 1.14+ resources: - ingresses - ingresses/status verbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;] --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: default-resource-reader namespace: default subjects: - kind: ServiceAccount name: cloudprober namespace: default roleRef: kind: ClusterRole name: resource-reader apiGroup: rbac.authorization.k8s.io EOF This will create a new service account cloudprober and will give it read-only access to the cluster resources.
`}),e.add({id:6,href:"/docs/how-to/run-on-kubernetes/",title:"Running On Kubernetes",description:`Kubernetes is a popular platform for running containers, and Cloudprober container runs on Kubernetes right out of the box. This document shows how you can run Cloudprober on kubernetes, use ConfigMap for config, and discover kubernetes targets automatically.
ⓘ If you use helm charts for k8s installations, Cloudprober helm chart provides the most convenient way to run Cloudprober on k8s.ConfigMap #In Kubernetes, a convenient way to provide config to containers is to use config maps.`,content:`Kubernetes is a popular platform for running containers, and Cloudprober container runs on Kubernetes right out of the box. This document shows how you can run Cloudprober on kubernetes, use ConfigMap for config, and discover kubernetes targets automatically.
ⓘ If you use helm charts for k8s installations, Cloudprober helm chart provides the most convenient way to run Cloudprober on k8s.ConfigMap #In Kubernetes, a convenient way to provide config to containers is to use config maps. Let\u0026rsquo;s create a config that specifies a probe to monitor \u0026ldquo;google.com\u0026rdquo;.
probe { name: \u0026quot;google-http\u0026quot; type: HTTP targets { host_names: \u0026quot;www.google.com\u0026quot; } http_probe {} interval_msec: 15000 timeout_msec: 1000 } Save this config in cloudprober.cfg, create a config map using the following command:
kubectl create configmap cloudprober-config \\ --from-file=cloudprober.cfg=cloudprober.cfg If you change the config, you can update the config map using the following command:
kubectl create configmap cloudprober-config \\ --from-file=cloudprober.cfg=cloudprober.cfg -o yaml --dry-run | \\ kubectl replace -f - Deployment Map #Now let\u0026rsquo;s add a deployment.yaml to add the config volume and cloudprober container:
apiVersion: apps/v1 kind: Deployment metadata: name: cloudprober spec: replicas: 1 selector: matchLabels: app: cloudprober template: metadata: annotations: checksum/config: \u0026quot;\${CONFIG_CHECKSUM}\u0026quot; labels: app: cloudprober spec: volumes: - name: cloudprober-config configMap: name: cloudprober-config containers: - name: cloudprober image: cloudprober/cloudprober command: [\u0026quot;/cloudprober\u0026quot;] args: [\u0026quot;--config_file\u0026quot;, \u0026quot;/cfg/cloudprober.cfg\u0026quot;, \u0026quot;--logtostderr\u0026quot;] volumeMounts: - name: cloudprober-config mountPath: /cfg ports: - name: http containerPort: 9313 --- apiVersion: v1 kind: Service metadata: name: cloudprober labels: app: cloudprober spec: ports: - port: 9313 protocol: TCP targetPort: 9313 selector: app: cloudprober type: NodePort Note that we added an annotation to the deployment spec; this annotation allows us to update the deployment whenever cloudprober config changes. We can update this annotation based on the local cloudprober config content, and update the deployment using the following one-liner:
# Update the config checksum annotation in deployment.yaml before running # kubectl apply. export CONFIG_CHECKSUM=\$(kubectl get cm/cloudprober-config -o yaml | sha256sum) \u0026amp;\u0026amp; \\ cat deployment.yaml | envsubst | kubectl apply -f - (Note: If you use Helm for Kubernetes deployments, Helm provides a more native way to include config checksums in deployments.)
Applying the above yaml file, should create a deployment with a service at port 9313:
\$ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE cloudprober 1/1 1 1 94m \$ kubectl get service cloudprober NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE cloudprober NodePort 10.31.249.108 \u0026lt;none\u0026gt; 9313:31367/TCP 94m Now you should be able to access various cloudprober URLs (/status for status,/config for config, /metrics for prometheus-format metrics) from within the cluster. For quick verification you can also set up a port forwarder and access these URLs locally at localhost:9313:
kubectl port-forward svc/cloudprober 9313:9313 Once you\u0026rsquo;ve verified that everything is working as expected, you can go on setting up metrics collection through prometheus (or stackdriver) in usual ways.
Kubernetes Targets #If you\u0026rsquo;re running on Kubernetes, you\u0026rsquo;d probably want to monitor Kubernetes resources (e.g. pods, endpoints, etc) as well. Cloudprober supports dynamic discovery of Kubernetes resources through the targets type k8s.
For example, the following config adds an HTTP probe for the endpoints named cloudprober (equivalent to running kubectl get ep cloudprober).
probe { name: \u0026quot;pod-to-endpoints\u0026quot; type: HTTP targets { # Equivalent to kubectl get ep cloudprober k8s { endpoints: \u0026quot;cloudprober\u0026quot; } } # Note that the following http_probe automatically uses target's discovered # port. http_probe { relative_url: \u0026quot;/status\u0026quot; } } Supported Resource and Filters #Cloudprober supports discovery for the following k8s resources:
Services Endpoints Pods Ingresses You can filter k8s resources using the following options:
name: (regex) Resource name filter. It can be a regex. Example: # Endpoints with names ending in \u0026quot;service\u0026quot; k8s { endpoints: \u0026quot;.*-service\u0026quot; } namespace: Namespace filter. Example: # Ingresses in \u0026quot;prod\u0026quot; namespace, ending in \u0026quot;lb\u0026quot; k8s { namespace: \u0026quot;prod\u0026quot; ingresses: \u0026quot;.*-lb\u0026quot; } labelSelector: Label based selector. It can be repeated, and works similar to the kubectl\u0026rsquo;s \u0026ndash;selector/-l flag. Example: k8s { pods: \u0026quot;.*\u0026quot; labelSelector: \u0026quot;k8s-app\u0026quot; # k8a-app label exists labelSelector: \u0026quot;role=frontend\u0026quot; # label \u0026quot;role\u0026quot; is set to \u0026quot;frontend\u0026quot; labelSelector: \u0026quot;!no-monitoring\u0026quot; # label \u0026quot;no-monitoring is not set\u0026quot; } portFilter: (regex) Filter resources by port name or number (if port name is not set). This is useful for resources like endpoints and services, where each resource may have multiple ports. Example: k8s { endpoints: \u0026quot;.*-service\u0026quot; portFilter: \u0026quot;http-.*\u0026quot; } Cluster Resources Access #Cloudprober discovers k8s resources using kubernetes APIs. It assumes that we are interested in the cluster we are running it in, and uses in-cluster config to talk to the kubernetes API server. For this set up to work, we need to give our container read-only access to kubernetes resources:
# Define a ClusterRole (resource-reader) for read-only access to the cluster # resources and bind this ClusterRole to the default service account. cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: v1 kind: ServiceAccount metadata: name: cloudprober --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \u0026quot;true\u0026quot; name: resource-reader namespace: default rules: - apiGroups: [\u0026quot;\u0026quot;] resources: [\u0026quot;*\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;] - apiGroups: - extensions - \u0026quot;networking.k8s.io\u0026quot; # k8s 1.14+ resources: - ingresses - ingresses/status verbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;] --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: default-resource-reader namespace: default subjects: - kind: ServiceAccount name: cloudprober namespace: default roleRef: kind: ClusterRole name: resource-reader apiGroup: rbac.authorization.k8s.io EOF This will create a new service account cloudprober and will give it read-only access to the cluster resources.
Push Config Update #To push new cloudprober config to the cluster:
# Update the config map kubectl create configmap cloudprober-config \\ --from-file=cloudprober.cfg=cloudprober.cfg -o yaml --dry-run | \\ kubectl replace -f - # Update deployment export CONFIG_CHECKSUM=\$(kubectl get cm/cloudprober-config -o yaml | sha256sum) \u0026amp;\u0026amp; \\ cat deployment.yaml | envsubst | kubectl apply -f - Cloudprober should now start monitoring cloudprober endpoints. To verify:
# Set up port fowarding such that you can access cloudprober:9313 through # localhost:9313. kubectl port-forward svc/cloudprober 9313:9313 \u0026amp; # Check status curl localhost:9313/status # Check metrics (prometheus data format) curl localhost:9313/metrics If you\u0026rsquo;re running on GKE and have not disabled cloud logging, you\u0026rsquo;ll also see logs in Stackdriver Logging.
`}),e.add({id:7,href:"/docs/how-to/additional-labels/",title:"Additional Labels",description:`You can add additional labels to probe metrics using a probe-level field: additional_label. An additional label\u0026rsquo;s value can be static, or it can be determined at the run-time: from the environment that the probe is running in (e.g. GCE instance labels), or target\u0026rsquo;s labels.
Example config here demonstrates adding various types of additional labels to probe metrics. For this config (also listed below for quick rerefence):
if ingress target has label \u0026ldquo;fqdn:app.`,content:`You can add additional labels to probe metrics using a probe-level field: additional_label. An additional label\u0026rsquo;s value can be static, or it can be determined at the run-time: from the environment that the probe is running in (e.g. GCE instance labels), or target\u0026rsquo;s labels.
Example config here demonstrates adding various types of additional labels to probe metrics. For this config (also listed below for quick rerefence):
if ingress target has label \u0026ldquo;fqdn:app.example.com\u0026rdquo;, and prober is running in the GCE zone us-east1-c, and prober\u0026rsquo;s GCE instance has label env:prod. Probe metrics will look like the following:
total{probe=\u0026quot;my_ingress\u0026quot;,ptype=\u0026quot;http\u0026quot;,metrictype=\u0026quot;prober\u0026quot;,env=\u0026quot;prod\u0026quot;,src_zone=\u0026quot;us-east1-c\u0026quot;,host=\u0026quot;app.example.com\u0026quot;}: 90 success{probe=\u0026quot;my_ingress\u0026quot;,ptype=\u0026quot;http\u0026quot;,metrictype=\u0026quot;prober\u0026quot;,env=\u0026quot;prod\u0026quot;,src_zone=\u0026quot;us-east1-c\u0026quot;,host=\u0026quot;app.example.com\u0026quot;}: 80 probe { name: \u0026quot;my_ingress\u0026quot; type: HTTP targets { rds_targets { resource_path: \u0026quot;k8s://ingresses\u0026quot; filter { key: \u0026quot;namespace\u0026quot; value: \u0026quot;default\u0026quot; } } } # Static label additional_label { key: \u0026quot;metrictype\u0026quot; value: \u0026quot;prober\u0026quot; } # Label is configured at the run-time, based on the prober instance label (GCE). additional_label { key: \u0026quot;env\u0026quot; value: \u0026quot;{{.label_env}}\u0026quot; } # Label is configured at the run-time, based on the prober environment (GCE). additional_label { key: \u0026quot;src_zone\u0026quot; value: \u0026quot;{{.zone}}\u0026quot; } # Label is configured based on the target's labels. additional_label { key: \u0026quot;host\u0026quot; value: \u0026quot;@target.label.fqdn@\u0026quot; } http_probe {} } (Listing source: examples/additional_label/cloudprober.cfg)
Adding your own metrics #For external probes, Cloudprober also allows external programs to provide additional metrics. See External Probe for more details.
`}),e.add({id:8,href:"/docs/how-to/validators/",title:"Validators",description:`Validators allow you to run checks on the probe request output (if any). For example, you can specify if you expect the probe output to match a certain regex or return a certain status code (for HTTP). You can configure more than one validators and all validators should succeed for the probe to be marked as success.
probe { name: \u0026quot;google_homepage\u0026quot; type: HTTP targets { host_names: \u0026quot;www.google.com\u0026quot; } interval_msec: 10000 # Probe every 10s # This validator should succeed.`,content:`Validators allow you to run checks on the probe request output (if any). For example, you can specify if you expect the probe output to match a certain regex or return a certain status code (for HTTP). You can configure more than one validators and all validators should succeed for the probe to be marked as success.
probe { name: \u0026quot;google_homepage\u0026quot; type: HTTP targets { host_names: \u0026quot;www.google.com\u0026quot; } interval_msec: 10000 # Probe every 10s # This validator should succeed. validator { name: \u0026quot;status_code_2xx\u0026quot; http_validator { success_status_codes: \u0026quot;200-299\u0026quot; } } # This validator will fail, notice missing 'o' in our regex. validator { name: \u0026quot;gogle_re\u0026quot; regex: \u0026quot;gogle\u0026quot; } } (Full listing: https://github.com/cloudprober/cloudprober/blob/master/examples/validators/cloudprober_validator.cfg)
To make the debugging easier, validation failures are logged and exported as an independent map counter \u0026ndash; validation_failure, with validator key. For example, the above example will result in the following counters being exported after 5 runs:
total{probe=\u0026quot;google_homepage\u0026quot;,dst=\u0026quot;www.google.com\u0026quot;} 5 success{probe=\u0026quot;google_homepage\u0026quot;,dst=\u0026quot;www.google.com\u0026quot;} 0 validation_failure{validator=\u0026quot;status_code_2xx\u0026quot;,probe=\u0026quot;google_homepage\u0026quot;,dst=\u0026quot;www.google.com\u0026quot;} 0 validation_failure{validator=\u0026quot;gogle_re\u0026quot;,probe=\u0026quot;google_homepage\u0026quot;,dst=\u0026quot;www.google.com\u0026quot;} 5 Note that validator counter will not go up if probe fails for other reasons, for example web server timing out. That\u0026rsquo;s why you typically don\u0026rsquo;t want to alert only on validation failures. That said, in some cases, validation failures could be the only thing you\u0026rsquo;re interested in, for example, if you\u0026rsquo;re trying to make sure that a certain copyright is always present in your web pages or you want to catch data integrity issues in your network.
Let\u0026rsquo;s take a look at the types of validators you can configure.
Regex Validator #Regex validator simply checks for a regex in the probe request output. It works for all probe types except for UDP and UDP_LISTENER - these probe types don\u0026rsquo;t support any validators at the moment.
HTTP Validator #HTTP response validator works only for the HTTP probe type. You can currently use HTTP validator to define success and failure status codes (represented by success_status_codes and failure_stauts_codes in the config):
If failure_status_codes is defined and response status code falls within that range, validator is considered to have failed. If success_status_codes is defined and response status code does not fall within that range, validator is considered to have failed. If failure_header is defined and HTTP response include specified header and there are matching values, validator is considered to have failed. Leaving value_regex empty checks only for header name. If success_header is defined and HTTP response does not include specified header with matching values, validator is considered to have failed. Leaving value_regex empty checks only for header name. Data Integrity Validator #Data integrity validator is designed to catch the packet corruption issues in the network. We have a basic check that verifies that the probe output is made up purely of a pattern repeated many times over.
`}),e.add({id:9,href:"/docs/how-to/percentiles/",title:"Percentiles, Histograms, and Distributions",description:"Percentiles give you a deeper insight into how your system is behaving. For example, if your application\u0026rsquo;s response latency is very low 94 times out 100 but very high for the remaining 6 times, your average latency will still be low but it won\u0026rsquo;t be a great experience for your users. In other words, this is the case where your 95th percentile latency is high, even though your average and median (50th-%ile) latency is very low.",content:`Percentiles give you a deeper insight into how your system is behaving. For example, if your application\u0026rsquo;s response latency is very low 94 times out 100 but very high for the remaining 6 times, your average latency will still be low but it won\u0026rsquo;t be a great experience for your users. In other words, this is the case where your 95th percentile latency is high, even though your average and median (50th-%ile) latency is very low.
A typical way to measure percentiles from continuous monitoring data, which you may have to aggregate across various sources, is to use histograms (also called, distributions). In a histogram, you assign the incoming data points (samples) to pre-defined buckets. Each data point increases the count for the bucket that it falls into; data point itself is discarded after that. You can take a look at the bucket counts at any point of time and get an estimate of the percentiles. Histograms make it easy to aggregate data across multiple entities, for example, from probes running on multiple machines.
Following diagram shows distribution of latencies into 9 equal sized histogram buckets:
(Above diagram shows histogram for the following samples: 5.1, 6.2, 9.0, 12.1, 8.3, 9.7, 9.4, 10.3, 14.1, 11.2, 16.6, 9.9, 10.6, 14.1, 0.9, 7.1, 17.7)
Histograms in Cloudprober (Distributions) #Cloudprober uses a metric type called \u0026lsquo;distribution\u0026rsquo; to create and export histograms. Cloudprober supports creating distributions for probe latencies, and for metrics generated from external probe payloads. To create distributions, you have to specify how the data should be bucketed \u0026ndash; you can either explicitly specify all bucket bounds, or use exponential buckets type which generates bucket bounds from only a few variables.
Here is an example of using explicit buckets for latencies:
probe { name: \u0026quot;...\u0026quot; type: HTTP targets { host_names: \u0026quot;...\u0026quot; } latency_unit: \u0026quot;ms\u0026quot; latency_distribution { explicit_buckets: \u0026quot;0.01,0.1,0.15,0.2,0.25,0.35,0.5,0.75,1.0,1.5,2.0,3.0,4.0,5.0,10.0,15.0,20.0\u0026quot; } } Configuring distributions #As seen in the example above, for latencies you configure distribution at the probe level by adding a field called latency_distribution. Without this field, cloudprober exports only cumulative latencies. To create distributions from an external probe\u0026rsquo;s data, take a look at the external probe\u0026rsquo;s documentation.
Format for the distribution field is in turn defined in dist.proto.
// Dist defines a Distribution data type. message Dist { oneof buckets { // Comma-separated list of lower bounds, where each lower bound is a float // value. Example: 0.5,1,2,4,8. string explicit_buckets = 1; // Exponentially growing buckets ExponentialBuckets exponential_buckets = 2; } } // ExponentialBucket defines a set of num_buckets+2 buckets: // bucket[0] covers (−Inf, 0) // bucket[1] covers [0, scale_factor) // bucket[2] covers [scale_factor, scale_factor*base) // ... // bucket[i] covers [scale_factor*base^(i−2), scale_factor*base^(i−1)) // ... // bucket[num_buckets+1] covers [scale_factor*base^(num_buckets−1), +Inf) // Note: Base must be at least 1.01. message ExponentialBuckets { optional float scale_factor = 1 [default = 1.0]; optional float base = 2 [default = 2]; optional uint32 num_buckets = 3 [default = 20]; } Percentiles and Heatmap #Now that we\u0026rsquo;ve configured cloudprober to generate distributions, how do we make use of this new information. This depends on the monitoring system (prometheus, stackdriver, postgres, etc) you\u0026rsquo;re exporting your data to.
Both prometheus and stackdriver support computing and plotting percentiles from the distributions data. Stackdriver can natively create heatmaps from distributions while for prometheus you need to use grafana to create heatmaps.
Stackdriver (Google Cloud Monitoring) #Stackdriver automatically shows percentile aggregator for distribution metrics in metrics explorer (example). You can also use Stackdriver MQL to create percentiles (see stackdriver documentation for other usages of MQL for cloudprober metrics):
fetch gce_instance | metric 'custom.googleapis.com/cloudprober/http/google_homepage/latency' | filter (resource.zone == 'us-central1-a') | align delta(1m) | every 1m | group_by [resource.zone], [value_latency_percentile: percentile(value.latency, 95)] Stackdriver has detailed documentation on charting distributions.
Prometheus #Cloudprober surfaces distributions to prometheus as prometheus metric type histogram. Here is an example of prometheus metrics page created by cloudprober:
# TYPE latency histogram latency_sum{ptype=\u0026quot;http\u0026quot;,probe=\u0026quot;my_probe\u0026quot;,dst=\u0026quot;hostA\u0026quot;} 77557.14022499947 1607766316442 latency_count{ptype=\u0026quot;http\u0026quot;,probe=\u0026quot;my_probe\u0026quot;,dst=\u0026quot;hostA\u0026quot;} 172150 1607766316442 latency_bucket{ptype=\u0026quot;http\u0026quot;,probe=\u0026quot;my_probe\u0026quot;,dst=\u0026quot;hostA\u0026quot;,le=\u0026quot;0.01\u0026quot;} 0 1607766316442 latency_bucket{ptype=\u0026quot;http\u0026quot;,probe=\u0026quot;my_probe\u0026quot;,dst=\u0026quot;hostA\u0026quot;,le=\u0026quot;0.1\u0026quot;} 0 1607766316442 ... ... latency_bucket{ptype=\u0026quot;http\u0026quot;,probe=\u0026quot;my_probe\u0026quot;,dst=\u0026quot;hostA\u0026quot;,le=\u0026quot;75\u0026quot;} 172150 1607766316442 latency_bucket{ptype=\u0026quot;http\u0026quot;,probe=\u0026quot;my_probe\u0026quot;,dst=\u0026quot;hostA\u0026quot;,le=\u0026quot;100\u0026quot;} 172150 1607766316442 latency_bucket{ptype=\u0026quot;http\u0026quot;,probe=\u0026quot;my_probe\u0026quot;,dst=\u0026quot;hostA\u0026quot;,le=\u0026quot;+Inf\u0026quot;} 172150 1607766316442 Fortunately there is already a plenty of good documentation on how to make use of histograms in prometheus and grafana:
Grafana blog on how to visualize prometheus histograms in grafana. Prometheus documentation on histrograms. More Resources #The Problem with Percentiles – Aggregation brings Aggravation. Why percentiles don\u0026rsquo;t work the way you think. `}),e.add({id:10,href:"/docs/how-to/targets/",title:"Targets",description:`Cloudprober probes usually run against some targets1 to check those targets' status, such as an HTTP probe to your APIs servers, or PING/TCP probes to a third-party provider to verify network connectivity to them. Each probe can have multiple targets. If a probe has multiple targets, Cloudprober runs concurrent probes against each target. This page further explains how targets work in Cloudprober.
Dynamically Discovered Targets #One of the core features of Cloudprober is the automatic and continuous discovery of targets.`,content:`Cloudprober probes usually run against some targets1 to check those targets' status, such as an HTTP probe to your APIs servers, or PING/TCP probes to a third-party provider to verify network connectivity to them. Each probe can have multiple targets. If a probe has multiple targets, Cloudprober runs concurrent probes against each target. This page further explains how targets work in Cloudprober.
Dynamically Discovered Targets #One of the core features of Cloudprober is the automatic and continuous discovery of targets. This feature is especially critical for the dynamic environments that today\u0026rsquo;s cloud based deployments make possible. For exmaple in a kubernetes cluster the number of pods and their IPs can change on the fly, either in response to replica count changes or node failures. Automated targets discovery makes sure that we don\u0026rsquo;t have to reconfigure Cloudprober in response to such events.
Targets Configuration #Cloudprober provides multiple ways to configure targets for a probe.
Static targets #Static targets are the easiest and most straight-forward to configure:
probe { ... targets { host_names: \u0026quot;www.google.com,www.yahoo.com,cloudprober:9313\u0026quot; } .. } In the above config, probe will run concurrently against 3 hosts: www.google.com, www.yahoo.com, and cloudprober:9313 (yes, you can specify ports here for port-aware probes).
File based targets #You can define your targets in a file and refer to them in Cloudprober through that file. This file can be modified independently, and whenever that happens cloudprober will reload it automatically.
Example configuration:
targets { file_targets { file_path: \u0026quot;/var/run/cloudprober/vips.json\u0026quot; } } In the targets file, resources should be specified in a specific format. Here is an example of targets in JSON format:
{ \u0026quot;resource\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;switch-xx-1\u0026quot;, \u0026quot;ip\u0026quot;: \u0026quot;10.1.1.1\u0026quot;, \u0026quot;port\u0026quot;: 8080, \u0026quot;labels\u0026quot;: { \u0026quot;device_type\u0026quot;: \u0026quot;switch\u0026quot;, \u0026quot;cluster\u0026quot;: \u0026quot;xx\u0026quot; } }, { \u0026quot;name\u0026quot;: \u0026quot;switch-xx-2\u0026quot;, \u0026quot;ip\u0026quot;: \u0026quot;10.1.1.2\u0026quot;, \u0026quot;port\u0026quot;: 8081, \u0026quot;labels\u0026quot;: { \u0026quot;cluster\u0026quot;: \u0026quot;xx\u0026quot; } } ] } (You can also define targets in the textproto format: example. Full example with cloudprober.cfg: file_based_targets)
Even if you don\u0026rsquo;t intend to use the auto-reload feature of the file targets, they can still be quite useful over static targets as they allow you to specify additional details for targets. For example, specifying target\u0026rsquo;s IP address in the example above lets you tackle the case where you want to specify target\u0026rsquo;s name, let\u0026rsquo;s say for better identification or for HTTP requests to work, but don\u0026rsquo;t want to rely on DNS for resolving its IP address.
K8s targets #K8s targets are explained at Kubernetes Targets.
GCP targets #Since Cloudprober started at GCP, it\u0026rsquo;s no surprise that Cloudprober has great support for GCP targets. Cloudprober supports the following GCP resources:
GCE Instances Forwarding Rules (regional and global) Cloud pub/sub (list of hostnames over cloud pub/sub) TODO: Add more details on GCP targets.
Probe configuration through target fields #Field Probe Type Configuration port Port aware probes (HTTP, DNS, TCP, UDP, etc) If a target has an associated port, for example, a Kubernetes endpoint, it will automatically be used for probing unless a port has been explicitly configured in the probe. label:relative_url HTTP If an explicit relative URL is not set, HTTP probe will use relative_url label\u0026rsquo;s value if set. label:fqdn HTTP HTTP probe will use target\u0026rsquo;s fqdn label as the URL-host (host part of the URL) and Host header if available and if Host header has not been configured explicitly. Metrics #Target name: All metrics generated by Cloudprober have a dst label which is set to the target name. Target labels: See additional labels for how resource labels can be used to set additional labels on the metrics. Scaling targets discovery and other features #If you run a lot of Cloudprober instances with targets discovery, you may end up overwhelming the API servers, or running out of your API quota in case of Cloud resources. To avoid that, Cloudprober allows centralizing the targets discovery through the Resource Discovery Service (RDS) mechanism. See Resource Discovery Service for more details on that.
Other salient features of the cloudprober\u0026rsquo;s targets discovery:
Continuous discovery. We don\u0026rsquo;t just discover targets in the beginning, but keep refreshing them at a regular interval. Protection against the upstream provider failures. If refreshing of the targets fails during one of the refresh cycles, we continue using the existing set of targets. There are some cases where there is no explicit target, for example, you may run a probe to measure your CI system\u0026rsquo;s performance, or run a complex probe that touches many endpoints.\u0026#160;\u0026#x21a9;\u0026#xfe0e;
`}),e.add({id:11,href:"/docs/how-to/built-in-servers/",title:"Built-in Servers",description:`Cloudprober has a few built in servers. This is useful when you are probing that a connection is working, or as a baseline to compare the probing results from your actual service to.
HTTP #server { type: HTTP http_server { port: 8080 } } This creates an HTTP server that responds on port 8080. By default it will respond to the following endpoints:
/healthcheck /lameduck server { type: HTTP http_server { port: 8080 pattern_data_handler { response_size: 1024 } pattern_data_handler { response_size: 4 pattern: \u0026quot;four\u0026quot; } } } This adds two endpoints to the HTTP server:`,content:`Cloudprober has a few built in servers. This is useful when you are probing that a connection is working, or as a baseline to compare the probing results from your actual service to.
HTTP #server { type: HTTP http_server { port: 8080 } } This creates an HTTP server that responds on port 8080. By default it will respond to the following endpoints:
/healthcheck /lameduck server { type: HTTP http_server { port: 8080 pattern_data_handler { response_size: 1024 } pattern_data_handler { response_size: 4 pattern: \u0026quot;four\u0026quot; } } } This adds two endpoints to the HTTP server:
/data_1024 which responds with 1024 bytes of cloudprobercloudprobercloudprober. /data_4 which responds with four. See servers/http/proto/config.go for all HTTP server configuration options.
UDP #A Cloudprober UDP server can be configured to either echo or discard packets it receives.
server { type: UDP udp_server { port: 85 type: ECHO } } server { type: UDP udp_server { port: 90 type: DISCARD } } See servers/udp/proto/config.go for all UDP server configuration options.
GRPC #See servers/grpc/proto/config.go for all GRPC server configuration options.
`}),e.add({id:12,href:"/docs/surfacers/cloudwatch/",title:"Cloudwatch (AWS Cloud Monitoring)",description:`Cloudprober can natively export metrics to AWS Cloudwatch using the cloudwatch surfacer. Adding the cloudwatch surfacer to cloudprover is as simple as adding the following stanza to the config:
surfacer { type: CLOUDWATCH } Authentication #The cloudwatch surfacer uses the AWS Go SDK, and supports the default credential chain:
Environment variables. Shared credentials file. If your application uses an ECS task definition or RunTask API operation, IAM role for tasks.`,content:`Cloudprober can natively export metrics to AWS Cloudwatch using the cloudwatch surfacer. Adding the cloudwatch surfacer to cloudprover is as simple as adding the following stanza to the config:
surfacer { type: CLOUDWATCH } Authentication #The cloudwatch surfacer uses the AWS Go SDK, and supports the default credential chain:
Environment variables. Shared credentials file. If your application uses an ECS task definition or RunTask API operation, IAM role for tasks. If your application is running on an Amazon EC2 instance, IAM role for Amazon EC2. Cloudwatch Region #The list below is the order of precedence that will be used to determine the AWS region that Cloudprober will publish metrics to.
Region configuration EC2 metadata. AWS_REGION environment variable. AWS_DEFAULT_REGION environment variable, if AWS_SDK_LOAD_CONFIG is set (See AWS package documentation for more details). Authorization #In order to permit Cloudprober to publish metric data to cloudwatch, ensure the profile being used for authentication has the following permissions, where the \u0026ldquo;cloudwatch:namespace\u0026rdquo; is the metric namespace used by Cloudprober.
If the default metric namespace is changed, also change the condition in the IAM policy below to match the same value.
{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Condition\u0026quot;: { \u0026quot;StringEqualsIgnoreCase\u0026quot;: { \u0026quot;cloudwatch:namespace\u0026quot;: \u0026quot;cloudprober\u0026quot; } }, \u0026quot;Action\u0026quot;: [ \u0026quot;cloudwatch:PutMetricData\u0026quot; ], \u0026quot;Resource\u0026quot;: [ \u0026quot;*\u0026quot; ], \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Sid\u0026quot;: \u0026quot;PutMetrics\u0026quot; } ] } Metric Namespace #The metric namespace used to publish metrics to by default is set to cloudprober. This can be changed by expanding the surfacer configuration:
surfacer { type: CLOUDWATCH cloudwatch_surfacer { namespace: \u0026quot;/cloudprober/website/probes\u0026quot; } } Note: If the namespace is modified, also modify the IAM policy condition for the namespace PutMetricData call.
Configuration Options #The full list of configuration options for the cloudwatch surfacer is:
// The cloudwatch metric namespace optional string namespace = 1 [default = \u0026quot;cloudprober\u0026quot;]; // The cloudwatch resolution value, lowering this below 60 will incur // additional charges as the metrics will be charged at a high resolution rate. optional int64 resolution = 2 [default=60]; // The AWS Region, used to create a CloudWatch session. // The order of fallback for evaluating the AWS Region: // 1. This config value. // 2. EC2 metadata endpoint, via cloudprober sysvars. // 3. AWS_REGION environment value. // 4. AWS_DEFAULT_REGION environment value, if AWS_SDK_LOAD_CONFIG is set. // https://docs.aws.amazon.com/sdk-for-go/api/aws/session/ optional string region = 3; // The maximum number of metrics that will be published at one // time. Metrics will be stored locally in a cache until this // limit is reached. 1000 is the maximum number of metrics // supported by the Cloudwatch PutMetricData API. // Metrics will be published when the timer expires, or the buffer is // full, whichever happens first. optional int32 metrics_batch_size = 4 [default = 1000]; // The maximum amount of time to hold metrics in the buffer (above). // Metrics will be published when the timer expires, or the buffer is // full, whichever happens first. optional int32 batch_timer_sec = 5 [default = 30]; (Source: https://github.com/cloudprober/cloudprober/blob/master/surfacers/cloudwatch/proto/config.proto)
Calculating the metric delta with Cloudwatch Metric Maths #The metrics produced by Cloudprober are cumulative. Most services producing metrics into cloudwatch produce snapshot data whereby the metrics are recorded for a specific point in time.
In order to achieve a similar effect here, the Cloudwatch Metric Maths RATE and PERIOD functions can be used to determine the delta values.
RATE(m1) * PERIOD(m1) Whereby m1 is the metric id for the Cloudprober metrics, for example:
namespace: cloudprober metric name: latency dst: google.com ptype: http probe: probe name `}),e.add({id:13,href:"/docs/how-to/extensions/",title:"Extending Cloudprober",description:"Cloudprober allows you to extend it across \u0026ldquo;probe\u0026rdquo; and \u0026ldquo;target\u0026rdquo; dimensions, that is, you can add new probe and target types to it without having to fork the entire codebase. Note that to extend cloudprober in this way, you will have to maintain your own cloudprober binary (which is mostly a wrapper around the \u0026ldquo;cloudprober package\u0026rdquo;), but you\u0026rsquo;ll be able to use rest of the cloudprober code from the common location.",content:`Cloudprober allows you to extend it across \u0026ldquo;probe\u0026rdquo; and \u0026ldquo;target\u0026rdquo; dimensions, that is, you can add new probe and target types to it without having to fork the entire codebase. Note that to extend cloudprober in this way, you will have to maintain your own cloudprober binary (which is mostly a wrapper around the \u0026ldquo;cloudprober package\u0026rdquo;), but you\u0026rsquo;ll be able to use rest of the cloudprober code from the common location.
Sample probe type #To demonstrate how it works, let\u0026rsquo;s add a new probe-type to Cloudprober. We\u0026rsquo;ll take the sample redis probe that we added in the external probe how-to, and convert it into a probe type that one can easily re-use. Let\u0026rsquo;s say that this probe-type provides a way to test redis server functionality and it takes the following options - operation (GET vs SET vs DELETE), key, value. This probe\u0026rsquo;s configuration looks like this:
probe { name: \u0026quot;redis_set\u0026quot; type: EXTENSION targets { host_names: \u0026quot;localhost:6379\u0026quot; } redis_probe { op: \u0026quot;set\u0026quot; key: \u0026quot;testkey\u0026quot; value: \u0026quot;testval\u0026quot; } } To make cloudprober understand this config, we\u0026rsquo;ll have to do a few things:
Define the probe config in a protobuf (.proto) file and mark it as an extension of the overall config.
Implement the probe type, possibly as a Go package, even though it can be embedded directly into the top-level binary.
Create a new cloudprober binary that includes the new probe type package.
Protobuf for the new probe type #Let\u0026rsquo;s create a new directory for our code: \$GOPATH/src/myprober.
// File: \$GOPATH/src/myprober/myprobe/myprobe.proto syntax = \u0026quot;proto2\u0026quot;; import \u0026quot;github.com/cloudprober/cloudprober/probes/proto/config.proto\u0026quot;; package myprober; message ProbeConf { // Redis operation required string op = 1; // Key and value for the redis operation required string key = 2; optional string value = 3; } extend cloudprober.probes.ProbeDef { optional ProbeConf redis_probe = 200; } Let\u0026rsquo;s generate Go code for this protobuf:
# From the myprober directory protoc --go_out=.,import_path=myprobe:. --proto_path=\$GOPATH/src:. myprobe/*.proto \$ ls myprobe/ myprobe.pb.go myprobe.proto Implement the probe type #Now let\u0026rsquo;s implement our probe type. Our probe type should implement the probes.Probe interface.
package myprobe // Probe holds aggregate information about all probe runs, per-target. type Probe struct { name string c *configpb.ProbeConf targets []string opts *options.Options ... } // Init initializes the probe with the given params. func (p *Probe) Init(name string, opts *options.Options) error { c, ok := opts.ProbeConf.(*ProbeConf) if !ok { return fmt.Errorf(\u0026quot;not a my probe config\u0026quot;) } // initialize p fields, p.name = name, etc. } // Start runs the probe indefinitely, at the configured interval. func (p *Probe) Start(ctx context.Context, dataChan chan *metrics.EventMetrics) { probeTicker := time.NewTicker(p.opts.Interval) for { select { case \u0026lt;-ctx.Done(): probeTicker.Stop() return case \u0026lt;-probeTicker.C: for _, em := range p.res { dataChan \u0026lt;- em } p.targets = p.opts.Targets.List() ... probeCtx, cancelFunc := context.WithDeadline(ctx, time.Now().Add(p.opts.Timeout)) p.runProbe(probeCtx) cancelFunc() } } } // runProbe runs probe for all targets and update EventMetrics. func (p *Probe) runProbe(ctx context.Context) { p.targets = p.opts.Targets.List() var wg sync.WaitGroup for _, target := range p.targets { wg.Add(1) go func(target string, em *metrics.EventMetrics) { defer wg.Done() em.Metric(\u0026quot;total\u0026quot;).(*metrics.Int).Inc() start := time.Now() err := p.runProbeForTarget(ctx, target) // run probe just for a single target if err != nil { p.l.Errorf(err.Error()) return } em.Metric(\u0026quot;success\u0026quot;).(*metrics.Int).Inc() em.Metric(\u0026quot;latency\u0026quot;).(metrics.LatencyValue).AddFloat64( time.Since(start). Seconds() / p.opts.LatencyUnit.Seconds()) }(target, p.res[target]) } wg.Wait() } Full example in examples/extensions/myprober/myprobe/myprobe.go.
This probe type sets or gets (depending on the configuration) a key-valye in redis and records success and time taken (latency) if operation is successful.
Implement a cloudprober binary that includes support for our probe #package main ... func main() { flag.Parse() // Register our probe type probes.RegisterProbeType(int(myprobe.E_RedisProbe.Field), func() probes.Probe { return \u0026amp;myprobe.Probe{} }) err := cloudprober.InitFromConfig(getConfig()) // getConfig not shown here. if err != nil { glog.Exitf(\u0026quot;Error initializing cloudprober. Err: %v\u0026quot;, err) } // web.Init sets up web UI for cloudprober. web.Init() cloudprober.Start(context.Background()) // Wait forever select {} } Full example in examples/extensions/myprober/myprober.go.
Let\u0026rsquo;s write a test config that uses the newly defined probe type:
probe { name: \u0026quot;redis_set\u0026quot; type: EXTENSION interval_msec: 10000 timeout_msec: 5000 targets { host_names: \u0026quot;localhost:6379\u0026quot; } [myprober.redis_probe] { op: \u0026quot;set\u0026quot; key: \u0026quot;testkey\u0026quot; value: \u0026quot;testval\u0026quot; } } Full example in examples/extensions/myprober/myprober.cfg.
Let\u0026rsquo;s compile our prober and run it with the above config:
go build ./myprober.go ./myprober --config_file=myprober.cfg you should see an output like the following:
cloudprober 1540848577649139842 1540848587 labels=ptype=redis,probe=redis_set,dst=localhost:6379 total=31 success=31 latency=70579.823 cloudprober 1540848577649139843 1540848887 labels=ptype=sysvars,probe=sysvars hostname=\u0026quot;manugarg-macbookpro5.roam.corp.google.com\u0026quot; start_timestamp=\u0026quot;1540848577\u0026quot; cloudprober 1540848577649139844 1540848887 labels=ptype=sysvars,probe=sysvars uptime_msec=310007.784 gc_time_msec=0.000 mallocs=14504 frees=826 cloudprober 1540848577649139845 1540848887 labels=ptype=sysvars,probe=sysvars goroutines=12 mem_stats_sys_bytes=7211256 cloudprober 1540848577649139846 1540848587 labels=ptype=redis,probe=redis_set,dst=localhost:6379 total=32 success=32 latency=72587.981 cloudprober 1540848577649139847 1540848897 labels=ptype=sysvars,probe=sysvars hostname=\u0026quot;manugarg-macbookpro5.roam.corp.google.com\u0026quot; start_timestamp=\u0026quot;1540848577\u0026quot; cloudprober 1540848577649139848 1540848897 labels=ptype=sysvars,probe=sysvars uptime_msec=320006.541 gc_time_msec=0.000 mallocs=14731 frees=844 cloudprober 1540848577649139849 1540848897 labels=ptype=sysvars,probe=sysvars goroutines=12 mem_stats_sys_bytes=7211256 You can import this data in prometheus following the process outlined at: Running Prometheus.
Conclusion #The article shows how to add a new probe type to cloudprober. Extending cloudprober allows you to implement new probe types that may make sense for your organization, but not for the open source community. You have to implement the logic for the probe type, but other cloudprober features work as it is \u0026ndash; targets, metrics (e.g. latency distribution if you configure it), surfacers - data can be multiple systems simultaneously, etc.
`}),e.add({id:14,href:"/docs/overview/getting-started/",title:"Getting Started",description:`Installation #From Source
If you have Go 1.9 or higher installed and GOPATH environment variable properly set up, you can download and install cloudprober using the following commands:
go get github.com/cloudprober/cloudprober GOBIN=\$GOPATH/bin go install \$GOPATH/src/github.com/cloudprober/cloudprober/cmd/cloudprober.go Pre-built Binaries
You can download pre-built binaries for Linux, MacOS and Windows from the project\u0026rsquo;s releases page.
(See this page for how to download the unreleased binaries.)
Docker Image You can download and run the latest docker image using the following command:`,content:`Installation #From Source
If you have Go 1.9 or higher installed and GOPATH environment variable properly set up, you can download and install cloudprober using the following commands:
go get github.com/cloudprober/cloudprober GOBIN=\$GOPATH/bin go install \$GOPATH/src/github.com/cloudprober/cloudprober/cmd/cloudprober.go Pre-built Binaries
You can download pre-built binaries for Linux, MacOS and Windows from the project\u0026rsquo;s releases page.
(See this page for how to download the unreleased binaries.)
Docker Image You can download and run the latest docker image using the following command:
docker run ghcr.io/cloudprober/cloudprober Configuration #Without any config, cloudprober will run only the \u0026ldquo;sysvars\u0026rdquo; module (no probes) and write metrics to stdout in cloudprober\u0026rsquo;s line protocol format (to be documented). It will also start a Prometheus exporter at: http://localhost:9313 (you can change the default port through the environment variable CLOUDPROBER_PORT and the default listening address through the environment variable CLOUDPROBER_HOST).
Since sysvars variables are not very interesting themselves, lets add a simple config that probes Google\u0026rsquo;s homepage:
# Write config to a file in /tmp cat \u0026gt; /tmp/cloudprober.cfg \u0026lt;\u0026lt;EOF probe { name: \u0026quot;google_homepage\u0026quot; type: HTTP targets { host_names: \u0026quot;www.google.com\u0026quot; } interval_msec: 5000 # 5s timeout_msec: 1000 # 1s } EOF This config adds an HTTP probe that accesses the homepage of the target \u0026ldquo;www.google.com\u0026rdquo; every 5s with a timeout of 1s. Cloudprober configuration is specified in the text protobuf format, with config schema described by the proto file: config.proto.
Assuming that you saved this file at /tmp/cloudprober.cfg (following the command above), you can have cloudprober use this config file using the following command line:
./cloudprober --config_file /tmp/cloudprober.cfg You can have the standard docker image use this config using the following command:
docker run -v /tmp/cloudprober.cfg:/etc/cloudprober.cfg \\ ghcr.io/cloudprober/cloudprober Note: While running on GCE, cloudprober config can also be provided through a custom metadata attribute: cloudprober_config.
Verification #One quick way to verify that cloudprober got the correct config is to access the URL http://localhost:9313/config (through cURL or in browser). It returns the config that cloudprober is using. You can also look at its current status at the URL (replace localhost by the actual hostname if not running locally): http://localhost:9313/status.
You should be able to see the generated metrics at http://localhost:9313/metrics (prometheus format) and the stdout (cloudprober format):
cloudprober 15.. 1500590520 labels=ptype=http,probe=google-http,dst=.. total=17 success=17 latency=180835 cloudprober 15.. 1500590530 labels=ptype=sysvars,probe=sysvars hostname=\u0026quot;manugarg-ws\u0026quot; uptime=100 cloudprober 15.. 1500590530 labels=ptype=http,probe=google-http,dst=.. total=19 success=19 latency=211644 This information is good for debugging monitoring issues, but to really make sense of this data, you\u0026rsquo;ll need to feed this data to another monitoring system like Prometheus or StackDriver (see Surfacers for more details). Lets set up a Prometheus and Grafana stack to make pretty graphs for us.
Running Prometheus #Download prometheus binary from its release page. You can use a config like the following to scrape a cloudprober instance running on the same host.
# Write config to a file in /tmp cat \u0026gt; /tmp/prometheus.yml \u0026lt;\u0026lt;EOF scrape_configs: - job_name: 'cloudprober' scrape_interval: 10s static_configs: - targets: ['localhost:9313'] EOF # Start prometheus: ./prometheus --config.file=/tmp/prometheus.yml Prometheus provides a web interface at http://localhost:9090. You can explore probe metrics and build useful graphs through this interface. All probes in cloudprober export at least 3 counters:
total: Total number of probes. success: Number of successful probes. Difference between total and success indicates failures. latency: Total (cumulative) probe latency. Using these counters, probe failure ratio and average latency can be calculated as:
failure_ratio = (rate(total) - rate(success)) / rate(total) avg_latency = rate(latency) / rate(success) Assuming that prometheus is running at localhost:9090, graphs depicting failure ratio and latency over time can be accessed in prometheus at: this url . Even though prometheus provides a graphing interface, Grafana provides much richer interface and has excellent support for prometheus.
Grafana #Grafana is a popular tool for building monitoring dashboards. Grafana has native support for prometheus and thanks to the excellent support for prometheus in Cloudprober itself, it\u0026rsquo;s a breeze to build Grafana dashboards from Cloudprober\u0026rsquo;s probe results.
To get started with Grafana, follow the Grafana-Prometheus integration guide.
`}),e.add({id:15,href:"/docs/overview/cloudprober/",title:"Overview",description:"Cloudprober is a monitoring software that makes it super-easy to monitor availability and performance of various components of your system. Cloudprober uses the \u0026ldquo;active\u0026rdquo; monitoring model. It runs probes against (or on) your components to verify that they are working as expected. For example, it can run a probe to verify that your frontends can reach your backends. Similarly it can run a probe to verify that your in-Cloud VMs can actually reach your on-premise systems.",content:` Cloudprober is a monitoring software that makes it super-easy to monitor availability and performance of various components of your system. Cloudprober uses the \u0026ldquo;active\u0026rdquo; monitoring model. It runs probes against (or on) your components to verify that they are working as expected. For example, it can run a probe to verify that your frontends can reach your backends. Similarly it can run a probe to verify that your in-Cloud VMs can actually reach your on-premise systems. This kind of monitoring makes it possible to monitor your systems' interfaces regardless of the implementation and helps you quickly pin down what\u0026rsquo;s broken in your system.
Features #Out of the box, config based, integration with many popular monitoring systems:
Prometheus/Grafana DataDog PostgreSQL AWS CloudWatch StackDriver / Google Cloud Monitoring Multiple options for checks:
Efficient, highly scalable, built-in probes: HTTP, PING, TCP, DNS, gRPC, UDP. Run custom checks through the \u0026quot;external\u0026quot; probe type. Automated targets discovery to make Cloud deployments as painless as possible:
Kubernetes resources. GCP instances, forwarding rules, and pub/sub messages. File based targets. Deployment friendly:
Written entirely in Go, and compiles into a static binary. Deploy as a standalone binary, or through docker containers. Continuous, automated target discovery, to ensure that most infrastructure changes don\u0026rsquo;t require re-deployment. Low footprint. Cloudprober takes advantage of the Go\u0026rsquo;s concurrency paradigms, and makes most of the available processing power. Configurable metrics:
Configurable metrics labels, based on the resource labels. Latency histograms for percentile calculations. Extensible architecture. Cloudprober can be easily extended along most of the dimensions. Adding support for other Cloud targets, monitoring systems and even a new probe type, is straight-forward and fairly easy. Getting Started #Visit Getting Started page to get started with Cloudprober.
Feedback #We\u0026rsquo;d love to hear your feedback. If you\u0026rsquo;re using Cloudprober, would you please mind sharing how you use it by adding a comment here. It will be a great help in planning Cloudprober\u0026rsquo;s future progression.
Join Cloudprober Slack or Github discussions for questions and discussion about Cloudprober.
`}),e.add({id:16,href:"/docs/how-to/rds/",title:"Resource Discovery Service",description:`Note: This is an advanced topic. From a user\u0026rsquo;s perspective, it\u0026rsquo;s only useful if you want to scale targets discovery by centralizing it.Cloudprober internally defines and uses a protocol called resource discovery service (RDS1) for targets discovery2. It helps provide a consistent interface between the targets subsystem, actual resource discovery mechanisms, and probes subsystem. It also provides a way to move targets discovery into an independent process, which can be used to reduce the upstream API traffic.`,content:`Note: This is an advanced topic. From a user\u0026rsquo;s perspective, it\u0026rsquo;s only useful if you want to scale targets discovery by centralizing it.Cloudprober internally defines and uses a protocol called resource discovery service (RDS1) for targets discovery2. It helps provide a consistent interface between the targets subsystem, actual resource discovery mechanisms, and probes subsystem. It also provides a way to move targets discovery into an independent process, which can be used to reduce the upstream API traffic.
Protocol (rds_targets) #To understand the RDS protocol, let\u0026rsquo;s look at the rds_targets targets type. You can think of rds_targets as a configuration interface to the RDS service. When you configure rds_targets, you\u0026rsquo;re creating an RDS client that talks to an RDS backend that is either part of the same process (default) or available over gRPC (usefule for centralizing the upstream API calls).
Here are the RDS targets configuration options:
message RDSTargets { // RDS server options, for example: // rds_server_options { // server_address: \u0026quot;rds-server.xyz:9314\u0026quot; // oauth_config: { // ... // } // } // Default is to use the local server if any. optional rds.ClientConf.ServerOptions rds_server_options = 1; // Resource path specifies the resources to return. Resources paths have the // following format: // \u0026lt;resource_provider\u0026gt;://\u0026lt;resource_type\u0026gt;/\u0026lt;additional_params\u0026gt; // // Examples: // For GCE instances in projectA: \u0026quot;gcp://gce_instances/\u0026lt;projectA\u0026gt;\u0026quot; // Kubernetes Pods : \u0026quot;k8s://pods\u0026quot; optional string resource_path = 2; // Filters to filter resources by. Example: // filter { // key: \u0026quot;namespace\u0026quot; // value: \u0026quot;mynamesspace\u0026quot; // } // filter { // key: \u0026quot;labels.app\u0026quot; // value: \u0026quot;web-service\u0026quot; // } repeated rds.Filter filter = 3; // IP config to specify the IP address to pick for a resource. IPConfig // is defined here: // https://github.com/cloudprober/cloudprober/blob/master/rds/proto/rds.proto optional rds.IPConfig ip_config = 4; } Most options are explained in the comments for a quick reference. Here is the further explanation of some of these options:
rds_server_options #This field specifies how to connect to the RDS server: server address and security options (OAuth and TLS). If left unspecified, it connects to the local server if any (started through rds_server option). Next up it looks for the rds_server_options in global_targets_options.
resource_path #Resource path specifies the resources we are interested in. It consists of a resource provider, resource type and an optional relative path: \u0026lt;resource_provider\u0026gt;://\u0026lt;resource_type\u0026gt;/\u0026lt;optional_relative_path\u0026gt;
resource_provider: Resource provider is a generic concept within the RDS protocol but usually maps to the cloud provider. Cloudprober RDS server currently implements the Kubernetes (k8s) and GCP (gcp) resource providers. We plan to add more resource providers in future. resource_type: Available resource types depend on the providers, for example, for k8s provider supports the following resource types: pods, endpoints, and services. optional_relative_path: For most resource types you can specify resource name in the resource path itself, e.g. k8s://services/cloudprober. Alternatively, you can use filters to filter by name, resource, etc. filter #Filters are key-value strings that can be used to filter resources by various fields. Filters depend on the resource types, but most resources support filtering by name and labels.
# Return resources that start with \u0026quot;web\u0026quot; and have label \u0026quot;service:service-a\u0026quot; ... filter { key: \u0026quot;name\u0026quot; value: \u0026quot;^web.*\u0026quot; } filter { key: \u0026quot;labels.service\u0026quot; value: \u0026quot;service-a\u0026quot; } Filters supported by kubernetes resources: k8s filters. Filters supported by GCP: GCE Instances Forwarding Rules Pub/Sub Messages Running RDS Server #RDS server can either be run as an independent process, or it can be a part of the main prober process. Former mode is useful for large deployments where you may want to reduce the API upcall traffic (for example, to GCP). For example, if you run 1000+ prober processes, it will be much more economical from the API quota usage point of view to have a centralized RDS service with much fewer (2-3) instances instead of having each prober process make its own API calls.
RDS server can be added to a cloudprober process using the rds_server stanza. If you\u0026rsquo;re running RDS server in a remote process, you\u0026rsquo;ll have to enable gRPC server in that process (using grpc_port) so that other instances can access it remotely.
Here is an example RDS server configuration:
rds_server { # GCP provider to discover GCP resources. provider { gcp_config { # Projects to discover resources in. project: \u0026quot;test-project-1\u0026quot; project: \u0026quot;test-project-2\u0026quot; # Discover GCE instances in us-central1. gce_instances { zone_filter: \u0026quot;name = us-central1-*\u0026quot; re_eval_sec: 60 # How often to refresh, default is 300s. } # GCE forwarding rules. forwarding_rules {} } } # Kubernetes targets are further discussed at: # https://cloudprober.org/how-to/run-on-kubernetes/#kubernetes-targets provider { kubernetes_config { endpoints {} } } } For the remote RDS server setup, if accessing over external network, you can secure the underlying gRPC communication using TLS certificates.
Remote RDS Server Example #Cloudprober config:
probe { rds_targets { rds_server_options { server_address: \u0026quot;rds-service:9314\u0026quot; # mTLS configuration tls_config { ca_cert_file: \u0026quot;/vol/certs/server_ca.crt\u0026quot; # To verify the server tls_cert_file: \u0026quot;/vol/certs/client.crt\u0026quot; # Own cert to present to server tls_key_file: \u0026quot;/vol/certs/client.key\u0026quot; # Own cert's private key } } resource_path: \u0026quot;gcp://gce_instances\u0026quot; filter { key: \u0026quot;name\u0026quot; value: \u0026quot;ins-cf-.*\u0026quot; } } } On a different cloudprober instance:
... rds_server { provider { gcp_config { instances { zone_filter: \u0026quot;us-east1-a\u0026quot; } } } } grpc_tls_config { ca_cert_file: \u0026quot;/vol/certs/client_ca.crt\u0026quot; # To verify the server tls_cert_file: \u0026quot;/vol/certs/server.crt\u0026quot; # Own cert to present to client tls_key_file: \u0026quot;/vol/certs/server.key\u0026quot; # Own cert's private key } # Required for remote access grpc_port: 9314 Note that this has nothing to do with the AWS RDS product. Naming is unfortunate.\u0026#160;\u0026#x21a9;\u0026#xfe0e;
In fact, all dynamically discovered targets use the RDS protocol behind the scene.\u0026#160;\u0026#x21a9;\u0026#xfe0e;
`}),e.add({id:17,href:"/docs/surfacers/stackdriver/",title:"Stackdriver (Google Cloud Monitoring)",description:`Cloudprober can natively export metrics to Google Cloud Monitoring (formerly, Stackdriver) using stackdriver surfacer. Adding stackdriver surfacer to cloudprober is as simple as adding the following stanza to the config:
surfacer { type: STACKDRIVER } This config will work if you\u0026rsquo;re running on GCP and your VM (or GKE pod) has access to Cloud Monitoring (Stackdriver). If running on any other platform, you\u0026rsquo;ll have to specify the GCP project where you want to send the metrics, and you\u0026rsquo;ll have to configure your environment for Google Application Default Credentials.`,content:`Cloudprober can natively export metrics to Google Cloud Monitoring (formerly, Stackdriver) using stackdriver surfacer. Adding stackdriver surfacer to cloudprober is as simple as adding the following stanza to the config:
surfacer { type: STACKDRIVER } This config will work if you\u0026rsquo;re running on GCP and your VM (or GKE pod) has access to Cloud Monitoring (Stackdriver). If running on any other platform, you\u0026rsquo;ll have to specify the GCP project where you want to send the metrics, and you\u0026rsquo;ll have to configure your environment for Google Application Default Credentials.
By default, stackdriver surfacer exports metrics with the following prefix: custom.googleapis.com/cloudprober/\u0026lt;probe-type\u0026gt;/\u0026lt;probe\u0026gt;. For example, for HTTP probe named google_com, standard metrics will be exported as:
custom.googleapis.com/cloudprober/http/google_com/total custom.googleapis.com/cloudprober/http/google_com/success custom.googleapis.com/cloudprober/http/google_com/failure custom.googleapis.com/cloudprober/http/google_com/latency Here are all the config options for stackdriver surfacer:
// GCP project name for stackdriver. If not specified and running on GCP, // local project is used. optional string project = 1; // If allowed_metrics_regex is specified, only metrics matching the given // regular expression will be exported to stackdriver. Since probe type and // probe name are part of the metric name, you can use this field to restrict // stackdriver metrics to a particular probe. // Example: // allowed_metrics_regex: \u0026quot;.*(http|ping).*(success|validation_failure).*\u0026quot; optional string allowed_metrics_regex = 3; // Monitoring URL base. Full metric URL looks like the following: // \u0026lt;monitoring_url\u0026gt;/\u0026lt;ptype\u0026gt;/\u0026lt;probe\u0026gt;/\u0026lt;metric\u0026gt; // Example: // custom.googleapis.com/cloudprober/http/google-homepage/latency optional string monitoring_url = 4 [default = \u0026quot;custom.googleapis.com/cloudprober/\u0026quot;]; (Source: https://github.com/cloudprober/cloudprober/blob/master/surfacers/stackdriver/proto/config.proto)
For example, you can configure stackdriver surfacer to export only metrics that match a specific regex:
surfacer { stackdriver_surfacer { # Export only \u0026quot;http\u0026quot; probe metrics. allowed_metrics_regex: \u0026quot;.*\\\\/http\\\\/.*\u0026quot; } } Accessing the data #Cloudprober exports metrics to stackdriver as custom metrics. Since all cloudprober metrics are counters (total number of probes, success, latency), you\u0026rsquo;ll see rates of these metrics in stackdriver metrics explorer by default. This data may not be very useful as it is (unless you\u0026rsquo;re using distributions in cludprober, more on that later).
However, stackdriver now provides a powerful monitoring query language,MQL, using which we can get more useful metrics.
MQL to get failure ratio:
fetch global | { metric 'custom.googleapis.com/cloudprober/http/google_com/failure' ; metric 'custom.googleapis.com/cloudprober/http/google_com/total' } | align delta(1m) | join | div MQL to get average latency for a probe:
fetch global | { metric 'custom.googleapis.com/cloudprober/http/google_com/latency' ; metric 'custom.googleapis.com/cloudprober/http/google_com/success' } | align delta(1m) | join | div You can use MQL to create graphs and generate alerts. Note that in the examples here we are fetching from the \u0026ldquo;global\u0026rdquo; source (fetch global); if you\u0026rsquo;re running on GCP, you can improve performance of your queries by specifying the \u0026ldquo;gceinstance\u0026rdquo; resource type: _fetch gce_instance.
`}),e.add({id:18,href:"/docs/surfacers/overview/",title:"Surfacers",description:"One of the biggest strengths of cloudprober is that it can export data to multiple monitoring systems, even simultaneously, just based on simple configuration. Cloudprober does that using a built-in mechanism, called surfacers. Each surfacer type implements interface for a specific monitoring system, for example, pubsub surfacer publishes data to Google Pub/Sub. You can configure multiple surfacers at the same time. If you don\u0026rsquo;t specify any surfacer, prometheus and file surfacers are enabled automatically.",content:`One of the biggest strengths of cloudprober is that it can export data to multiple monitoring systems, even simultaneously, just based on simple configuration. Cloudprober does that using a built-in mechanism, called surfacers. Each surfacer type implements interface for a specific monitoring system, for example, pubsub surfacer publishes data to Google Pub/Sub. You can configure multiple surfacers at the same time. If you don\u0026rsquo;t specify any surfacer, prometheus and file surfacers are enabled automatically.
Why other monitoring systems? Cloudprober\u0026rsquo;s main purpose is to run probes and build standard, usable metrics based on the results of those probes. It doesn\u0026rsquo;t take any action on the generated data. Instead, it provides an easy interface to make that probe data available to systems that provide ways to consume monitoring data, for example for graphing and alerting.
Cloudprober currently supports following surfacer types:
Prometheus (config) Stackdriver (Google Cloud Monitoring) Google Pub/Sub (config) Postgres (config) File (config) Cloudwatch (AWS Cloud Monitoring) Source: surfacers config.
It\u0026rsquo;s easy to add more surfacers without having to understand the internals of cloudprober. You only need to implement the Surfacer interface.
Configuration #Adding surfacers to cloudprober is as easy as adding \u0026ldquo;surfacer\u0026rdquo; config stanzas to your config, like the following:
# Enable prometheus and stackdriver surfacers. # Make probe metrics available at the URL :\u0026lt;cloudprober_port\u0026gt;/metrics, for # scraping by prometheus. surfacer { type: PROMETHEUS prometheus_surfacer { # Following option adds a prefix to exported metrics, for example, # \u0026quot;total\u0026quot; metric is exported as \u0026quot;cloudprober_total\u0026quot;. metrics_prefix: \u0026quot;cloudprober_\u0026quot; } } # Stackdriver (Google Cloud Monitoring) surfacer. No other configuration # is necessary if running on GCP. surfacer { type: STACKDRIVER } Filtering Metrics #It is possible to filter the metrics that the surfacers receive.
Filtering by Label #Cloudprober can filter the metrics that are published to surfacers. To filter metrics by labels, reference one of the following keys in the surfacer configuration:
allow_metrics_with_label ignore_metrics_with_label Note: ignore_metrics_with_label takes precedence over allow_metrics_with_label.
For example, to ignore all sysvar metrics:
surfacer { type: PROMETHEUS ignore_metrics_with_label { key: \u0026quot;probe\u0026quot;, value: \u0026quot;sysvars\u0026quot;, } } Or to only allow metrics from http probes:
surfacer { type: PROMETHEUS allow_metrics_with_label { key: \u0026quot;ptype\u0026quot;, value: \u0026quot;http\u0026quot;, } } Filtering by Metric Name #For certain surfacers, cloudprober can filter the metrics that are published by name. The surfacers that support this functionality are:
Cloudwatch Prometheus Stackdriver Within the surfacer configuration, the following options are defined:
allow_metrics_with_name ignore_metrics_with_name Note: ignore_metrics_with_name takes precedence over allow_metrics_with_name.
To filter out all validation_failure metrics by name:
surfacer { type: PROMETHEUS ignore_metrics_with_name: \u0026quot;validation_failure\u0026quot; } (Source: https://github.com/cloudprober/cloudprober/blob/master/surfacers/proto/config.proto)
`}),e.add({id:19,href:"/docs/overview/probe/",title:"What is a Probe",description:`Cloudprober\u0026rsquo;s main task is to run probes. A probe executes something, usually against a set of targets, to verify that the systems are working as expected from consumers\u0026rsquo; point of view. For example, an HTTP probe executes an HTTP request against a web server to verify that the web server is available. Cloudprober probes run repeatedly at a configured interval and export probe results as a set of metrics.
A probe is defined as a set of the following fields:`,content:`Cloudprober\u0026rsquo;s main task is to run probes. A probe executes something, usually against a set of targets, to verify that the systems are working as expected from consumers\u0026rsquo; point of view. For example, an HTTP probe executes an HTTP request against a web server to verify that the web server is available. Cloudprober probes run repeatedly at a configured interval and export probe results as a set of metrics.
A probe is defined as a set of the following fields:
Field Description type Probe type, for example: HTTP, PING or UDP name Probe name. Each probe should have a unique name. interval_msec How often to run the probe (in milliseconds). timeout_msec Probe timeout (in milliseconds). targets Targets to run probe against. validator Probe validators, further explained here. \u0026lt;type\u0026gt;_probe Probe type specific configuration. Please take a look at the ProbeDef protobuf for further details on various fields and options. All probe types export following metrics at a minimum:
Metric Description total Total number of probes. success Number of successful probes. Deficit between total and success indicates failures. latency Cumulative probe latency (by default in microseconds). Latency can also be configured to be a distribution (histogram) metric through a config option (latency_distribution). By default it\u0026rsquo;s just the sum of the latencies observed so far. Average latency can be computed using rate(latency) / rate(success). Probe Types #Cloudprober has built-in support for the following probe types:
Ping HTTP UDP DNS External More probe types can be added through cloudprober extensions (to be documented).
Ping #Code | Config options
Ping probe type implements a fast ping prober, that can probe hundreds of targets in parallel. Probe results are reported as number of packets sent (total), received (success) and round-trip time (latency). It supports raw sockets (requires root access) as well as datagram sockets for ICMP (doesn\u0026rsquo;t require root access).
ICMP datagram sockets are not enabled by default on most Linux systems. You can enable them by running the following command: sudo sysctl -w net.ipv4.ping_group_range=\u0026quot;0 5000\u0026quot;
HTTP #Code | Config options
HTTP probe is be used to send HTTP(s) requests to a target and verify that a response is received. Apart from the core probe metrics (total, success, and latency), HTTP probes also export a map of response code counts. Requests are marked as failed if there is a timeout.
SSL Certificate Expiry: If the target serves a SSL Certificate, cloudprober will walk the certificate chain and export the earliest expiry time in seconds as a metric. The metric is named ssl_earliest_cert_expiry_sec, and will only be exported when the expiry time in seconds is a positive number. UDP #Code | Config options
UDP probe sends a UDP packet to the configured targets. UDP probe (and all other probes that use ports) provides more coverage for the network elements on the data path as most packet forwarding elements use 5-tuple hashing and using a new source port for each probe ensures that we hit different network element each time.
DNS #Code | Config options
DNS probe type is implemented in a similar way as other probes except for that it sends DNS requests to the target.
External #Code | Config options
External probe type allows running arbitrary probes through cloudprober. For an external probe, actual probe logic resides in an external program; cloudprober only manages the execution of that program and provides a way to export that data through the standard channel.
External probe can be configured in two modes:
ONCE: In this mode, an external program is executed for each probe run. Exit status of the program determines the success or failure of the probe. External probe can optionally be configured to interpret external program\u0026rsquo;s output as metrics. This is a simple model but it doesn\u0026rsquo;t allow the external program to maintain state and multiple forks can be expensive depending on the frequency of the probes.
SERVER: In this mode, external program is expected to run in server mode. Cloudprober automatically starts the external program if it\u0026rsquo;s not running at the time of the probe execution. Cloudprober and external probe process communicate with each other over stdin/stdout using protobuf messages defined in probes/external/proto/server.proto.
`}),search.addEventListener("input",t,!0);function t(){const s=5;var n=this.value,o=e.search(n,{limit:s,enrich:!0});const t=new Map;for(const e of o.flatMap(e=>e.result)){if(t.has(e.doc.href))continue;t.set(e.doc.href,e.doc)}if(suggestions.innerHTML="",suggestions.classList.remove("d-none"),t.size===0&&n){const e=document.createElement("div");e.innerHTML=`No results for "<strong>${n}</strong>"`,e.classList.add("suggestion__no-results"),suggestions.appendChild(e);return}for(const[r,a]of t){const n=document.createElement("div");suggestions.appendChild(n);const e=document.createElement("a");e.href=r,n.appendChild(e);const o=document.createElement("span");o.textContent=a.title,o.classList.add("suggestion__title"),e.appendChild(o);const i=document.createElement("span");if(i.textContent=a.description,i.classList.add("suggestion__description"),e.appendChild(i),suggestions.appendChild(n),suggestions.childElementCount==s)break}}})()